{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as tv\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from C2AE import C2AE, save_model, load_model, Fe, Fx, Fd, eval_metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from scipy.io import arff\n",
    "\n",
    "import arff as arff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2AE Architecture\n",
    "* X:\n",
    "    * (N, d)\n",
    "* Y:\n",
    "    * (N, m)\n",
    "* Z:\n",
    "    * (N, l)\n",
    "\n",
    "## Three main components:\n",
    "* Fx:\n",
    "    * Encodes x into latent space z.\n",
    "* Fe:\n",
    "    * Encodes y into latent space z.\n",
    "* Fd:\n",
    "    * Decodes z into label space. \n",
    "\n",
    "## Loss functions:\n",
    "\n",
    "$$L_1 = ||F_x(X) - F_e(Y)||^2 s.t. F_x(X)Fx(X)^T = F_e(Y)F_e(Y)^T = I$$\n",
    "$$L_2 = \\Gamma(F_e, F_d) = \\Sigma_i^N E_i$$\n",
    "$$E_i = \\frac{1}{|y_i^1||y_i^0|} \\Sigma_{p,q \\in y_i^1\\times y_i^0} e^{F_d(F_e(y_i))^q - F_d(F_e(y_I))^p}$$\n",
    "\n",
    "## Combined Loss:\n",
    "$$L_1 + \\alpha L_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nus_train = arff2.load(open('./nus/nus-wide-full-cVLADplus-train.arff'))\n",
    "nus_test = arff2.load(open('./nus/nus-wide-full-cVLADplus-test.arff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nus_train_x[nus_train_y.sum(axis=1) != 0][:2**10]\n",
    "# nus_train_y[nus_train_y.sum(axis=1) != 0][:2**10]\n",
    "nus_train_x = np.array(nus_train['data'])[:, 1:129].astype('float64')\n",
    "nus_train_y = np.array(nus_train['data'])[:, 129:].astype('float64')\n",
    "nus_train_x = nus_train_x[nus_train_y.sum(axis=1) != 0][:2**10]\n",
    "nus_train_y = nus_train_y[nus_train_y.sum(axis=1) != 0][:2**10]\n",
    "\n",
    "\n",
    "nus_test_x = np.array(nus_test['data'])[:, 1:129].astype('float64')\n",
    "nus_test_y = np.array(nus_test['data'])[:, 129:].astype('float64')\n",
    "nus_test_x = nus_test_x[nus_test_y.sum(axis=1) != 0][:2**10]\n",
    "nus_test_y = nus_test_y[nus_test_y.sum(axis=1) != 0][:2**10]\n",
    "train_dataset = TensorDataset(torch.Tensor(nus_train_x), torch.Tensor(nus_train_y))\n",
    "test_dataset = TensorDataset(torch.Tensor(nus_test_x), torch.Tensor(nus_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2**10)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configs.\n",
    "batch_size = 54\n",
    "num_epochs = 1000\n",
    "lr = 0.0001\n",
    "device = torch.device('cuda')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# # Nus config\n",
    "feat_dim = 128\n",
    "latent_dim = 10\n",
    "num_labels = 81\n",
    "h_dim=50\n",
    "fx_h_dim=100\n",
    "# Scene models.\n",
    "Fx_nus = Fx(feat_dim, fx_h_dim, fx_h_dim, latent_dim)\n",
    "Fe_nus = Fe(num_labels, h_dim, latent_dim)\n",
    "Fd_nus = Fd(latent_dim, h_dim, num_labels, fin_act=torch.sigmoid)\n",
    "               \n",
    "# Initializing net.\n",
    "net = C2AE(Fx_nus, Fe_nus, Fd_nus, alpha=5, emb_lambda=0.001, latent_dim=latent_dim, device=device)\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "# Doing weight_decay here is eqiv to adding the L2 norm.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.0)\n",
    "writer = SummaryWriter(comment='nus-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='micro')\n",
    "def macro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='macro')\n",
    "def micro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='micro')\n",
    "def macro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='macro')\n",
    "def micro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='micro')\n",
    "def macro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='macro')\n",
    "def ham_los(*args, **kwargs):\n",
    "    return hamming_loss(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Epoch: 0, Loss: 96.51275873184204,  L-Loss: 1.008283395320177, C-Loss: 19.100895047187805\n",
      "Epoch: 100, Loss: 58.89303755760193,  L-Loss: 1.3860370218753815, C-Loss: 11.501400113105774\n",
      "Epoch: 200, Loss: 57.99732780456543,  L-Loss: 2.07010905072093, C-Loss: 11.185443818569183\n",
      "Epoch: 300, Loss: 58.92934536933899,  L-Loss: 3.254526101052761, C-Loss: 11.134963899850845\n",
      "Epoch: 400, Loss: 61.09421181678772,  L-Loss: 5.580952599644661, C-Loss: 11.102651804685593\n",
      "Epoch: 500, Loss: 62.69799470901489,  L-Loss: 7.070828527212143, C-Loss: 11.125433325767517\n",
      "Epoch: 600, Loss: 62.806777000427246,  L-Loss: 7.005776256322861, C-Loss: 11.160200208425522\n",
      "Epoch: 700, Loss: 62.80078625679016,  L-Loss: 6.675743967294693, C-Loss: 11.225008308887482\n",
      "Epoch: 800, Loss: 63.28416895866394,  L-Loss: 6.414846122264862, C-Loss: 11.373864531517029\n",
      "Epoch: 900, Loss: 63.1254026889801,  L-Loss: 6.2795035392045975, C-Loss: 11.369179874658585\n",
      "Epoch: 1000, Loss: 63.5850510597229,  L-Loss: 6.034939795732498, C-Loss: 11.510022401809692\n",
      "{'dataset_0': {'hamming_loss': 0.07384500385802469, 'accuracy_score': 0.0}, 'dataset_1': {'hamming_loss': 0.06721402391975309, 'accuracy_score': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training!\")\n",
    "best_loss = np.infty\n",
    "for epoch in range(num_epochs+1): \n",
    "    # Training.\n",
    "    net.train()\n",
    "    loss_tracker = 0.0\n",
    "    latent_loss_tracker = 0.0\n",
    "    cor_loss_tracker = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()      \n",
    "\n",
    "        # Pass x, y to network. Retrieve both encodings, and decoding of ys encoding.\n",
    "        fx_x, fe_y, fd_z = net(x, y)\n",
    "        # Calc loss.\n",
    "        l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "        # Normalize losses by batch.\n",
    "        l_loss /= x.shape[0]\n",
    "        c_loss /= x.shape[0]\n",
    "        loss = l_loss + net.alpha*c_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_tracker+=loss.item()\n",
    "        latent_loss_tracker+=l_loss.item()\n",
    "        cor_loss_tracker+=c_loss.item()\n",
    "    writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "    writer.add_scalar('train/latent_loss', latent_loss_tracker, epoch)\n",
    "    writer.add_scalar('train/corr_loss', cor_loss_tracker, epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    net.eval()\n",
    "    loss_tracker = 0.0\n",
    "    latent_loss_tracker = 0.0\n",
    "    cor_loss_tracker = 0.0\n",
    "    acc_track = 0.0\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # evaluation only requires x. As its just Fd(Fx(x))\n",
    "        fx_x, fe_y = net.Fx(x), net.Fe(y)\n",
    "        fd_z = net.Fd(fx_x)\n",
    "\n",
    "        l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "        # Normalize losses by batch.\n",
    "        l_loss /= x.shape[0]\n",
    "        c_loss /= x.shape[0]\n",
    "        loss = l_loss + net.alpha*c_loss\n",
    "        \n",
    "        latent_loss_tracker += l_loss.item()\n",
    "        cor_loss_tracker += c_loss.item()\n",
    "        loss_tracker += loss.item()\n",
    "        lab_preds = torch.round(net.Fd(net.Fx(x))).cpu().detach().numpy()\n",
    "        acc_track += accuracy_score(y.cpu().detach().numpy(), lab_preds)\n",
    "        \n",
    "#     if loss_tracker < best_loss:\n",
    "#         best_loss = loss_tracker\n",
    "#         print(\"Saving model.\")\n",
    "#         torch.save(net.state_dict(), f'./models/nus_best/best.pt')\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save(net.state_dict(), f'./models/nus_best/best_{epoch}.pt')\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss_tracker},  L-Loss: {latent_loss_tracker}, C-Loss: {cor_loss_tracker}\")\n",
    "    mets = eval_metrics(net, [ham_los, accuracy_score, micro_f1, micro_p, micro_r, macro_f1, macro_p, macro_r], \n",
    "                        [test_dataset, train_dataset], torch.device('cuda'))\n",
    "    \n",
    "    # Val\n",
    "    for k, v in mets['dataset_1'].items():\n",
    "        writer.add_scalar(f'train/{k}', v, epoch)\n",
    "    \n",
    "    # Train\n",
    "    for k, v in mets['dataset_0'].items():\n",
    "        writer.add_scalar(f'val/{k}', v, epoch)\n",
    "    \n",
    "    writer.add_scalar('val/loss', loss_tracker, epoch)\n",
    "    writer.add_scalar('val/latent_loss', latent_loss_tracker, epoch)\n",
    "    writer.add_scalar('val/corr_loss', cor_loss_tracker, epoch)\n",
    "#     writer.add_scalar('val/acc', acc_track, epoch)\n",
    "print(eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'ham_los': 0.13558545524691357,\n",
       "  'accuracy_score': 0.0,\n",
       "  'micro_f1': 0.21290593505039193,\n",
       "  'micro_p': 0.123779296875,\n",
       "  'micro_r': 0.7605,\n",
       "  'macro_f1': 0.029859431562645266,\n",
       "  'macro_p': 0.018337673611111112,\n",
       "  'macro_r': 0.14814814814814814},\n",
       " 'dataset_1': {'ham_los': 0.13442804783950618,\n",
       "  'accuracy_score': 0.0,\n",
       "  'micro_f1': 0.21123372948500285,\n",
       "  'micro_p': 0.12150065104166667,\n",
       "  'micro_r': 0.8079004329004329,\n",
       "  'macro_f1': 0.029287285999225632,\n",
       "  'macro_p': 0.018000096450617283,\n",
       "  'macro_r': 0.14814814814814814}}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [ham_los, accuracy_score, micro_f1, micro_p, micro_r, macro_f1, macro_p, macro_r], \n",
    "                        [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "net.load_state_dict(torch.load('./models/nus_best/best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.07098765432098765, 'accuracy_score': 0.0},\n",
       " 'dataset_1': {'hamming_loss': 0.06867283950617284, 'accuracy_score': 0.0}}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .75 acc, .003 ham on val or die mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.0030864197530864196, 'accuracy_score': 0.75},\n",
       " 'dataset_1': {'hamming_loss': 0.0, 'accuracy_score': 1.0}}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.0030864197530864196, 'accuracy_score': 0.75},\n",
       " 'dataset_1': {'hamming_loss': 0.0, 'accuracy_score': 1.0}}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.07175925925925926, 'accuracy_score': 0.0},\n",
       " 'dataset_1': {'hamming_loss': 0.06867283950617284, 'accuracy_score': 0.0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.07175925925925926, 'accuracy_score': 0.0},\n",
       " 'dataset_1': {'hamming_loss': 0.06867283950617284, 'accuracy_score': 0.0}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.07175925925925926, 'accuracy_score': 0.0},\n",
       " 'dataset_1': {'hamming_loss': 0.06867283950617284, 'accuracy_score': 0.0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('torch_sp_env': virtualenv)",
   "language": "python",
   "name": "python37264bittorchspenvvirtualenv64bc8108ce9c407e99ec601d0f3b78f0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
