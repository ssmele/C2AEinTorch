{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from skmultilearn.dataset import load_dataset\n",
    "\n",
    "from C2AE import save_model, eval_metrics, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmc2007_500:train - exists, not redownloading\n",
      "tmc2007_500:test - exists, not redownloading\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "train_x, train_y, feat_names, label_names = load_dataset('tmc2007_500', 'train')\n",
    "test_x, test_y, _, _ = load_dataset('tmc2007_500', 'test')\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_x.todense(), device=device, dtype=torch.float),torch.tensor(train_y.todense(), device=device,dtype=torch.float))\n",
    "test_dataset = TensorDataset(torch.tensor(test_x.todense(), device=device, dtype=torch.float), torch.tensor(test_y.todense(), device=device, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='micro')\n",
    "def macro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='macro')\n",
    "def micro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='micro')\n",
    "def macro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='macro')\n",
    "def micro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='micro')\n",
    "def macro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='macro')\n",
    "def ham_los(*args, **kwargs):\n",
    "    return hamming_loss(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMCModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TMCModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(500, 22)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making TensorDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21519, 7077)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "lr = 0.0001\n",
    "batch_size=32\n",
    "\n",
    "net = TMCModel().to(device)\n",
    "writer = SummaryWriter(comment='tmc_fc')\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stone\\miniconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.5197060941587054, Test loss: 0.41104362462017985\n",
      "Epoch: 5, Train loss: 0.23351587344066171, Test loss: 0.22873962026190114\n",
      "Epoch: 10, Train loss: 0.1979138252462356, Test loss: 0.19559759304330154\n",
      "Epoch: 15, Train loss: 0.18074242742995097, Test loss: 0.17922576530291154\n",
      "Epoch: 20, Train loss: 0.17030092164940502, Test loss: 0.16914177075162665\n",
      "Epoch: 25, Train loss: 0.16324709213019126, Test loss: 0.1622517042466112\n",
      "Epoch: 30, Train loss: 0.15815647709201386, Test loss: 0.1572293255914439\n",
      "Epoch: 35, Train loss: 0.1543061088820446, Test loss: 0.15339991487227045\n",
      "Epoch: 40, Train loss: 0.1512896227823324, Test loss: 0.15038158398893503\n",
      "Epoch: 45, Train loss: 0.1488605431249202, Test loss: 0.14794019874822986\n",
      "Epoch: 50, Train loss: 0.1468604507829778, Test loss: 0.1459233756135176\n",
      "Epoch: 55, Train loss: 0.14518305993629246, Test loss: 0.14422777543465296\n",
      "Epoch: 60, Train loss: 0.1437545874741421, Test loss: 0.14278110381853473\n",
      "Epoch: 65, Train loss: 0.1425223117486683, Test loss: 0.14153142377465694\n",
      "Epoch: 70, Train loss: 0.14144758875710117, Test loss: 0.14044052651068112\n",
      "Epoch: 75, Train loss: 0.14050144860507122, Test loss: 0.1394796619477036\n",
      "Epoch: 80, Train loss: 0.13966173764988005, Test loss: 0.13862676874877097\n",
      "Epoch: 85, Train loss: 0.13891119817700817, Test loss: 0.13786457885090295\n",
      "Epoch: 90, Train loss: 0.1382361945611761, Test loss: 0.13717939389181566\n",
      "Epoch: 95, Train loss: 0.1376257998672318, Test loss: 0.1365601783534428\n",
      "Epoch: 100, Train loss: 0.1370711296486819, Test loss: 0.1359979401233497\n",
      "Epoch: 105, Train loss: 0.1365648944589571, Test loss: 0.13548525184527174\n",
      "Epoch: 110, Train loss: 0.13610103745207214, Test loss: 0.1350159430960277\n",
      "Epoch: 115, Train loss: 0.1356744742765469, Test loss: 0.13458480944370363\n",
      "Epoch: 120, Train loss: 0.13528090683303298, Test loss: 0.13418746437575366\n",
      "Epoch: 125, Train loss: 0.1349166710964815, Test loss: 0.13382015202764994\n",
      "Epoch: 130, Train loss: 0.1345786230325876, Test loss: 0.13347966656894297\n",
      "Epoch: 135, Train loss: 0.1342640563092827, Test loss: 0.13316322248932477\n",
      "Epoch: 140, Train loss: 0.13397062252127576, Test loss: 0.13286841395604718\n",
      "Epoch: 145, Train loss: 0.13369627941239287, Test loss: 0.13259315198740443\n",
      "Epoch: 150, Train loss: 0.13343924707989105, Test loss: 0.13233559296743289\n",
      "Epoch: 155, Train loss: 0.13319795150282124, Test loss: 0.1320941232413322\n",
      "Epoch: 160, Train loss: 0.1329710086887871, Test loss: 0.13186730578676\n",
      "Epoch: 165, Train loss: 0.13275718814063744, Test loss: 0.13165388088505547\n",
      "Epoch: 170, Train loss: 0.1325554050073404, Test loss: 0.13145272665329882\n",
      "Epoch: 175, Train loss: 0.13236468640691634, Test loss: 0.13126283326932975\n",
      "Epoch: 180, Train loss: 0.13218415784268997, Test loss: 0.1310832992800184\n",
      "Epoch: 185, Train loss: 0.13201304237298767, Test loss: 0.13091331938499803\n",
      "Epoch: 190, Train loss: 0.1318506317166127, Test loss: 0.13075216768963918\n",
      "Epoch: 195, Train loss: 0.13169629326059773, Test loss: 0.13059917787039602\n",
      "Epoch: 200, Train loss: 0.1315494492030073, Test loss: 0.13045377958868\n",
      "Epoch: 205, Train loss: 0.1314095783685327, Test loss: 0.13031540841266914\n",
      "Epoch: 210, Train loss: 0.13127620763977074, Test loss: 0.13018359576125402\n",
      "Epoch: 215, Train loss: 0.13114890224822032, Test loss: 0.13005787171087824\n",
      "Epoch: 220, Train loss: 0.13102726705848375, Test loss: 0.1299378478580767\n",
      "Epoch: 225, Train loss: 0.13091094205142131, Test loss: 0.1298231532847559\n",
      "Epoch: 230, Train loss: 0.13079959502856056, Test loss: 0.1297134341218987\n",
      "Epoch: 235, Train loss: 0.13069292175601532, Test loss: 0.12960838643951458\n",
      "Epoch: 240, Train loss: 0.13059064160263026, Test loss: 0.12950772114165193\n",
      "Epoch: 245, Train loss: 0.13049249531477025, Test loss: 0.12941117953878264\n",
      "Epoch: 250, Train loss: 0.130398243599271, Test loss: 0.12931850310918447\n",
      "Epoch: 255, Train loss: 0.13030766902759916, Test loss: 0.12922948484753702\n",
      "Epoch: 260, Train loss: 0.13022056510907226, Test loss: 0.12914390543156917\n",
      "Epoch: 265, Train loss: 0.13013674315469226, Test loss: 0.12906156711884448\n",
      "Epoch: 270, Train loss: 0.13005602267884925, Test loss: 0.12898230109665845\n",
      "Epoch: 275, Train loss: 0.12997824263608118, Test loss: 0.12890593304827408\n",
      "Epoch: 280, Train loss: 0.12990325459045252, Test loss: 0.12883231980172363\n",
      "Epoch: 285, Train loss: 0.1298309092646545, Test loss: 0.12876129519563537\n",
      "Epoch: 290, Train loss: 0.12976107758540456, Test loss: 0.12869275022331658\n",
      "Epoch: 295, Train loss: 0.12969363600369901, Test loss: 0.12862655369413867\n",
      "Epoch: 300, Train loss: 0.12962846293004804, Test loss: 0.1285625680744111\n",
      "Epoch: 305, Train loss: 0.12956545361349522, Test loss: 0.12850070355443266\n",
      "Epoch: 310, Train loss: 0.12950450527366877, Test loss: 0.128440848677545\n",
      "Epoch: 315, Train loss: 0.12944551977989047, Test loss: 0.12838291920520165\n",
      "Epoch: 320, Train loss: 0.1293884028762269, Test loss: 0.12832680955394968\n",
      "Epoch: 325, Train loss: 0.1293330796804683, Test loss: 0.12827245005079219\n",
      "Epoch: 330, Train loss: 0.12927946254588873, Test loss: 0.1282197398444017\n",
      "Epoch: 335, Train loss: 0.12922747795197442, Test loss: 0.12816862802247744\n",
      "Epoch: 340, Train loss: 0.12917705514588645, Test loss: 0.12811903121906357\n",
      "Epoch: 345, Train loss: 0.1291281249025424, Test loss: 0.12807088049950902\n",
      "Epoch: 350, Train loss: 0.12908062623346012, Test loss: 0.12802412089061094\n",
      "Epoch: 355, Train loss: 0.12903449671096645, Test loss: 0.1279786891631178\n",
      "Epoch: 360, Train loss: 0.12898967964201058, Test loss: 0.12793453507595234\n",
      "Epoch: 365, Train loss: 0.1289461218761654, Test loss: 0.12789160194429192\n",
      "Epoch: 370, Train loss: 0.12890377122615457, Test loss: 0.12784984251400372\n",
      "Epoch: 375, Train loss: 0.1288625812946776, Test loss: 0.12780920093929446\n",
      "Epoch: 380, Train loss: 0.1288225036252838, Test loss: 0.12776964003438349\n",
      "Epoch: 385, Train loss: 0.12878349786527443, Test loss: 0.12773111600193893\n",
      "Epoch: 390, Train loss: 0.12874551986470087, Test loss: 0.12769358531311825\n",
      "Epoch: 395, Train loss: 0.12870853110860042, Test loss: 0.12765701232595486\n",
      "Epoch: 400, Train loss: 0.1286724911889208, Test loss: 0.12762136032452454\n",
      "Epoch: 405, Train loss: 0.12863736648394838, Test loss: 0.12758659195524077\n",
      "Epoch: 410, Train loss: 0.12860312737956267, Test loss: 0.12755268241639608\n",
      "Epoch: 415, Train loss: 0.1285697349733265, Test loss: 0.12751958838051503\n",
      "Epoch: 420, Train loss: 0.12853716407939902, Test loss: 0.12748729283208246\n",
      "Epoch: 425, Train loss: 0.12850538560398267, Test loss: 0.1274557633510044\n",
      "Epoch: 430, Train loss: 0.1284743654603944, Test loss: 0.12742496644322937\n",
      "Epoch: 435, Train loss: 0.128444083300689, Test loss: 0.1273948877781361\n",
      "Epoch: 440, Train loss: 0.12841450900149382, Test loss: 0.12736548748505008\n",
      "Epoch: 445, Train loss: 0.12838562248056037, Test loss: 0.12733675113266651\n",
      "Epoch: 450, Train loss: 0.12835739911665356, Test loss: 0.12730865865140348\n",
      "Epoch: 455, Train loss: 0.12832981901572682, Test loss: 0.12728119171686\n",
      "Epoch: 460, Train loss: 0.128302856194849, Test loss: 0.12725432126505956\n",
      "Epoch: 465, Train loss: 0.12827649252021472, Test loss: 0.12722802400454744\n",
      "Epoch: 470, Train loss: 0.12825070600541527, Test loss: 0.12720228879301398\n",
      "Epoch: 475, Train loss: 0.12822548197070344, Test loss: 0.12717709562799953\n",
      "Epoch: 480, Train loss: 0.12820080333399028, Test loss: 0.12715243299802145\n",
      "Epoch: 485, Train loss: 0.12817664790959452, Test loss: 0.12712827918900027\n",
      "Epoch: 490, Train loss: 0.1281529992353863, Test loss: 0.127104617218982\n",
      "Epoch: 495, Train loss: 0.12812984714846376, Test loss: 0.12708142977040093\n",
      "Epoch: 500, Train loss: 0.12810717285077958, Test loss: 0.12705870573450853\n",
      "Epoch: 505, Train loss: 0.1280849649505842, Test loss: 0.1270364286663296\n",
      "Epoch: 510, Train loss: 0.1280632074507172, Test loss: 0.12701459282690342\n",
      "Epoch: 515, Train loss: 0.12804188561607788, Test loss: 0.1269931740186236\n",
      "Epoch: 520, Train loss: 0.12802098858631028, Test loss: 0.12697216868400574\n",
      "Epoch: 525, Train loss: 0.1280005032204944, Test loss: 0.12695155752537488\n",
      "Epoch: 530, Train loss: 0.12798041644413438, Test loss: 0.1269313340989856\n",
      "Epoch: 535, Train loss: 0.12796071861465122, Test loss: 0.12691148645706005\n",
      "Epoch: 540, Train loss: 0.12794139562797405, Test loss: 0.1268920068805282\n",
      "Epoch: 545, Train loss: 0.1279224395906943, Test loss: 0.12687288432776392\n",
      "Epoch: 550, Train loss: 0.12790383912213313, Test loss: 0.12685410104490616\n",
      "Epoch: 555, Train loss: 0.1278855863399527, Test loss: 0.1268356609250511\n",
      "Epoch: 560, Train loss: 0.12786767180084652, Test loss: 0.12681754208631343\n",
      "Epoch: 565, Train loss: 0.1278500849101554, Test loss: 0.12679974486430487\n",
      "Epoch: 570, Train loss: 0.1278328180512383, Test loss: 0.12678225989545788\n",
      "Epoch: 575, Train loss: 0.12781586261109115, Test loss: 0.12676507613814628\n",
      "Epoch: 580, Train loss: 0.12779921006527556, Test loss: 0.12674818520207662\n",
      "Epoch: 585, Train loss: 0.12778285121404045, Test loss: 0.12673158114692112\n",
      "Epoch: 590, Train loss: 0.12776677955888532, Test loss: 0.1267152539378888\n",
      "Epoch: 595, Train loss: 0.12775098568971585, Test loss: 0.12669919974900581\n",
      "Epoch: 600, Train loss: 0.12773546298625388, Test loss: 0.12668340079285004\n",
      "Epoch: 605, Train loss: 0.12772020315654564, Test loss: 0.12666786062690588\n",
      "Epoch: 610, Train loss: 0.12770520378717873, Test loss: 0.12665257894912282\n",
      "Epoch: 615, Train loss: 0.12769045596624023, Test loss: 0.12663754619456627\n",
      "Epoch: 620, Train loss: 0.12767595677106555, Test loss: 0.12662275115380417\n",
      "Epoch: 625, Train loss: 0.12766169717903478, Test loss: 0.12660819174604374\n",
      "Epoch: 630, Train loss: 0.1276476719094244, Test loss: 0.1265938629035477\n",
      "Epoch: 635, Train loss: 0.12763387387698685, Test loss: 0.1265797514367748\n",
      "Epoch: 640, Train loss: 0.12762029668285801, Test loss: 0.126565856775185\n",
      "Epoch: 645, Train loss: 0.12760693581019256, Test loss: 0.12655217304557292\n",
      "Epoch: 650, Train loss: 0.12759378764994253, Test loss: 0.12653869356926498\n",
      "Epoch: 655, Train loss: 0.12758084770740408, Test loss: 0.12652542250784668\n",
      "Epoch: 660, Train loss: 0.1275681091851678, Test loss: 0.12651235744491354\n",
      "Epoch: 665, Train loss: 0.12755557006836646, Test loss: 0.1264994874059617\n",
      "Epoch: 670, Train loss: 0.1275432238363582, Test loss: 0.12648680490684938\n",
      "Epoch: 675, Train loss: 0.12753106648154988, Test loss: 0.12647431313588814\n",
      "Epoch: 680, Train loss: 0.12751909468273173, Test loss: 0.12646199725903906\n",
      "Epoch: 685, Train loss: 0.12750730162035304, Test loss: 0.12644985338320602\n",
      "Epoch: 690, Train loss: 0.12749568483671853, Test loss: 0.12643788536792402\n",
      "Epoch: 695, Train loss: 0.12748424002532618, Test loss: 0.1264260854270007\n",
      "Epoch: 700, Train loss: 0.12747296316751222, Test loss: 0.12641445094266454\n",
      "Epoch: 705, Train loss: 0.1274618501449765, Test loss: 0.12640297503487483\n",
      "Epoch: 710, Train loss: 0.12745089832289258, Test loss: 0.12639166119399373\n",
      "Epoch: 715, Train loss: 0.12744010371580874, Test loss: 0.1263805025735417\n",
      "Epoch: 720, Train loss: 0.12742946310215153, Test loss: 0.12636949504549438\n",
      "Epoch: 725, Train loss: 0.12741897464418483, Test loss: 0.12635864163035745\n",
      "Epoch: 730, Train loss: 0.1274086345778709, Test loss: 0.12634793044747533\n",
      "Epoch: 735, Train loss: 0.1273984391834548, Test loss: 0.12633737096109907\n",
      "Epoch: 740, Train loss: 0.12738838504009048, Test loss: 0.1263269490755356\n",
      "Epoch: 745, Train loss: 0.12737847002220368, Test loss: 0.12631666650240486\n",
      "Epoch: 750, Train loss: 0.12736869073108967, Test loss: 0.12630652199994336\n",
      "Epoch: 755, Train loss: 0.12735904416658902, Test loss: 0.12629650459364727\n",
      "Epoch: 760, Train loss: 0.12734952855738985, Test loss: 0.12628661743826694\n",
      "Epoch: 765, Train loss: 0.12734013987375759, Test loss: 0.12627685523113688\n",
      "Epoch: 770, Train loss: 0.12733087429630102, Test loss: 0.1262672165626878\n",
      "Epoch: 775, Train loss: 0.12732173267746397, Test loss: 0.12625770116443033\n",
      "Epoch: 780, Train loss: 0.1273127115410469, Test loss: 0.1262483108486678\n",
      "Epoch: 785, Train loss: 0.12730380960284868, Test loss: 0.1262390405812242\n",
      "Epoch: 790, Train loss: 0.12729502341988183, Test loss: 0.12622988526080106\n",
      "Epoch: 795, Train loss: 0.12728635165259175, Test loss: 0.12622084518944895\n",
      "Epoch: 800, Train loss: 0.12727779182114182, Test loss: 0.1262119168768058\n",
      "Epoch: 805, Train loss: 0.12726934113571584, Test loss: 0.12620309938315874\n",
      "Epoch: 810, Train loss: 0.12726099807962798, Test loss: 0.1261943862312012\n",
      "Epoch: 815, Train loss: 0.127252760527304, Test loss: 0.1261857853749314\n",
      "Epoch: 820, Train loss: 0.12724462597676583, Test loss: 0.1261772863097019\n",
      "Epoch: 825, Train loss: 0.12723659327666084, Test loss: 0.126168887927994\n",
      "Epoch: 830, Train loss: 0.12722865742303288, Test loss: 0.12616059214279457\n",
      "Epoch: 835, Train loss: 0.12722082045289063, Test loss: 0.12615239603428152\n",
      "Epoch: 840, Train loss: 0.1272130800081752, Test loss: 0.12614429443403408\n",
      "Epoch: 845, Train loss: 0.1272054345389886, Test loss: 0.1261362856639935\n",
      "Epoch: 850, Train loss: 0.12719788137729243, Test loss: 0.12612836945567046\n",
      "Epoch: 855, Train loss: 0.12719041834215883, Test loss: 0.126120544667985\n",
      "Epoch: 860, Train loss: 0.12718304483577011, Test loss: 0.12611281546252268\n",
      "Epoch: 865, Train loss: 0.1271757585664915, Test loss: 0.1261051730126948\n",
      "Epoch: 870, Train loss: 0.12716855751945572, Test loss: 0.12609761490209684\n",
      "Epoch: 875, Train loss: 0.12716144095292595, Test loss: 0.12609014676900598\n",
      "Epoch: 880, Train loss: 0.12715440519143, Test loss: 0.12608275381294456\n",
      "Epoch: 885, Train loss: 0.12714745131989644, Test loss: 0.12607544821661873\n",
      "Epoch: 890, Train loss: 0.1271405773455994, Test loss: 0.12606822524790293\n",
      "Epoch: 895, Train loss: 0.1271337826817918, Test loss: 0.12606108004042693\n",
      "Epoch: 900, Train loss: 0.1271270644833039, Test loss: 0.12605401309760841\n",
      "Epoch: 905, Train loss: 0.127120422373732, Test loss: 0.1260470226742663\n",
      "Epoch: 910, Train loss: 0.12711385450426926, Test loss: 0.12604011212651794\n",
      "Epoch: 915, Train loss: 0.12710736088598643, Test loss: 0.126033278098246\n",
      "Epoch: 920, Train loss: 0.12710094071072242, Test loss: 0.1260265165621096\n",
      "Epoch: 925, Train loss: 0.12709459155399414, Test loss: 0.12601982933041211\n",
      "Epoch: 930, Train loss: 0.1270883126076405, Test loss: 0.1260132149600231\n",
      "Epoch: 935, Train loss: 0.12708210235497572, Test loss: 0.12600667069892627\n",
      "Epoch: 940, Train loss: 0.12707595894719304, Test loss: 0.1260001965471216\n",
      "Epoch: 945, Train loss: 0.12706988218501986, Test loss: 0.12599379287378207\n",
      "Epoch: 950, Train loss: 0.1270638712381537, Test loss: 0.12598745266462233\n",
      "Epoch: 955, Train loss: 0.12705792461205026, Test loss: 0.12598117937644324\n",
      "Epoch: 960, Train loss: 0.12705204178638657, Test loss: 0.12597497203597077\n",
      "Epoch: 965, Train loss: 0.12704622052488143, Test loss: 0.12596882359535844\n",
      "Epoch: 970, Train loss: 0.1270404610710902, Test loss: 0.12596273902165997\n",
      "Epoch: 975, Train loss: 0.1270347626057812, Test loss: 0.12595671609983788\n",
      "Epoch: 980, Train loss: 0.12702912364548063, Test loss: 0.1259507559374109\n",
      "Epoch: 985, Train loss: 0.12702354132287746, Test loss: 0.12594485363444766\n",
      "Epoch: 990, Train loss: 0.1270180183613637, Test loss: 0.1259390135203396\n",
      "Epoch: 995, Train loss: 0.12701255210397147, Test loss: 0.12593323217184693\n",
      "Epoch: 1000, Train loss: 0.12700714267247853, Test loss: 0.12592750522601712\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    \n",
    "    net.train()\n",
    "    loss_tracker = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = net(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_tracker+=loss.item()\n",
    "    train_loss.append(loss_tracker/len(train_dataloader))\n",
    "    writer.add_scalar('train/loss', loss_tracker/len(train_dataloader), epoch)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        net.eval()\n",
    "        test_tracker = 0.0\n",
    "        for x, y in test_dataloader:\n",
    "            preds = net(x)\n",
    "            loss = criterion(preds, y)\n",
    "            test_tracker += loss.item()\n",
    "        test_loss.append(test_tracker/len(test_dataloader))\n",
    "        writer.add_scalar('val/loss', test_tracker/len(test_dataloader), epoch)\n",
    "\n",
    "        # Log all the good metrics to the board.\n",
    "        mets = eval_metrics(net, [ham_los, accuracy_score, micro_f1, micro_p, micro_r, macro_f1, macro_p, macro_r], \n",
    "                                [test_dataset, train_dataset], device, apply_sig=True)\n",
    "        for k, v in mets['dataset_1'].items():\n",
    "            writer.add_scalar(f'train/{k}', v, epoch)\n",
    "        # Train\n",
    "        for k, v in mets['dataset_0'].items():\n",
    "            writer.add_scalar(f'val/{k}', v, epoch)\n",
    "        print(\"Epoch: {}, Train loss: {}, Test loss: {}\".format(epoch, train_loss[-1], test_loss[-1]))\n",
    "    torch.save(net.state_dict(), f'./models/tmc_fc/{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "def load_model(model_cls, path, *args, **kwargs):\n",
    "    model = model_cls(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_net = load_model(TMCModel, './models/tmc_fc/1000.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'ham_los': 0.05343815432836204,\n",
       "  'accuracy_score': 0.3300833686590363,\n",
       "  'micro_f1': 0.7169490372184798,\n",
       "  'micro_p': 0.7627768930070943,\n",
       "  'micro_r': 0.6763157894736842,\n",
       "  'macro_f1': 0.6546244349804987,\n",
       "  'macro_p': 0.7888285445446485,\n",
       "  'macro_r': 0.5790272326789471},\n",
       " 'dataset_1': {'ham_los': 0.052674380779776014,\n",
       "  'accuracy_score': 0.3398392118592871,\n",
       "  'micro_f1': 0.7241756904732937,\n",
       "  'micro_p': 0.7699508431921349,\n",
       "  'micro_r': 0.6835379604109246,\n",
       "  'macro_f1': 0.6553832344888076,\n",
       "  'macro_p': 0.7876056752223366,\n",
       "  'macro_r': 0.5791569417923896}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mets = eval_metrics(eval_net, [ham_los, accuracy_score, micro_f1, micro_p, micro_r, macro_f1, macro_p, macro_r], [test_dataset, train_dataset], device, apply_sig=True)\n",
    "mets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
