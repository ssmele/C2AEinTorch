{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as tv\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from C2AE import C2AE, save_model, load_model, Fe, Fx, Fd, eval_metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from scipy.io import arff\n",
    "\n",
    "import arff as arff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2AE Architecture\n",
    "* X:\n",
    "    * (N, d)\n",
    "* Y:\n",
    "    * (N, m)\n",
    "* Z:\n",
    "    * (N, l)\n",
    "\n",
    "## Three main components:\n",
    "* Fx:\n",
    "    * Encodes x into latent space z.\n",
    "* Fe:\n",
    "    * Encodes y into latent space z.\n",
    "* Fd:\n",
    "    * Decodes z into label space. \n",
    "\n",
    "## Loss functions:\n",
    "\n",
    "$$L_1 = ||F_x(X) - F_e(Y)||^2 s.t. F_x(X)Fx(X)^T = F_e(Y)F_e(Y)^T = I$$\n",
    "$$L_2 = \\Gamma(F_e, F_d) = \\Sigma_i^N E_i$$\n",
    "$$E_i = \\frac{1}{|y_i^1||y_i^0|} \\Sigma_{p,q \\in y_i^1\\times y_i^0} e^{F_d(F_e(y_i))^q - F_d(F_e(y_I))^p}$$\n",
    "\n",
    "## Combined Loss:\n",
    "$$L_1 + \\alpha L_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene = arff.loadarff('./scene.arff')\n",
    "# scene_df = pd.DataFrame(scene[0])\n",
    "# data_labels = scene_df.loc[:, 'Beach': 'Urban'].astype('int').values\n",
    "\n",
    "# X, Y = scene_df.loc[:, 'Att1':'Att294'].values, scene_df.loc[:, 'Beach': 'Urban'].astype('int').values\n",
    "# trans = tv.transforms.Compose([tv.transforms.Lambda(lambda x: torch.Tensor(x))])\n",
    "# X, Y = trans(X), trans(Y)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "\n",
    "# train_dataset = TensorDataset(X_train,y_train)\n",
    "# test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "train = arff.loadarff('./scene.arff')\n",
    "test = arff.loadarff('./scene-train.arff')\n",
    "train_df = pd.DataFrame(train[0])\n",
    "test_df = pd.DataFrame(test[0])\n",
    "X, Y = train_df.loc[:, 'Att1':'Att294'].values, train_df.loc[:, 'Beach': 'Urban'].astype('int').values\n",
    "X_test, Y_test  = test_df.loc[:, 'Att1':'Att294'].values, test_df.loc[:, 'Beach': 'Urban'].astype('int').values\n",
    "\n",
    "trans = tv.transforms.Compose([tv.transforms.Lambda(lambda x: torch.Tensor(x))])\n",
    "X, Y = trans(X), trans(Y)\n",
    "X_test, Y_test = trans(X_test), trans(Y_test)\n",
    "\n",
    "train_dataset = TensorDataset(X,Y)\n",
    "test_dataset = TensorDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nus_train = arff2.load(open('./nus/nus-wide-full-cVLADplus-train.arff'))\n",
    "# nus_test = arff2.load(open('./nus/nus-wide-full-cVLADplus-test.arff'))\n",
    "# nus_train_x = np.array(nus_train['data'])[:, 1:129].astype('float64')\n",
    "# nus_train_y = np.array(nus_train['data'])[:, 129:].astype('float64')\n",
    "# nus_train_x[nus_train_y.sum(axis=1) != 0]\n",
    "# nus_train_y[nus_train_y.sum(axis=1) != 0]\n",
    "\n",
    "# nus_test_x = np.array(nus_test['data'])[:, 1:129].astype('float64')\n",
    "# nus_test_y = np.array(nus_test['data'])[:, 129:].astype('float64')\n",
    "# nus_test_x[nus_test_y.sum(axis=1) != 0]\n",
    "# nus_test_y[nus_test_y.sum(axis=1) != 0]\n",
    "# train_dataset = TensorDataset(torch.Tensor(nus_train_x), torch.Tensor(nus_train_y))\n",
    "# test_dataset = TensorDataset(torch.Tensor(nus_test_x), torch.Tensor(nus_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1211, 294]),\n",
       " torch.Size([1211, 6]),\n",
       " torch.Size([1211, 294]),\n",
       " torch.Size([1211, 6]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:][0].shape, train_dataset[:][1].shape, test_dataset[:][0].shape, test_dataset[:][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='micro')\n",
    "def macro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='macro')\n",
    "def micro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='micro')\n",
    "def macro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='macro')\n",
    "def micro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='micro')\n",
    "def macro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='macro')\n",
    "def ham_los(*args, **kwargs):\n",
    "    return hamming_loss(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configs.\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "lr = 0.001\n",
    "device = torch.device('cuda')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# # Nus config\n",
    "# feat_dim = 128\n",
    "# latent_dim = 200\n",
    "# num_labels = 81\n",
    "# # Nus Models.\n",
    "# Fx_scene = Fx(feat_dim, 512, 512, latent_dim)\n",
    "# Fe_scene = Fe(num_labels, 512, latent_dim)\n",
    "# Fd_scene = Fd(latent_dim, 512, num_labels, fin_act=torch.sigmoid)\n",
    "\n",
    "# # Scene config\n",
    "feat_dim = 294\n",
    "latent_dim = 5\n",
    "num_labels = 6\n",
    "h_dim=40\n",
    "\n",
    "fx_h_dim=100\n",
    "# Scene models.\n",
    "Fx_scene = Fx(feat_dim, fx_h_dim, fx_h_dim, latent_dim)\n",
    "Fe_scene = Fe(num_labels, h_dim, latent_dim)\n",
    "Fd_scene = Fd(latent_dim, h_dim, num_labels, fin_act=torch.sigmoid)\n",
    "               \n",
    "# Initializing net.\n",
    "net = C2AE(Fx_scene, Fe_scene, Fd_scene, alpha=5, emb_lambda=0.01, latent_dim=latent_dim, device=device)\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "# Doing weight_decay here is eqiv to adding the L2 norm.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0)\n",
    "writer = SummaryWriter(comment='nus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5][1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0597, -0.0095,  0.1325, -0.0084, -0.0076],\n",
       "         [ 0.0205, -0.0122,  0.1384, -0.0084, -0.0072],\n",
       "         [ 0.0272, -0.0123,  0.1544, -0.0075, -0.0082],\n",
       "         [ 0.0390, -0.0132,  0.1572, -0.0086, -0.0077],\n",
       "         [ 0.0316, -0.0126,  0.1361, -0.0071, -0.0080]], device='cuda:0',\n",
       "        grad_fn=<LeakyReluBackward0>),\n",
       " tensor([[-0.0139, -0.0177, -0.0310,  0.1932, -0.0158],\n",
       "         [ 0.0791, -0.0150, -0.0020,  0.1551, -0.0146],\n",
       "         [-0.0029, -0.0071, -0.0064,  0.2071, -0.0088],\n",
       "         [-0.0029, -0.0071, -0.0064,  0.2071, -0.0088],\n",
       "         [-0.0029, -0.0071, -0.0064,  0.2071, -0.0088]], device='cuda:0',\n",
       "        grad_fn=<LeakyReluBackward0>))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.Fx(train_dataset[:5][0].to(device)), net.Fe(train_dataset[:5][1].to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Saving model.\n",
      "Epoch: 0, Loss: 190.78302192687988,  L-Loss: 0.26020116941072047, C-Loss: 38.10456430912018\n",
      "Saving model.\n",
      "Epoch: 1, Loss: 190.54774522781372,  L-Loss: 0.204106422374025, C-Loss: 38.06872779130936\n",
      "Saving model.\n",
      "Epoch: 2, Loss: 190.4091601371765,  L-Loss: 0.23050008225254714, C-Loss: 38.035731852054596\n",
      "Epoch: 3, Loss: 190.55819129943848,  L-Loss: 0.5447562548797578, C-Loss: 38.00268656015396\n",
      "Saving model.\n",
      "Epoch: 4, Loss: 190.32458448410034,  L-Loss: 0.4879605807363987, C-Loss: 37.967325150966644\n",
      "Epoch: 5, Loss: 190.77154684066772,  L-Loss: 1.1393505791202188, C-Loss: 37.926439225673676\n",
      "Epoch: 6, Loss: 190.5609049797058,  L-Loss: 1.0958964959718287, C-Loss: 37.893001437187195\n",
      "Epoch: 7, Loss: 192.15753650665283,  L-Loss: 2.8998786881566048, C-Loss: 37.85153144598007\n",
      "Epoch: 8, Loss: 193.4620599746704,  L-Loss: 4.633823934942484, C-Loss: 37.76564657688141\n",
      "Epoch: 9, Loss: 196.85033178329468,  L-Loss: 8.098749078810215, C-Loss: 37.75031679868698\n",
      "Epoch: 10, Loss: 196.43773937225342,  L-Loss: 8.738051392138004, C-Loss: 37.53993767499924\n",
      "Epoch: 11, Loss: 197.5508337020874,  L-Loss: 11.350176140666008, C-Loss: 37.24013167619705\n",
      "Epoch: 12, Loss: 200.70416498184204,  L-Loss: 15.06392826884985, C-Loss: 37.12804740667343\n",
      "Epoch: 13, Loss: 197.35065126419067,  L-Loss: 11.1695934869349, C-Loss: 37.23621118068695\n",
      "Epoch: 14, Loss: 195.00415420532227,  L-Loss: 10.657593429088593, C-Loss: 36.86931198835373\n",
      "Epoch: 15, Loss: 195.400381565094,  L-Loss: 15.037538036704063, C-Loss: 36.07256853580475\n",
      "Saving model.\n",
      "Epoch: 16, Loss: 185.8987853527069,  L-Loss: 11.007278375327587, C-Loss: 34.978301644325256\n",
      "Epoch: 17, Loss: 189.45579361915588,  L-Loss: 13.466669216752052, C-Loss: 35.19782483577728\n",
      "Epoch: 18, Loss: 189.8496572971344,  L-Loss: 14.23604753613472, C-Loss: 35.122721791267395\n",
      "Epoch: 19, Loss: 194.8568263053894,  L-Loss: 10.874782927334309, C-Loss: 36.796408891677856\n",
      "Epoch: 20, Loss: 187.4043164253235,  L-Loss: 11.549657076597214, C-Loss: 35.1709321141243\n",
      "Saving model.\n",
      "Epoch: 21, Loss: 175.48029589653015,  L-Loss: 9.291845753788948, C-Loss: 33.23768991231918\n",
      "Saving model.\n",
      "Epoch: 22, Loss: 171.4998586177826,  L-Loss: 8.963849425315857, C-Loss: 32.50720202922821\n",
      "Epoch: 23, Loss: 174.37449550628662,  L-Loss: 9.400970458984375, C-Loss: 32.99470525979996\n",
      "Epoch: 24, Loss: 171.7163610458374,  L-Loss: 8.412474676966667, C-Loss: 32.66077733039856\n",
      "Saving model.\n",
      "Epoch: 25, Loss: 168.73332715034485,  L-Loss: 8.697024926543236, C-Loss: 32.007260382175446\n",
      "Epoch: 26, Loss: 170.81898760795593,  L-Loss: 8.32939875870943, C-Loss: 32.49791759252548\n",
      "Saving model.\n",
      "Epoch: 27, Loss: 168.38624382019043,  L-Loss: 7.864294677972794, C-Loss: 32.10438936948776\n",
      "Saving model.\n",
      "Epoch: 28, Loss: 167.9451265335083,  L-Loss: 8.440299049019814, C-Loss: 31.900965452194214\n",
      "Epoch: 29, Loss: 168.8259072303772,  L-Loss: 7.657235316932201, C-Loss: 32.23373430967331\n",
      "Epoch: 30, Loss: 170.54418110847473,  L-Loss: 9.179470181465149, C-Loss: 32.27294212579727\n",
      "Saving model.\n",
      "Epoch: 31, Loss: 165.36331415176392,  L-Loss: 7.221395306289196, C-Loss: 31.628383994102478\n",
      "Epoch: 32, Loss: 168.51161694526672,  L-Loss: 8.813118882477283, C-Loss: 31.939699292182922\n",
      "Epoch: 33, Loss: 165.64183735847473,  L-Loss: 6.84874302148819, C-Loss: 31.758619129657745\n",
      "Epoch: 34, Loss: 166.93161273002625,  L-Loss: 7.988863751292229, C-Loss: 31.788549959659576\n",
      "Epoch: 35, Loss: 167.73929738998413,  L-Loss: 7.494896367192268, C-Loss: 32.04888033866882\n",
      "Epoch: 36, Loss: 169.17041039466858,  L-Loss: 7.0701287016272545, C-Loss: 32.42005652189255\n",
      "Epoch: 37, Loss: 169.52776384353638,  L-Loss: 8.255959436297417, C-Loss: 32.25436097383499\n",
      "Saving model.\n",
      "Epoch: 38, Loss: 163.34361219406128,  L-Loss: 6.539366193115711, C-Loss: 31.36084944009781\n",
      "Epoch: 39, Loss: 165.88252234458923,  L-Loss: 7.763129040598869, C-Loss: 31.62387865781784\n",
      "Epoch: 40, Loss: 164.55518198013306,  L-Loss: 7.354116678237915, C-Loss: 31.44021302461624\n",
      "Epoch: 41, Loss: 163.360289812088,  L-Loss: 6.40736623108387, C-Loss: 31.390584409236908\n",
      "Epoch: 42, Loss: 163.40625047683716,  L-Loss: 7.32587007433176, C-Loss: 31.216075718402863\n",
      "Epoch: 43, Loss: 166.54203367233276,  L-Loss: 7.916376613080502, C-Loss: 31.725131511688232\n",
      "Saving model.\n",
      "Epoch: 44, Loss: 161.9231493473053,  L-Loss: 7.0317569598555565, C-Loss: 30.97827821969986\n",
      "Epoch: 45, Loss: 167.57542991638184,  L-Loss: 6.945530474185944, C-Loss: 32.12597960233688\n",
      "Epoch: 46, Loss: 166.45400094985962,  L-Loss: 6.06179166585207, C-Loss: 32.078441858291626\n",
      "Saving model.\n",
      "Epoch: 47, Loss: 160.88220286369324,  L-Loss: 6.009880341589451, C-Loss: 30.97446471452713\n",
      "Epoch: 48, Loss: 162.38502526283264,  L-Loss: 6.757747493684292, C-Loss: 31.125455796718597\n",
      "Saving model.\n",
      "Epoch: 49, Loss: 158.55724024772644,  L-Loss: 5.490300357341766, C-Loss: 30.613388180732727\n",
      "Epoch: 50, Loss: 159.2781364917755,  L-Loss: 6.202435314655304, C-Loss: 30.615140199661255\n",
      "Saving model.\n",
      "Epoch: 51, Loss: 158.4737799167633,  L-Loss: 5.385372743010521, C-Loss: 30.617681205272675\n",
      "Epoch: 52, Loss: 160.698086977005,  L-Loss: 7.588759899139404, C-Loss: 30.621865153312683\n",
      "Saving model.\n",
      "Epoch: 53, Loss: 156.23414158821106,  L-Loss: 5.383849360048771, C-Loss: 30.17005866765976\n",
      "Epoch: 54, Loss: 161.01431035995483,  L-Loss: 8.107188202440739, C-Loss: 30.581424832344055\n",
      "Saving model.\n",
      "Epoch: 55, Loss: 154.9641716480255,  L-Loss: 4.865750625729561, C-Loss: 30.01968425512314\n",
      "Saving model.\n",
      "Epoch: 56, Loss: 150.29597234725952,  L-Loss: 6.027512766420841, C-Loss: 28.85369199514389\n",
      "Saving model.\n",
      "Epoch: 57, Loss: 142.48312330245972,  L-Loss: 5.134099669754505, C-Loss: 27.46980482339859\n",
      "Epoch: 58, Loss: 148.69809699058533,  L-Loss: 5.54559363424778, C-Loss: 28.63050079345703\n",
      "Epoch: 59, Loss: 146.03645849227905,  L-Loss: 4.771287754178047, C-Loss: 28.25303417444229\n",
      "Epoch: 60, Loss: 144.8737714290619,  L-Loss: 5.141816608607769, C-Loss: 27.94639092683792\n",
      "Saving model.\n",
      "Epoch: 61, Loss: 142.09333181381226,  L-Loss: 4.903674364089966, C-Loss: 27.437931567430496\n",
      "Epoch: 62, Loss: 144.71730375289917,  L-Loss: 5.384881116449833, C-Loss: 27.86648452281952\n",
      "Epoch: 63, Loss: 143.65283918380737,  L-Loss: 5.590236701071262, C-Loss: 27.612520426511765\n",
      "Saving model.\n",
      "Epoch: 64, Loss: 138.5814504623413,  L-Loss: 4.542830727994442, C-Loss: 26.80772390961647\n",
      "Epoch: 65, Loss: 140.79330563545227,  L-Loss: 5.037855174392462, C-Loss: 27.151090264320374\n",
      "Saving model.\n",
      "Epoch: 66, Loss: 137.33433985710144,  L-Loss: 4.3267141580581665, C-Loss: 26.601525247097015\n",
      "Epoch: 67, Loss: 140.52579617500305,  L-Loss: 5.736077085137367, C-Loss: 26.957943469285965\n",
      "Epoch: 68, Loss: 137.33800625801086,  L-Loss: 3.8597916662693024, C-Loss: 26.69564265012741\n",
      "Epoch: 69, Loss: 139.49405312538147,  L-Loss: 4.706985358148813, C-Loss: 26.95741355419159\n",
      "Saving model.\n",
      "Epoch: 70, Loss: 134.27295327186584,  L-Loss: 3.5918323397636414, C-Loss: 26.13622412085533\n",
      "Epoch: 71, Loss: 134.75497484207153,  L-Loss: 4.262729164212942, C-Loss: 26.09844920039177\n",
      "Epoch: 72, Loss: 137.00262665748596,  L-Loss: 3.7029721029102802, C-Loss: 26.659930914640427\n",
      "Saving model.\n",
      "Epoch: 73, Loss: 131.66870594024658,  L-Loss: 3.6623543091118336, C-Loss: 25.601269900798798\n",
      "Epoch: 74, Loss: 134.54050970077515,  L-Loss: 3.884256962686777, C-Loss: 26.13125056028366\n",
      "Epoch: 75, Loss: 131.84572339057922,  L-Loss: 3.7187820859253407, C-Loss: 25.62538844347\n",
      "Epoch: 76, Loss: 132.05504727363586,  L-Loss: 3.6277185194194317, C-Loss: 25.68546560406685\n",
      "Saving model.\n",
      "Epoch: 77, Loss: 130.09843635559082,  L-Loss: 3.606102306395769, C-Loss: 25.29846692085266\n",
      "Epoch: 78, Loss: 136.86243224143982,  L-Loss: 3.7541504204273224, C-Loss: 26.621656239032745\n",
      "Epoch: 79, Loss: 140.67730045318604,  L-Loss: 3.72060739248991, C-Loss: 27.39133858680725\n",
      "Epoch: 80, Loss: 130.5590217113495,  L-Loss: 4.021764989942312, C-Loss: 25.3074511885643\n",
      "Saving model.\n",
      "Epoch: 81, Loss: 129.1085319519043,  L-Loss: 3.0016234144568443, C-Loss: 25.221381813287735\n",
      "Epoch: 82, Loss: 132.67218160629272,  L-Loss: 3.580198984593153, C-Loss: 25.818396508693695\n",
      "Epoch: 83, Loss: 135.4249243736267,  L-Loss: 3.2253315411508083, C-Loss: 26.439918756484985\n",
      "Saving model.\n",
      "Epoch: 84, Loss: 127.27887225151062,  L-Loss: 3.744469106197357, C-Loss: 24.706880778074265\n",
      "Epoch: 85, Loss: 129.46695470809937,  L-Loss: 3.092317081987858, C-Loss: 25.274927347898483\n",
      "Epoch: 86, Loss: 139.92771649360657,  L-Loss: 3.1048367470502853, C-Loss: 27.3645761013031\n",
      "Epoch: 87, Loss: 128.2067995071411,  L-Loss: 3.720746897161007, C-Loss: 24.897210508584976\n",
      "Epoch: 88, Loss: 136.10677552223206,  L-Loss: 2.970790583640337, C-Loss: 26.627197057008743\n",
      "Epoch: 89, Loss: 129.14033842086792,  L-Loss: 3.6595867052674294, C-Loss: 25.096150010824203\n",
      "Saving model.\n",
      "Epoch: 90, Loss: 125.14895439147949,  L-Loss: 2.7745137475430965, C-Loss: 24.47488847374916\n",
      "Epoch: 91, Loss: 134.02361345291138,  L-Loss: 4.427289616316557, C-Loss: 25.91926497220993\n",
      "Epoch: 92, Loss: 127.75467467308044,  L-Loss: 2.6843503415584564, C-Loss: 25.014064848423004\n",
      "Epoch: 93, Loss: 127.79628610610962,  L-Loss: 3.666936229914427, C-Loss: 24.825869768857956\n",
      "Saving model.\n",
      "Epoch: 94, Loss: 123.58101105690002,  L-Loss: 2.4645359963178635, C-Loss: 24.223294973373413\n",
      "Epoch: 95, Loss: 126.12641835212708,  L-Loss: 2.9260093569755554, C-Loss: 24.640081882476807\n",
      "Epoch: 96, Loss: 124.69506859779358,  L-Loss: 2.422956036403775, C-Loss: 24.454422146081924\n",
      "Saving model.\n",
      "Epoch: 97, Loss: 122.68864512443542,  L-Loss: 2.7334854528307915, C-Loss: 23.991032272577286\n",
      "Saving model.\n",
      "Epoch: 98, Loss: 120.38916563987732,  L-Loss: 2.174046767875552, C-Loss: 23.643023818731308\n",
      "Epoch: 99, Loss: 128.50185203552246,  L-Loss: 2.694339131936431, C-Loss: 25.16150262951851\n",
      "Epoch: 100, Loss: 123.22513842582703,  L-Loss: 2.4247074499726295, C-Loss: 24.160085797309875\n",
      "Epoch: 101, Loss: 122.09188723564148,  L-Loss: 2.9174118489027023, C-Loss: 23.834895133972168\n",
      "Epoch: 102, Loss: 126.45838141441345,  L-Loss: 2.665698977187276, C-Loss: 24.75853669643402\n",
      "Epoch: 103, Loss: 123.03426170349121,  L-Loss: 2.551246799528599, C-Loss: 24.096602886915207\n",
      "Epoch: 104, Loss: 122.91256141662598,  L-Loss: 2.1344854291528463, C-Loss: 24.15561518073082\n",
      "Epoch: 105, Loss: 121.05620503425598,  L-Loss: 2.7272797003388405, C-Loss: 23.66578507423401\n",
      "Saving model.\n",
      "Epoch: 106, Loss: 118.88870334625244,  L-Loss: 1.9818322248756886, C-Loss: 23.381374031305313\n",
      "Epoch: 107, Loss: 126.8248701095581,  L-Loss: 2.8334337044507265, C-Loss: 24.798287481069565\n",
      "Epoch: 108, Loss: 120.36706137657166,  L-Loss: 1.9534413069486618, C-Loss: 23.682724237442017\n",
      "Epoch: 109, Loss: 120.5473575592041,  L-Loss: 2.934660844504833, C-Loss: 23.522539615631104\n",
      "Epoch: 110, Loss: 118.90193438529968,  L-Loss: 1.9652974344789982, C-Loss: 23.387327671051025\n",
      "Epoch: 111, Loss: 123.38529133796692,  L-Loss: 2.3237791936844587, C-Loss: 24.212302803993225\n",
      "Saving model.\n",
      "Epoch: 112, Loss: 116.72263479232788,  L-Loss: 2.439513470977545, C-Loss: 22.856624454259872\n",
      "Epoch: 113, Loss: 131.38683128356934,  L-Loss: 2.5750055015087128, C-Loss: 25.76236492395401\n",
      "Epoch: 114, Loss: 122.58815431594849,  L-Loss: 2.6367878764867783, C-Loss: 23.990273594856262\n",
      "Epoch: 115, Loss: 121.47277426719666,  L-Loss: 3.970253601670265, C-Loss: 23.50050437450409\n",
      "Epoch: 116, Loss: 145.44951486587524,  L-Loss: 4.529285825788975, C-Loss: 28.184045612812042\n",
      "Epoch: 117, Loss: 122.07658553123474,  L-Loss: 3.7095529101788998, C-Loss: 23.673406541347504\n",
      "Epoch: 118, Loss: 153.75225520133972,  L-Loss: 3.6516346633434296, C-Loss: 30.02012425661087\n",
      "Epoch: 119, Loss: 120.7503228187561,  L-Loss: 2.530104950070381, C-Loss: 23.644043624401093\n",
      "Epoch: 120, Loss: 125.49346923828125,  L-Loss: 2.180331837385893, C-Loss: 24.662627667188644\n",
      "Epoch: 121, Loss: 118.74694013595581,  L-Loss: 2.34041659347713, C-Loss: 23.281304866075516\n",
      "Saving model.\n",
      "Epoch: 122, Loss: 116.43316221237183,  L-Loss: 1.549593023955822, C-Loss: 22.97671401500702\n",
      "Epoch: 123, Loss: 117.23009443283081,  L-Loss: 1.9030818678438663, C-Loss: 23.065402656793594\n",
      "Saving model.\n",
      "Epoch: 124, Loss: 112.06363463401794,  L-Loss: 1.5406737830489874, C-Loss: 22.10459214448929\n",
      "Epoch: 125, Loss: 115.3058397769928,  L-Loss: 1.7889688238501549, C-Loss: 22.70337411761284\n",
      "Epoch: 126, Loss: 112.42155241966248,  L-Loss: 1.5044019185006618, C-Loss: 22.183430045843124\n",
      "Epoch: 127, Loss: 112.50538277626038,  L-Loss: 1.7544776443392038, C-Loss: 22.150181114673615\n",
      "Saving model.\n",
      "Epoch: 128, Loss: 110.47528743743896,  L-Loss: 1.437168437987566, C-Loss: 21.807623833417892\n",
      "Epoch: 129, Loss: 113.39812564849854,  L-Loss: 1.7398532144725323, C-Loss: 22.331654578447342\n",
      "Saving model.\n",
      "Epoch: 130, Loss: 108.61953067779541,  L-Loss: 1.412809168919921, C-Loss: 21.4413443505764\n",
      "Epoch: 131, Loss: 111.56226420402527,  L-Loss: 1.6439329646527767, C-Loss: 21.983666270971298\n",
      "Epoch: 132, Loss: 110.79872059822083,  L-Loss: 1.4796127937734127, C-Loss: 21.86382156610489\n",
      "Epoch: 133, Loss: 109.18779397010803,  L-Loss: 1.6664125937968493, C-Loss: 21.504276365041733\n",
      "Epoch: 134, Loss: 109.11230707168579,  L-Loss: 1.308965915814042, C-Loss: 21.56066843867302\n",
      "Epoch: 135, Loss: 113.043701171875,  L-Loss: 1.689230538904667, C-Loss: 22.27089437842369\n",
      "Saving model.\n",
      "Epoch: 136, Loss: 106.51258659362793,  L-Loss: 1.4110126961022615, C-Loss: 21.02031460404396\n",
      "Epoch: 137, Loss: 112.02276301383972,  L-Loss: 1.5593740437179804, C-Loss: 22.092677742242813\n",
      "Epoch: 138, Loss: 109.98354363441467,  L-Loss: 1.4517666883766651, C-Loss: 21.706355422735214\n",
      "Epoch: 139, Loss: 107.89480578899384,  L-Loss: 1.768398530781269, C-Loss: 21.22528150677681\n",
      "Epoch: 140, Loss: 115.44335913658142,  L-Loss: 1.768834287300706, C-Loss: 22.73490497469902\n",
      "Epoch: 141, Loss: 110.139333486557,  L-Loss: 1.6539898347109556, C-Loss: 21.69706892967224\n",
      "Epoch: 142, Loss: 114.11048769950867,  L-Loss: 1.284941878169775, C-Loss: 22.565109074115753\n",
      "Saving model.\n",
      "Epoch: 143, Loss: 105.4945981502533,  L-Loss: 1.4312685802578926, C-Loss: 20.812666028738022\n",
      "Saving model.\n",
      "Epoch: 144, Loss: 104.18671011924744,  L-Loss: 1.3307083658874035, C-Loss: 20.571200340986252\n",
      "Epoch: 145, Loss: 109.59799194335938,  L-Loss: 1.4926838129758835, C-Loss: 21.621061474084854\n",
      "Epoch: 146, Loss: 104.69938731193542,  L-Loss: 1.2333527030423284, C-Loss: 20.693206906318665\n",
      "Epoch: 147, Loss: 105.81975173950195,  L-Loss: 1.2236226918175817, C-Loss: 20.91922563314438\n",
      "Saving model.\n",
      "Epoch: 148, Loss: 103.8824815750122,  L-Loss: 1.2047473974525928, C-Loss: 20.53554692864418\n",
      "Saving model.\n",
      "Epoch: 149, Loss: 102.46016025543213,  L-Loss: 1.4599586483091116, C-Loss: 20.200040131807327\n",
      "Epoch: 150, Loss: 108.17625641822815,  L-Loss: 1.2364813424646854, C-Loss: 21.38795492053032\n",
      "Epoch: 151, Loss: 104.53950524330139,  L-Loss: 1.318513559177518, C-Loss: 20.64419862627983\n",
      "Saving model.\n",
      "Epoch: 152, Loss: 102.11640119552612,  L-Loss: 1.0269114030525088, C-Loss: 20.217898041009903\n",
      "Saving model.\n",
      "Epoch: 153, Loss: 100.79745638370514,  L-Loss: 1.1818048181012273, C-Loss: 19.92313042283058\n",
      "Epoch: 154, Loss: 101.80802202224731,  L-Loss: 1.0643428638577461, C-Loss: 20.148735880851746\n",
      "Epoch: 155, Loss: 103.73833584785461,  L-Loss: 1.2488889070227742, C-Loss: 20.49788922071457\n",
      "Saving model.\n",
      "Epoch: 156, Loss: 99.99038326740265,  L-Loss: 1.058206394314766, C-Loss: 19.786435395479202\n",
      "Epoch: 157, Loss: 105.50956177711487,  L-Loss: 1.323405135422945, C-Loss: 20.837231189012527\n",
      "Saving model.\n",
      "Epoch: 158, Loss: 99.88123214244843,  L-Loss: 1.1858235308900476, C-Loss: 19.739081621170044\n",
      "Epoch: 159, Loss: 105.28489995002747,  L-Loss: 1.3783550318330526, C-Loss: 20.781308829784393\n",
      "Epoch: 160, Loss: 102.24695348739624,  L-Loss: 1.1070023830980062, C-Loss: 20.227990299463272\n",
      "Epoch: 161, Loss: 100.09288692474365,  L-Loss: 1.157497282139957, C-Loss: 19.787078022956848\n",
      "Saving model.\n",
      "Epoch: 162, Loss: 99.2159948348999,  L-Loss: 1.3466868717223406, C-Loss: 19.573861718177795\n",
      "Saving model.\n",
      "Epoch: 163, Loss: 96.56683361530304,  L-Loss: 1.4095688732340932, C-Loss: 19.03145283460617\n",
      "Epoch: 164, Loss: 130.86438918113708,  L-Loss: 2.288560938090086, C-Loss: 25.715165674686432\n",
      "Epoch: 165, Loss: 103.86732220649719,  L-Loss: 1.9798752777278423, C-Loss: 20.377489417791367\n",
      "Epoch: 166, Loss: 117.41174030303955,  L-Loss: 1.281390080228448, C-Loss: 23.226069927215576\n",
      "Epoch: 167, Loss: 99.17503595352173,  L-Loss: 1.7783353989943862, C-Loss: 19.4793401658535\n",
      "Epoch: 168, Loss: 97.9341344833374,  L-Loss: 1.0877800835296512, C-Loss: 19.36927106976509\n",
      "Epoch: 169, Loss: 99.12173342704773,  L-Loss: 1.0479670036584139, C-Loss: 19.614753305912018\n",
      "Saving model.\n",
      "Epoch: 170, Loss: 96.28493845462799,  L-Loss: 0.9528846740722656, C-Loss: 19.066410899162292\n",
      "Saving model.\n",
      "Epoch: 171, Loss: 95.11199033260345,  L-Loss: 0.8905324144288898, C-Loss: 18.84429180622101\n",
      "Saving model.\n",
      "Epoch: 172, Loss: 93.31463503837585,  L-Loss: 0.9528867872431874, C-Loss: 18.47234982252121\n",
      "Saving model.\n",
      "Epoch: 173, Loss: 93.1282969713211,  L-Loss: 0.8935975078493357, C-Loss: 18.446939915418625\n",
      "Epoch: 174, Loss: 96.37739658355713,  L-Loss: 0.9072851724922657, C-Loss: 19.0940223634243\n",
      "Saving model.\n",
      "Epoch: 175, Loss: 92.10982668399811,  L-Loss: 0.9475085539743304, C-Loss: 18.232463657855988\n",
      "Epoch: 176, Loss: 93.8482301235199,  L-Loss: 0.7852450162172318, C-Loss: 18.612597197294235\n",
      "Epoch: 177, Loss: 92.72537422180176,  L-Loss: 0.8435170352458954, C-Loss: 18.376371532678604\n",
      "Saving model.\n",
      "Epoch: 178, Loss: 90.58837103843689,  L-Loss: 0.8942352337762713, C-Loss: 17.938827097415924\n",
      "Epoch: 179, Loss: 94.65176677703857,  L-Loss: 0.8215142311528325, C-Loss: 18.76605051755905\n",
      "Epoch: 180, Loss: 90.86746299266815,  L-Loss: 0.776600225828588, C-Loss: 18.018172591924667\n",
      "Saving model.\n",
      "Epoch: 181, Loss: 90.17507064342499,  L-Loss: 0.7286864798516035, C-Loss: 17.889276772737503\n",
      "Epoch: 182, Loss: 95.30675113201141,  L-Loss: 0.9475033218041062, C-Loss: 18.871849596500397\n",
      "Saving model.\n",
      "Epoch: 183, Loss: 89.37641310691833,  L-Loss: 0.8324284814298153, C-Loss: 17.708796739578247\n",
      "Epoch: 184, Loss: 96.80710995197296,  L-Loss: 0.894822827540338, C-Loss: 19.182457506656647\n",
      "Epoch: 185, Loss: 99.60868263244629,  L-Loss: 1.0109069608151913, C-Loss: 19.71955493092537\n",
      "Saving model.\n",
      "Epoch: 186, Loss: 89.15929889678955,  L-Loss: 1.0123130306601524, C-Loss: 17.62939715385437\n",
      "Epoch: 187, Loss: 108.87968230247498,  L-Loss: 1.2263763640075922, C-Loss: 21.530661433935165\n",
      "Epoch: 188, Loss: 91.12158143520355,  L-Loss: 0.9380856156349182, C-Loss: 18.036699175834656\n",
      "Epoch: 189, Loss: 96.74657475948334,  L-Loss: 0.6889370232820511, C-Loss: 19.211527556180954\n",
      "Epoch: 190, Loss: 90.05000460147858,  L-Loss: 0.7900332398712635, C-Loss: 17.85199424624443\n",
      "Saving model.\n",
      "Epoch: 191, Loss: 88.53030467033386,  L-Loss: 0.7313207238912582, C-Loss: 17.559796631336212\n",
      "Epoch: 192, Loss: 92.30277180671692,  L-Loss: 0.8407495357096195, C-Loss: 18.29240444302559\n",
      "Epoch: 193, Loss: 88.57197999954224,  L-Loss: 0.7287491485476494, C-Loss: 17.568646013736725\n",
      "Epoch: 194, Loss: 91.07356774806976,  L-Loss: 0.6603243760764599, C-Loss: 18.08264857530594\n",
      "Epoch: 195, Loss: 91.51505446434021,  L-Loss: 0.6974497940391302, C-Loss: 18.163520872592926\n",
      "Saving model.\n",
      "Epoch: 196, Loss: 87.12342131137848,  L-Loss: 0.7126004686579108, C-Loss: 17.282164245843887\n",
      "Epoch: 197, Loss: 93.20260047912598,  L-Loss: 0.6891644187271595, C-Loss: 18.50268703699112\n",
      "Epoch: 198, Loss: 90.54461431503296,  L-Loss: 0.7107414663769305, C-Loss: 17.966774374246597\n",
      "Epoch: 199, Loss: 97.90574264526367,  L-Loss: 1.0240826457738876, C-Loss: 19.376332104206085\n",
      "Epoch: 200, Loss: 91.9982795715332,  L-Loss: 0.9320782767608762, C-Loss: 18.213240176439285\n",
      "Epoch: 201, Loss: 88.9422219991684,  L-Loss: 0.9199770549312234, C-Loss: 17.604448974132538\n",
      "Epoch: 202, Loss: 110.28905737400055,  L-Loss: 1.0986144519411027, C-Loss: 21.838088870048523\n",
      "Epoch: 203, Loss: 119.24711084365845,  L-Loss: 1.3319048965349793, C-Loss: 23.58304101228714\n",
      "Epoch: 204, Loss: 100.10038995742798,  L-Loss: 1.144503803923726, C-Loss: 19.79117715358734\n",
      "Epoch: 205, Loss: 108.31394505500793,  L-Loss: 1.25692955031991, C-Loss: 21.411403208971024\n",
      "Epoch: 206, Loss: 95.02135932445526,  L-Loss: 1.0158584509044886, C-Loss: 18.80110028386116\n",
      "Epoch: 207, Loss: 92.61624753475189,  L-Loss: 0.6230292967520654, C-Loss: 18.39864358305931\n",
      "Epoch: 208, Loss: 87.6659369468689,  L-Loss: 0.6003785850480199, C-Loss: 17.413111746311188\n",
      "Epoch: 209, Loss: 89.63493919372559,  L-Loss: 0.6811914704740047, C-Loss: 17.790749311447144\n",
      "Epoch: 210, Loss: 89.60435032844543,  L-Loss: 0.6061152564361691, C-Loss: 17.799647003412247\n",
      "Epoch: 211, Loss: 91.2492436170578,  L-Loss: 0.7362806140445173, C-Loss: 18.102592647075653\n",
      "Epoch: 212, Loss: 90.6226875782013,  L-Loss: 0.5484917447902262, C-Loss: 18.01483941078186\n",
      "Epoch: 213, Loss: 90.26588547229767,  L-Loss: 0.5963506177067757, C-Loss: 17.933906942605972\n",
      "Epoch: 214, Loss: 90.01295173168182,  L-Loss: 0.6134965419769287, C-Loss: 17.879890859127045\n",
      "Epoch: 215, Loss: 90.95219850540161,  L-Loss: 0.7007567002438009, C-Loss: 18.050288438796997\n",
      "Epoch: 216, Loss: 97.23231828212738,  L-Loss: 0.699669364374131, C-Loss: 19.30653005838394\n",
      "Epoch: 217, Loss: 93.33320355415344,  L-Loss: 0.8782522198744118, C-Loss: 18.49099025130272\n",
      "Epoch: 218, Loss: 104.29551720619202,  L-Loss: 0.718565808609128, C-Loss: 20.715390503406525\n",
      "Epoch: 219, Loss: 90.80179798603058,  L-Loss: 0.8631780501455069, C-Loss: 17.98772406578064\n",
      "Epoch: 220, Loss: 91.14440095424652,  L-Loss: 0.9300824487581849, C-Loss: 18.042863845825195\n",
      "Epoch: 221, Loss: 111.38652527332306,  L-Loss: 1.384168797172606, C-Loss: 22.000471264123917\n",
      "Epoch: 222, Loss: 98.30523908138275,  L-Loss: 1.0780788799747825, C-Loss: 19.44543182849884\n",
      "Epoch: 223, Loss: 95.98723316192627,  L-Loss: 0.7357205459848046, C-Loss: 19.05030244588852\n",
      "Epoch: 224, Loss: 88.93391561508179,  L-Loss: 0.8459436856210232, C-Loss: 17.617594450712204\n",
      "Epoch: 225, Loss: 92.20894253253937,  L-Loss: 0.6896178666502237, C-Loss: 18.303864926099777\n",
      "Epoch: 226, Loss: 94.11518704891205,  L-Loss: 0.779203868471086, C-Loss: 18.667196720838547\n",
      "Saving model.\n",
      "Epoch: 227, Loss: 86.80261731147766,  L-Loss: 0.5920456293970346, C-Loss: 17.242114335298538\n",
      "Epoch: 228, Loss: 88.33687388896942,  L-Loss: 0.5683887796476483, C-Loss: 17.553697049617767\n",
      "Saving model.\n",
      "Epoch: 229, Loss: 85.59295606613159,  L-Loss: 0.4740355685353279, C-Loss: 17.023784071207047\n",
      "Saving model.\n",
      "Epoch: 230, Loss: 85.36383426189423,  L-Loss: 0.56112315133214, C-Loss: 16.960542172193527\n",
      "Saving model.\n",
      "Epoch: 231, Loss: 83.94130563735962,  L-Loss: 0.5585175375454128, C-Loss: 16.67655748128891\n",
      "Epoch: 232, Loss: 91.23589265346527,  L-Loss: 0.6131829782389104, C-Loss: 18.12454190850258\n",
      "Saving model.\n",
      "Epoch: 233, Loss: 83.4601354598999,  L-Loss: 0.5805846890434623, C-Loss: 16.57591012120247\n",
      "Epoch: 234, Loss: 87.95100820064545,  L-Loss: 0.5152850532904267, C-Loss: 17.48714455962181\n",
      "Epoch: 235, Loss: 85.39815211296082,  L-Loss: 0.568684212397784, C-Loss: 16.965893477201462\n",
      "Saving model.\n",
      "Epoch: 236, Loss: 82.65679836273193,  L-Loss: 0.40990256145596504, C-Loss: 16.44937914609909\n",
      "Epoch: 237, Loss: 82.84841811656952,  L-Loss: 0.45702641597017646, C-Loss: 16.47827836871147\n",
      "Saving model.\n",
      "Epoch: 238, Loss: 82.16759777069092,  L-Loss: 0.5180857894010842, C-Loss: 16.32990238070488\n",
      "Epoch: 239, Loss: 85.7526388168335,  L-Loss: 0.5177303906530142, C-Loss: 17.046981900930405\n",
      "Saving model.\n",
      "Epoch: 240, Loss: 81.40342772006989,  L-Loss: 0.5771306706592441, C-Loss: 16.165259420871735\n",
      "Epoch: 241, Loss: 85.29913222789764,  L-Loss: 0.49391996255144477, C-Loss: 16.961042374372482\n",
      "Epoch: 242, Loss: 81.62283766269684,  L-Loss: 0.5392077411524951, C-Loss: 16.216726064682007\n",
      "Epoch: 243, Loss: 83.30532562732697,  L-Loss: 0.46403425373136997, C-Loss: 16.56825828552246\n",
      "Epoch: 244, Loss: 85.18967795372009,  L-Loss: 0.4826628929004073, C-Loss: 16.941402971744537\n",
      "Saving model.\n",
      "Epoch: 245, Loss: 81.37628269195557,  L-Loss: 0.49735801899805665, C-Loss: 16.17578488588333\n",
      "Epoch: 246, Loss: 83.21631073951721,  L-Loss: 0.4309586971066892, C-Loss: 16.557070314884186\n",
      "Saving model.\n",
      "Epoch: 247, Loss: 79.98536205291748,  L-Loss: 0.44873156771063805, C-Loss: 15.907326132059097\n",
      "Epoch: 248, Loss: 81.88093996047974,  L-Loss: 0.39771586284041405, C-Loss: 16.296644777059555\n",
      "Saving model.\n",
      "Epoch: 249, Loss: 79.91663551330566,  L-Loss: 0.377359420992434, C-Loss: 15.907855242490768\n",
      "Epoch: 250, Loss: 80.10595774650574,  L-Loss: 0.36496638925746083, C-Loss: 15.9481982588768\n",
      "Epoch: 251, Loss: 80.08247792720795,  L-Loss: 0.3525434355251491, C-Loss: 15.945986777544022\n",
      "Saving model.\n",
      "Epoch: 252, Loss: 79.49101054668427,  L-Loss: 0.40142421796917915, C-Loss: 15.817917197942734\n",
      "Epoch: 253, Loss: 80.23678183555603,  L-Loss: 0.3625045637600124, C-Loss: 15.974855422973633\n",
      "Saving model.\n",
      "Epoch: 254, Loss: 79.01528751850128,  L-Loss: 0.3372080628760159, C-Loss: 15.735615968704224\n",
      "Epoch: 255, Loss: 79.26247763633728,  L-Loss: 0.3184454655274749, C-Loss: 15.788806647062302\n",
      "Saving model.\n",
      "Epoch: 256, Loss: 78.58965420722961,  L-Loss: 0.319724858738482, C-Loss: 15.653985738754272\n",
      "Epoch: 257, Loss: 78.73569822311401,  L-Loss: 0.31931938929483294, C-Loss: 15.683275610208511\n",
      "Saving model.\n",
      "Epoch: 258, Loss: 78.3016585111618,  L-Loss: 0.3110740468837321, C-Loss: 15.598116874694824\n",
      "Saving model.\n",
      "Epoch: 259, Loss: 78.01389563083649,  L-Loss: 0.31111279875040054, C-Loss: 15.540556520223618\n",
      "Saving model.\n",
      "Epoch: 260, Loss: 77.85776793956757,  L-Loss: 0.31939168088138103, C-Loss: 15.507675290107727\n",
      "Epoch: 261, Loss: 78.61479759216309,  L-Loss: 0.3461377853527665, C-Loss: 15.653731882572174\n",
      "Epoch: 262, Loss: 78.00412654876709,  L-Loss: 0.3677668562158942, C-Loss: 15.527272075414658\n",
      "Epoch: 263, Loss: 78.82200729846954,  L-Loss: 0.3628378459252417, C-Loss: 15.691833913326263\n",
      "Epoch: 264, Loss: 78.56215953826904,  L-Loss: 0.38940144097432494, C-Loss: 15.634551584720612\n",
      "Epoch: 265, Loss: 79.95821392536163,  L-Loss: 0.44483931409195065, C-Loss: 15.902674943208694\n",
      "Epoch: 266, Loss: 87.83239126205444,  L-Loss: 0.5432805819436908, C-Loss: 17.457822054624557\n",
      "Epoch: 267, Loss: 103.04911744594574,  L-Loss: 0.7856918391771615, C-Loss: 20.452684968709946\n",
      "Epoch: 268, Loss: 90.15463697910309,  L-Loss: 0.8552764607593417, C-Loss: 17.859872102737427\n",
      "Epoch: 269, Loss: 96.520268201828,  L-Loss: 0.5985044129192829, C-Loss: 19.184352844953537\n",
      "Epoch: 270, Loss: 87.27388787269592,  L-Loss: 0.6296587521210313, C-Loss: 17.32884579896927\n",
      "Epoch: 271, Loss: 84.88233470916748,  L-Loss: 0.44790143007412553, C-Loss: 16.88688662648201\n",
      "Epoch: 272, Loss: 79.03849279880524,  L-Loss: 0.32737727323547006, C-Loss: 15.742223083972931\n",
      "Epoch: 273, Loss: 78.67619454860687,  L-Loss: 0.3650043122470379, C-Loss: 15.66223806142807\n",
      "Epoch: 274, Loss: 78.65106248855591,  L-Loss: 0.36872416781261563, C-Loss: 15.656467646360397\n",
      "Epoch: 275, Loss: 78.44119107723236,  L-Loss: 0.3424887019209564, C-Loss: 15.619740515947342\n",
      "Epoch: 276, Loss: 81.2605414390564,  L-Loss: 0.438649695366621, C-Loss: 16.164378255605698\n",
      "Epoch: 277, Loss: 84.42880487442017,  L-Loss: 0.49328302778303623, C-Loss: 16.787104219198227\n",
      "Epoch: 278, Loss: 78.88338887691498,  L-Loss: 0.4826968964189291, C-Loss: 15.68013858795166\n",
      "Epoch: 279, Loss: 87.22203528881073,  L-Loss: 0.42830777540802956, C-Loss: 17.358745634555817\n",
      "Epoch: 280, Loss: 80.03182220458984,  L-Loss: 0.4451881484128535, C-Loss: 15.917326867580414\n",
      "Epoch: 281, Loss: 82.65522110462189,  L-Loss: 0.41874956618994474, C-Loss: 16.447294503450394\n",
      "Epoch: 282, Loss: 78.78434348106384,  L-Loss: 0.36947871278971434, C-Loss: 15.682972878217697\n",
      "Epoch: 283, Loss: 79.88039982318878,  L-Loss: 0.4265705239959061, C-Loss: 15.890765845775604\n",
      "Epoch: 284, Loss: 79.71604895591736,  L-Loss: 0.4610522179864347, C-Loss: 15.850999265909195\n",
      "Epoch: 285, Loss: 79.71540009975433,  L-Loss: 0.34475162043236196, C-Loss: 15.874129682779312\n",
      "Epoch: 286, Loss: 83.14953196048737,  L-Loss: 0.50912347715348, C-Loss: 16.528081864118576\n",
      "Epoch: 287, Loss: 91.98718976974487,  L-Loss: 0.5271804155781865, C-Loss: 18.2920019030571\n",
      "Epoch: 288, Loss: 80.7605652809143,  L-Loss: 0.39198923064395785, C-Loss: 16.073715299367905\n",
      "Epoch: 289, Loss: 78.47789001464844,  L-Loss: 0.28602812020108104, C-Loss: 15.638372600078583\n",
      "Saving model.\n",
      "Epoch: 290, Loss: 76.7427966594696,  L-Loss: 0.26863544806838036, C-Loss: 15.294832170009613\n",
      "Epoch: 291, Loss: 79.2234491109848,  L-Loss: 0.26222736900672317, C-Loss: 15.7922443151474\n",
      "Saving model.\n",
      "Epoch: 292, Loss: 76.57185447216034,  L-Loss: 0.2385170510970056, C-Loss: 15.266667515039444\n",
      "Saving model.\n",
      "Epoch: 293, Loss: 76.36758399009705,  L-Loss: 0.2780644749291241, C-Loss: 15.217903912067413\n",
      "Epoch: 294, Loss: 80.01047015190125,  L-Loss: 0.3244441938586533, C-Loss: 15.937205344438553\n",
      "Epoch: 295, Loss: 78.43736231327057,  L-Loss: 0.30168097699061036, C-Loss: 15.62713634967804\n",
      "Epoch: 296, Loss: 76.92229998111725,  L-Loss: 0.43704578978940845, C-Loss: 15.297050833702087\n",
      "Epoch: 297, Loss: 91.67253458499908,  L-Loss: 0.5259884907864034, C-Loss: 18.229309111833572\n",
      "Epoch: 298, Loss: 78.49303567409515,  L-Loss: 0.4551282301545143, C-Loss: 15.607581377029419\n",
      "Epoch: 299, Loss: 79.12192332744598,  L-Loss: 0.3749767951667309, C-Loss: 15.749389410018921\n",
      "Epoch: 300, Loss: 76.40476274490356,  L-Loss: 0.2866950840689242, C-Loss: 15.223613619804382\n",
      "Saving model.\n",
      "Epoch: 301, Loss: 75.80666697025299,  L-Loss: 0.2458834913559258, C-Loss: 15.112156689167023\n",
      "Epoch: 302, Loss: 76.79974007606506,  L-Loss: 0.28179295524023473, C-Loss: 15.303589463233948\n",
      "Epoch: 303, Loss: 75.83870232105255,  L-Loss: 0.2808153638616204, C-Loss: 15.111577242612839\n",
      "Epoch: 304, Loss: 76.93419516086578,  L-Loss: 0.267220810521394, C-Loss: 15.333394944667816\n",
      "Epoch: 305, Loss: 75.85927557945251,  L-Loss: 0.2609078139066696, C-Loss: 15.11967357993126\n",
      "Epoch: 306, Loss: 76.12940239906311,  L-Loss: 0.23511191294528544, C-Loss: 15.178858160972595\n",
      "Epoch: 307, Loss: 75.99527728557587,  L-Loss: 0.23182615730911493, C-Loss: 15.152690201997757\n",
      "Epoch: 308, Loss: 75.94624090194702,  L-Loss: 0.26023469725623727, C-Loss: 15.13720127940178\n",
      "Saving model.\n",
      "Epoch: 309, Loss: 75.60565412044525,  L-Loss: 0.2903938344679773, C-Loss: 15.063051998615265\n",
      "Epoch: 310, Loss: 76.58212149143219,  L-Loss: 0.31016362295486033, C-Loss: 15.254391580820084\n",
      "Epoch: 311, Loss: 76.28429710865021,  L-Loss: 0.3344648773781955, C-Loss: 15.189966350793839\n",
      "Epoch: 312, Loss: 83.55002272129059,  L-Loss: 0.410649984376505, C-Loss: 16.627874732017517\n",
      "Epoch: 313, Loss: 76.26472854614258,  L-Loss: 0.4680470870807767, C-Loss: 15.159336268901825\n",
      "Epoch: 314, Loss: 82.38945627212524,  L-Loss: 0.4055092348717153, C-Loss: 16.39678943157196\n",
      "Epoch: 315, Loss: 76.63779664039612,  L-Loss: 0.31579735968261957, C-Loss: 15.264400005340576\n",
      "Epoch: 316, Loss: 78.91550016403198,  L-Loss: 0.26575260795652866, C-Loss: 15.729949593544006\n",
      "Epoch: 317, Loss: 78.54628145694733,  L-Loss: 0.27443537255749106, C-Loss: 15.654369235038757\n",
      "Epoch: 318, Loss: 77.63277471065521,  L-Loss: 0.3050435178447515, C-Loss: 15.46554633975029\n",
      "Saving model.\n",
      "Epoch: 319, Loss: 75.46196436882019,  L-Loss: 0.24499484058469534, C-Loss: 15.04339388012886\n",
      "Saving model.\n",
      "Epoch: 320, Loss: 75.31011855602264,  L-Loss: 0.25251057278364897, C-Loss: 15.01152166724205\n",
      "Epoch: 321, Loss: 75.48823082447052,  L-Loss: 0.24536836287006736, C-Loss: 15.04857251048088\n",
      "Epoch: 322, Loss: 75.49850499629974,  L-Loss: 0.2184035729151219, C-Loss: 15.056020349264145\n",
      "Saving model.\n",
      "Epoch: 323, Loss: 74.83890962600708,  L-Loss: 0.23801790224388242, C-Loss: 14.920178264379501\n",
      "Epoch: 324, Loss: 75.09090042114258,  L-Loss: 0.2210897421464324, C-Loss: 14.973962098360062\n",
      "Saving model.\n",
      "Epoch: 325, Loss: 74.56501531600952,  L-Loss: 0.23524452093988657, C-Loss: 14.865954101085663\n",
      "Epoch: 326, Loss: 75.80232214927673,  L-Loss: 0.22471524984575808, C-Loss: 15.115521281957626\n",
      "Epoch: 327, Loss: 74.6455751657486,  L-Loss: 0.22368713561445475, C-Loss: 14.884377509355545\n",
      "Epoch: 328, Loss: 74.57009196281433,  L-Loss: 0.2031784364953637, C-Loss: 14.87338274717331\n",
      "Saving model.\n",
      "Epoch: 329, Loss: 74.11255156993866,  L-Loss: 0.2233568630181253, C-Loss: 14.777838915586472\n",
      "Epoch: 330, Loss: 77.86274480819702,  L-Loss: 0.2817956448998302, C-Loss: 15.516189873218536\n",
      "Epoch: 331, Loss: 74.13659334182739,  L-Loss: 0.3087196433916688, C-Loss: 14.765574753284454\n",
      "Epoch: 332, Loss: 84.10519552230835,  L-Loss: 0.37740016635507345, C-Loss: 16.745559126138687\n",
      "Epoch: 333, Loss: 76.10493803024292,  L-Loss: 0.2655443586409092, C-Loss: 15.16787874698639\n",
      "Epoch: 334, Loss: 78.50361859798431,  L-Loss: 0.2836357466876507, C-Loss: 15.643996506929398\n",
      "Epoch: 335, Loss: 75.30529022216797,  L-Loss: 0.25335608050227165, C-Loss: 15.010386914014816\n",
      "Epoch: 336, Loss: 75.2543568611145,  L-Loss: 0.2648546632844955, C-Loss: 14.997900247573853\n",
      "Epoch: 337, Loss: 74.46516597270966,  L-Loss: 0.326982912607491, C-Loss: 14.827636629343033\n",
      "Epoch: 338, Loss: 92.06990206241608,  L-Loss: 0.47686702315695584, C-Loss: 18.318607091903687\n",
      "Epoch: 339, Loss: 77.31106543540955,  L-Loss: 0.3051571911200881, C-Loss: 15.40118157863617\n",
      "Epoch: 340, Loss: 76.90917992591858,  L-Loss: 0.27825850434601307, C-Loss: 15.326184064149857\n",
      "Saving model.\n",
      "Epoch: 341, Loss: 73.96132576465607,  L-Loss: 0.2578494579065591, C-Loss: 14.740695238113403\n",
      "Epoch: 342, Loss: 75.48957407474518,  L-Loss: 0.23866710532456636, C-Loss: 15.050181329250336\n",
      "Saving model.\n",
      "Epoch: 343, Loss: 73.93917000293732,  L-Loss: 0.25217778608202934, C-Loss: 14.737398445606232\n",
      "Epoch: 344, Loss: 76.57033979892731,  L-Loss: 0.2573577689472586, C-Loss: 15.262596279382706\n",
      "Epoch: 345, Loss: 73.9906302690506,  L-Loss: 0.2734448672272265, C-Loss: 14.743437111377716\n",
      "Epoch: 346, Loss: 81.16319143772125,  L-Loss: 0.2965786033309996, C-Loss: 16.173322677612305\n",
      "Epoch: 347, Loss: 75.35887217521667,  L-Loss: 0.3362272591330111, C-Loss: 15.004529029130936\n",
      "Epoch: 348, Loss: 81.61215019226074,  L-Loss: 0.30462537752464414, C-Loss: 16.261505126953125\n",
      "Epoch: 349, Loss: 74.84009337425232,  L-Loss: 0.3696558647789061, C-Loss: 14.89408752322197\n",
      "Epoch: 350, Loss: 95.01947605609894,  L-Loss: 0.49408477381803095, C-Loss: 18.905078202486038\n",
      "Epoch: 351, Loss: 83.3832676410675,  L-Loss: 0.38144934456795454, C-Loss: 16.600363552570343\n",
      "Epoch: 352, Loss: 88.99101865291595,  L-Loss: 0.48888336727395654, C-Loss: 17.700426995754242\n",
      "Epoch: 353, Loss: 80.65201163291931,  L-Loss: 0.3668092358857393, C-Loss: 16.057040333747864\n",
      "Epoch: 354, Loss: 90.41395103931427,  L-Loss: 0.36486801970750093, C-Loss: 18.009816497564316\n",
      "Epoch: 355, Loss: 75.96793949604034,  L-Loss: 0.33026010636240244, C-Loss: 15.127535998821259\n",
      "Epoch: 356, Loss: 78.99715495109558,  L-Loss: 0.20342464023269713, C-Loss: 15.758746027946472\n",
      "Epoch: 357, Loss: 85.23338377475739,  L-Loss: 0.35331414523534477, C-Loss: 16.9760140478611\n",
      "Epoch: 358, Loss: 111.28864216804504,  L-Loss: 0.44498564838431776, C-Loss: 22.168731212615967\n",
      "Epoch: 359, Loss: 77.77736103534698,  L-Loss: 0.33131061773747206, C-Loss: 15.48921012878418\n",
      "Epoch: 360, Loss: 82.89860165119171,  L-Loss: 0.2686428006272763, C-Loss: 16.52599173784256\n",
      "Epoch: 361, Loss: 79.09318208694458,  L-Loss: 0.30419405829161406, C-Loss: 15.757797658443451\n",
      "Epoch: 362, Loss: 80.25201404094696,  L-Loss: 0.24759849323891103, C-Loss: 16.000883102416992\n",
      "Epoch: 363, Loss: 74.47056710720062,  L-Loss: 0.2556532504968345, C-Loss: 14.84298262000084\n",
      "Epoch: 364, Loss: 79.61450552940369,  L-Loss: 0.24140053358860314, C-Loss: 15.874621003866196\n",
      "Saving model.\n",
      "Epoch: 365, Loss: 73.74231350421906,  L-Loss: 0.2393250991590321, C-Loss: 14.700597822666168\n",
      "Epoch: 366, Loss: 75.04332280158997,  L-Loss: 0.20306387054733932, C-Loss: 14.968051731586456\n",
      "Epoch: 367, Loss: 73.99043679237366,  L-Loss: 0.2435352101456374, C-Loss: 14.749380260705948\n",
      "Epoch: 368, Loss: 79.08310949802399,  L-Loss: 0.24047280405648053, C-Loss: 15.768527388572693\n",
      "Saving model.\n",
      "Epoch: 369, Loss: 73.30217826366425,  L-Loss: 0.21703958278521895, C-Loss: 14.617027759552002\n",
      "Epoch: 370, Loss: 74.85071969032288,  L-Loss: 0.1800761492922902, C-Loss: 14.934128671884537\n",
      "Saving model.\n",
      "Epoch: 371, Loss: 72.99998199939728,  L-Loss: 0.20542092598043382, C-Loss: 14.558912068605423\n",
      "Epoch: 372, Loss: 73.63257718086243,  L-Loss: 0.16879836516454816, C-Loss: 14.692755699157715\n",
      "Saving model.\n",
      "Epoch: 373, Loss: 72.90825378894806,  L-Loss: 0.1849928491283208, C-Loss: 14.544652283191681\n",
      "Epoch: 374, Loss: 74.38409447669983,  L-Loss: 0.17455161619000137, C-Loss: 14.841908514499664\n",
      "Epoch: 375, Loss: 73.0681881904602,  L-Loss: 0.18078454793430865, C-Loss: 14.577480733394623\n",
      "Epoch: 376, Loss: 74.25208556652069,  L-Loss: 0.18895520246587694, C-Loss: 14.8126260638237\n",
      "Epoch: 377, Loss: 72.99690246582031,  L-Loss: 0.1884662837255746, C-Loss: 14.561687260866165\n",
      "Epoch: 378, Loss: 74.77445650100708,  L-Loss: 0.20445039309561253, C-Loss: 14.914001137018204\n",
      "Epoch: 379, Loss: 73.15395843982697,  L-Loss: 0.21520093316212296, C-Loss: 14.587751477956772\n",
      "Epoch: 380, Loss: 76.58526301383972,  L-Loss: 0.22839379939250648, C-Loss: 15.27137404680252\n",
      "Epoch: 381, Loss: 73.55620288848877,  L-Loss: 0.19967812299728394, C-Loss: 14.671304911375046\n",
      "Epoch: 382, Loss: 74.70153963565826,  L-Loss: 0.19141164096072316, C-Loss: 14.902025669813156\n",
      "Epoch: 383, Loss: 73.21525084972382,  L-Loss: 0.19264619867317379, C-Loss: 14.604520946741104\n",
      "Epoch: 384, Loss: 73.87361109256744,  L-Loss: 0.19238349562510848, C-Loss: 14.736245453357697\n",
      "Epoch: 385, Loss: 73.0346291065216,  L-Loss: 0.18861175887286663, C-Loss: 14.569203466176987\n",
      "Epoch: 386, Loss: 76.87308716773987,  L-Loss: 0.2252450219821185, C-Loss: 15.329568535089493\n",
      "Epoch: 387, Loss: 73.3221275806427,  L-Loss: 0.196051393635571, C-Loss: 14.62521517276764\n",
      "Epoch: 388, Loss: 75.17672622203827,  L-Loss: 0.2030155889224261, C-Loss: 14.994742184877396\n",
      "Epoch: 389, Loss: 73.53192603588104,  L-Loss: 0.20656309532932937, C-Loss: 14.665072649717331\n",
      "Epoch: 390, Loss: 75.44285154342651,  L-Loss: 0.21396777243353426, C-Loss: 15.045776724815369\n",
      "Epoch: 391, Loss: 73.440358877182,  L-Loss: 0.18997729266993701, C-Loss: 14.650076359510422\n",
      "Epoch: 392, Loss: 74.25816905498505,  L-Loss: 0.18776685162447393, C-Loss: 14.814080446958542\n",
      "Saving model.\n",
      "Epoch: 393, Loss: 72.74992334842682,  L-Loss: 0.1948981995228678, C-Loss: 14.511005073785782\n",
      "Epoch: 394, Loss: 74.60201513767242,  L-Loss: 0.19092730805277824, C-Loss: 14.882217526435852\n",
      "Epoch: 395, Loss: 73.12263536453247,  L-Loss: 0.1766031365841627, C-Loss: 14.589206576347351\n",
      "Epoch: 396, Loss: 75.31031537055969,  L-Loss: 0.1874020453542471, C-Loss: 15.024582743644714\n",
      "Epoch: 397, Loss: 72.86918067932129,  L-Loss: 0.2252514990977943, C-Loss: 14.528785705566406\n",
      "Epoch: 398, Loss: 77.18363273143768,  L-Loss: 0.220954553456977, C-Loss: 15.39253556728363\n",
      "Epoch: 399, Loss: 76.42187714576721,  L-Loss: 0.2856278116814792, C-Loss: 15.227249920368195\n",
      "Epoch: 400, Loss: 83.41015315055847,  L-Loss: 0.3098768179770559, C-Loss: 16.62005516886711\n",
      "Epoch: 401, Loss: 77.8290901184082,  L-Loss: 0.20348929823376238, C-Loss: 15.525120198726654\n",
      "Epoch: 402, Loss: 76.76410830020905,  L-Loss: 0.23779884609393775, C-Loss: 15.305261969566345\n",
      "Epoch: 403, Loss: 83.26472532749176,  L-Loss: 0.2803995171561837, C-Loss: 16.596865236759186\n",
      "Epoch: 404, Loss: 73.70940959453583,  L-Loss: 0.1649949406273663, C-Loss: 14.708883076906204\n",
      "Epoch: 405, Loss: 73.39968776702881,  L-Loss: 0.14711414952762425, C-Loss: 14.650514721870422\n",
      "Epoch: 406, Loss: 72.99969244003296,  L-Loss: 0.14072562288492918, C-Loss: 14.57179319858551\n",
      "Epoch: 407, Loss: 72.89986944198608,  L-Loss: 0.16336106462404132, C-Loss: 14.547301650047302\n",
      "Epoch: 408, Loss: 72.75616705417633,  L-Loss: 0.14338574116118252, C-Loss: 14.522556215524673\n",
      "Saving model.\n",
      "Epoch: 409, Loss: 72.72172236442566,  L-Loss: 0.13323506154119968, C-Loss: 14.517697513103485\n",
      "Saving model.\n",
      "Epoch: 410, Loss: 72.64426112174988,  L-Loss: 0.12897075596265495, C-Loss: 14.503057956695557\n",
      "Saving model.\n",
      "Epoch: 411, Loss: 72.16592514514923,  L-Loss: 0.12562338542193174, C-Loss: 14.408060491085052\n",
      "Epoch: 412, Loss: 72.21663510799408,  L-Loss: 0.13234768132679164, C-Loss: 14.416857481002808\n",
      "Saving model.\n",
      "Epoch: 413, Loss: 72.07241690158844,  L-Loss: 0.13881105603650212, C-Loss: 14.38672125339508\n",
      "Epoch: 414, Loss: 72.47414577007294,  L-Loss: 0.14419072214514017, C-Loss: 14.465991109609604\n",
      "Epoch: 415, Loss: 72.16743004322052,  L-Loss: 0.16779192234389484, C-Loss: 14.39992767572403\n",
      "Epoch: 416, Loss: 73.596062541008,  L-Loss: 0.16904557053931057, C-Loss: 14.685403317213058\n",
      "Epoch: 417, Loss: 72.91782224178314,  L-Loss: 0.190538294846192, C-Loss: 14.545456737279892\n",
      "Epoch: 418, Loss: 72.93537843227386,  L-Loss: 0.170491729862988, C-Loss: 14.552977353334427\n",
      "Epoch: 419, Loss: 72.28087735176086,  L-Loss: 0.16899361205287278, C-Loss: 14.422376900911331\n",
      "Epoch: 420, Loss: 74.15558302402496,  L-Loss: 0.18631468364037573, C-Loss: 14.793853640556335\n",
      "Epoch: 421, Loss: 72.66279971599579,  L-Loss: 0.18447992880828679, C-Loss: 14.495663911104202\n",
      "Epoch: 422, Loss: 75.28317546844482,  L-Loss: 0.18086305283941329, C-Loss: 15.020462423563004\n",
      "Epoch: 423, Loss: 72.7809225320816,  L-Loss: 0.1692975447513163, C-Loss: 14.52232477068901\n",
      "Epoch: 424, Loss: 72.38205552101135,  L-Loss: 0.14162932988256216, C-Loss: 14.448085248470306\n",
      "Epoch: 425, Loss: 72.24117422103882,  L-Loss: 0.15554509940557182, C-Loss: 14.417125880718231\n",
      "Epoch: 426, Loss: 73.12541604042053,  L-Loss: 0.16115497634746134, C-Loss: 14.592852264642715\n",
      "Epoch: 427, Loss: 73.08502078056335,  L-Loss: 0.19940810883417726, C-Loss: 14.57712259888649\n",
      "Epoch: 428, Loss: 77.14099681377411,  L-Loss: 0.2250265998300165, C-Loss: 15.383194237947464\n",
      "Epoch: 429, Loss: 81.0314736366272,  L-Loss: 0.2416536696255207, C-Loss: 16.15796387195587\n",
      "Epoch: 430, Loss: 73.16322326660156,  L-Loss: 0.16926245926879346, C-Loss: 14.598792254924774\n",
      "Epoch: 431, Loss: 72.24672782421112,  L-Loss: 0.17290581134147942, C-Loss: 14.414764374494553\n",
      "Epoch: 432, Loss: 74.42127478122711,  L-Loss: 0.15315892081707716, C-Loss: 14.853623121976852\n",
      "Epoch: 433, Loss: 72.54119455814362,  L-Loss: 0.1786283024121076, C-Loss: 14.472513198852539\n",
      "Epoch: 434, Loss: 73.28821122646332,  L-Loss: 0.14471666142344475, C-Loss: 14.628698855638504\n",
      "Saving model.\n",
      "Epoch: 435, Loss: 71.95786583423615,  L-Loss: 0.15025893901474774, C-Loss: 14.361521422863007\n",
      "Epoch: 436, Loss: 73.5738662481308,  L-Loss: 0.1568560318555683, C-Loss: 14.683402001857758\n",
      "Saving model.\n",
      "Epoch: 437, Loss: 71.80042672157288,  L-Loss: 0.1668320505414158, C-Loss: 14.326718837022781\n",
      "Epoch: 438, Loss: 74.74592411518097,  L-Loss: 0.19184571015648544, C-Loss: 14.910815685987473\n",
      "Epoch: 439, Loss: 72.29527139663696,  L-Loss: 0.18493080441839993, C-Loss: 14.422068268060684\n",
      "Epoch: 440, Loss: 76.30341494083405,  L-Loss: 0.20118755684234202, C-Loss: 15.220445454120636\n",
      "Epoch: 441, Loss: 72.35350608825684,  L-Loss: 0.1709345942363143, C-Loss: 14.436514288187027\n",
      "Epoch: 442, Loss: 71.99714136123657,  L-Loss: 0.1394815114326775, C-Loss: 14.371531933546066\n",
      "Epoch: 443, Loss: 72.12810695171356,  L-Loss: 0.15732760657556355, C-Loss: 14.39415580034256\n",
      "Epoch: 444, Loss: 72.33731532096863,  L-Loss: 0.14624593337066472, C-Loss: 14.438213914632797\n",
      "Epoch: 445, Loss: 71.97423040866852,  L-Loss: 0.171228161547333, C-Loss: 14.360600471496582\n",
      "Epoch: 446, Loss: 76.07993710041046,  L-Loss: 0.20763262687250972, C-Loss: 15.174460798501968\n",
      "Epoch: 447, Loss: 73.93158626556396,  L-Loss: 0.191093914443627, C-Loss: 14.748098403215408\n",
      "Epoch: 448, Loss: 81.80864906311035,  L-Loss: 0.22745116241276264, C-Loss: 16.316239595413208\n",
      "Epoch: 449, Loss: 82.40198230743408,  L-Loss: 0.25076769082807004, C-Loss: 16.43024292588234\n",
      "Epoch: 450, Loss: 78.37040889263153,  L-Loss: 0.25897475075908005, C-Loss: 15.622286826372147\n",
      "Epoch: 451, Loss: 75.95047092437744,  L-Loss: 0.22909502289257944, C-Loss: 15.144275218248367\n",
      "Epoch: 452, Loss: 75.08916556835175,  L-Loss: 0.18002806999720633, C-Loss: 14.981827586889267\n",
      "Epoch: 453, Loss: 73.14177715778351,  L-Loss: 0.18988436739891768, C-Loss: 14.59037858247757\n",
      "Epoch: 454, Loss: 74.9927853345871,  L-Loss: 0.1722381147556007, C-Loss: 14.964109420776367\n",
      "Epoch: 455, Loss: 73.24341034889221,  L-Loss: 0.18904067808762193, C-Loss: 14.610873818397522\n",
      "Epoch: 456, Loss: 72.40225279331207,  L-Loss: 0.14264130126684904, C-Loss: 14.451922357082367\n",
      "Epoch: 457, Loss: 72.20471775531769,  L-Loss: 0.19729665550403297, C-Loss: 14.401484280824661\n",
      "Epoch: 458, Loss: 72.58620929718018,  L-Loss: 0.14220247347839177, C-Loss: 14.4888014793396\n",
      "Saving model.\n",
      "Epoch: 459, Loss: 71.66717004776001,  L-Loss: 0.15491137420758605, C-Loss: 14.302451759576797\n",
      "Epoch: 460, Loss: 72.58586156368256,  L-Loss: 0.13181039900518954, C-Loss: 14.49081027507782\n",
      "Epoch: 461, Loss: 71.69244253635406,  L-Loss: 0.151352432789281, C-Loss: 14.308217912912369\n",
      "Epoch: 462, Loss: 71.97575032711029,  L-Loss: 0.13540237839333713, C-Loss: 14.368069529533386\n",
      "Epoch: 463, Loss: 71.78131759166718,  L-Loss: 0.1535127826500684, C-Loss: 14.32556101679802\n",
      "Epoch: 464, Loss: 72.3944902420044,  L-Loss: 0.1553616572637111, C-Loss: 14.447825729846954\n",
      "Epoch: 465, Loss: 71.90300500392914,  L-Loss: 0.16759994276799262, C-Loss: 14.347081005573273\n",
      "Epoch: 466, Loss: 78.12402868270874,  L-Loss: 0.20889929705299437, C-Loss: 15.5830257833004\n",
      "Epoch: 467, Loss: 72.25417256355286,  L-Loss: 0.1720049763098359, C-Loss: 14.416433483362198\n",
      "Epoch: 468, Loss: 75.70116877555847,  L-Loss: 0.17254780791699886, C-Loss: 15.105724185705185\n",
      "Epoch: 469, Loss: 71.99923121929169,  L-Loss: 0.16886157100088894, C-Loss: 14.366073936223984\n",
      "Epoch: 470, Loss: 72.05410885810852,  L-Loss: 0.1486240925733, C-Loss: 14.38109689950943\n",
      "Epoch: 471, Loss: 71.79692423343658,  L-Loss: 0.1645179397892207, C-Loss: 14.326481223106384\n",
      "Epoch: 472, Loss: 74.57580196857452,  L-Loss: 0.15708941710181534, C-Loss: 14.883742541074753\n",
      "Epoch: 473, Loss: 72.18206369876862,  L-Loss: 0.1503435899503529, C-Loss: 14.406343936920166\n",
      "Epoch: 474, Loss: 72.2788313627243,  L-Loss: 0.1401013715658337, C-Loss: 14.427745938301086\n",
      "Epoch: 475, Loss: 72.02225542068481,  L-Loss: 0.18344917800277472, C-Loss: 14.367761254310608\n",
      "Epoch: 476, Loss: 73.11630344390869,  L-Loss: 0.1494806152768433, C-Loss: 14.593364477157593\n",
      "Epoch: 477, Loss: 71.98214888572693,  L-Loss: 0.16461572889238596, C-Loss: 14.363506555557251\n",
      "Epoch: 478, Loss: 73.92570996284485,  L-Loss: 0.19999784510582685, C-Loss: 14.745142310857773\n",
      "Epoch: 479, Loss: 83.13748526573181,  L-Loss: 0.2576262620277703, C-Loss: 16.575971752405167\n",
      "Epoch: 480, Loss: 98.67358708381653,  L-Loss: 0.3850201347377151, C-Loss: 19.657713174819946\n",
      "Epoch: 481, Loss: 86.8942346572876,  L-Loss: 0.2664826107211411, C-Loss: 17.325550317764282\n",
      "Epoch: 482, Loss: 91.87332963943481,  L-Loss: 0.26678210753016174, C-Loss: 18.321309566497803\n",
      "Epoch: 483, Loss: 102.14148700237274,  L-Loss: 0.4494791105389595, C-Loss: 20.33840149641037\n",
      "Epoch: 484, Loss: 85.10763025283813,  L-Loss: 0.2922050575725734, C-Loss: 16.963085263967514\n",
      "Epoch: 485, Loss: 118.8170154094696,  L-Loss: 0.4091133791953325, C-Loss: 23.68158048391342\n",
      "Epoch: 486, Loss: 78.48553669452667,  L-Loss: 0.23281904635950923, C-Loss: 15.650543361902237\n",
      "Epoch: 487, Loss: 74.73722469806671,  L-Loss: 0.19703643606044352, C-Loss: 14.908037662506104\n",
      "Epoch: 488, Loss: 77.1935361623764,  L-Loss: 0.17802021838724613, C-Loss: 15.403103291988373\n",
      "Epoch: 489, Loss: 73.36594271659851,  L-Loss: 0.23001869558356702, C-Loss: 14.627184838056564\n",
      "Epoch: 490, Loss: 90.19592726230621,  L-Loss: 0.2981474702246487, C-Loss: 17.979556143283844\n",
      "Epoch: 491, Loss: 77.13076102733612,  L-Loss: 0.18689109315164387, C-Loss: 15.388773828744888\n",
      "Epoch: 492, Loss: 75.51055669784546,  L-Loss: 0.15036177192814648, C-Loss: 15.072039097547531\n",
      "Epoch: 493, Loss: 73.10498213768005,  L-Loss: 0.15089061483740807, C-Loss: 14.590818524360657\n",
      "Epoch: 494, Loss: 72.33544218540192,  L-Loss: 0.13710840581916273, C-Loss: 14.439666777849197\n",
      "Epoch: 495, Loss: 71.76663744449615,  L-Loss: 0.13961944286711514, C-Loss: 14.325403571128845\n",
      "Epoch: 496, Loss: 73.71599423885345,  L-Loss: 0.14678542967885733, C-Loss: 14.713841795921326\n",
      "Epoch: 497, Loss: 71.76119935512543,  L-Loss: 0.15069485339336097, C-Loss: 14.322100847959518\n",
      "Epoch: 498, Loss: 73.02328979969025,  L-Loss: 0.14110138732939959, C-Loss: 14.57643774151802\n",
      "Epoch: 499, Loss: 72.18852818012238,  L-Loss: 0.15429877769201994, C-Loss: 14.40684574842453\n",
      "Epoch: 500, Loss: 76.09306609630585,  L-Loss: 0.1685862757731229, C-Loss: 15.184896051883698\n",
      "Epoch: 501, Loss: 72.37448644638062,  L-Loss: 0.18608751776628196, C-Loss: 14.437679797410965\n",
      "Epoch: 502, Loss: 80.2112147808075,  L-Loss: 0.19208049634471536, C-Loss: 16.00382685661316\n",
      "Epoch: 503, Loss: 81.74783647060394,  L-Loss: 0.19731877371668816, C-Loss: 16.310103565454483\n",
      "Epoch: 504, Loss: 72.69559800624847,  L-Loss: 0.1689897107426077, C-Loss: 14.505321681499481\n",
      "Epoch: 505, Loss: 73.11654877662659,  L-Loss: 0.13807656755670905, C-Loss: 14.595694541931152\n",
      "Epoch: 506, Loss: 73.60556268692017,  L-Loss: 0.1497124512679875, C-Loss: 14.6911700963974\n",
      "Epoch: 507, Loss: 71.69534635543823,  L-Loss: 0.18008975498378277, C-Loss: 14.30305129289627\n",
      "Epoch: 508, Loss: 74.40878891944885,  L-Loss: 0.1621221867389977, C-Loss: 14.84933340549469\n",
      "Epoch: 509, Loss: 72.55972397327423,  L-Loss: 0.16248613502830267, C-Loss: 14.479447692632675\n",
      "Epoch: 510, Loss: 73.3487035036087,  L-Loss: 0.1415041305590421, C-Loss: 14.641439944505692\n",
      "Epoch: 511, Loss: 71.79170787334442,  L-Loss: 0.14805123978294432, C-Loss: 14.328731328248978\n",
      "Epoch: 512, Loss: 73.39416229724884,  L-Loss: 0.15223989076912403, C-Loss: 14.648384511470795\n",
      "Saving model.\n",
      "Epoch: 513, Loss: 71.65157318115234,  L-Loss: 0.1379838224966079, C-Loss: 14.302717745304108\n",
      "Epoch: 514, Loss: 72.43918108940125,  L-Loss: 0.12637455179356039, C-Loss: 14.46256136894226\n",
      "Saving model.\n",
      "Epoch: 515, Loss: 71.5547844171524,  L-Loss: 0.12616665614768863, C-Loss: 14.28572365641594\n",
      "Epoch: 516, Loss: 72.55739212036133,  L-Loss: 0.12182907667011023, C-Loss: 14.487112641334534\n",
      "Epoch: 517, Loss: 71.57859909534454,  L-Loss: 0.11748564848676324, C-Loss: 14.292222678661346\n",
      "Epoch: 518, Loss: 71.76861345767975,  L-Loss: 0.1154855047352612, C-Loss: 14.330625593662262\n",
      "Saving model.\n",
      "Epoch: 519, Loss: 71.34883272647858,  L-Loss: 0.11445017508231103, C-Loss: 14.24687647819519\n",
      "Epoch: 520, Loss: 71.81690442562103,  L-Loss: 0.11596425692550838, C-Loss: 14.3401879966259\n",
      "Epoch: 521, Loss: 71.35635185241699,  L-Loss: 0.11525302263908088, C-Loss: 14.248219758272171\n",
      "Epoch: 522, Loss: 71.66611897945404,  L-Loss: 0.11518823844380677, C-Loss: 14.310186207294464\n",
      "Saving model.\n",
      "Epoch: 523, Loss: 71.21985912322998,  L-Loss: 0.11656550038605928, C-Loss: 14.22065868973732\n",
      "Epoch: 524, Loss: 71.72980046272278,  L-Loss: 0.11327602807432413, C-Loss: 14.323304861783981\n",
      "Saving model.\n",
      "Epoch: 525, Loss: 71.21979105472565,  L-Loss: 0.11799599532969296, C-Loss: 14.220359086990356\n",
      "Epoch: 526, Loss: 71.75035607814789,  L-Loss: 0.11727113951928914, C-Loss: 14.326616913080215\n",
      "Saving model.\n",
      "Epoch: 527, Loss: 71.19779765605927,  L-Loss: 0.11861214460805058, C-Loss: 14.215837121009827\n",
      "Epoch: 528, Loss: 71.83712482452393,  L-Loss: 0.1233117668889463, C-Loss: 14.342762470245361\n",
      "Epoch: 529, Loss: 71.50934541225433,  L-Loss: 0.1325239308644086, C-Loss: 14.275364339351654\n",
      "Epoch: 530, Loss: 71.61231648921967,  L-Loss: 0.12951574521139264, C-Loss: 14.296560049057007\n",
      "Epoch: 531, Loss: 71.46565914154053,  L-Loss: 0.13698856974951923, C-Loss: 14.265734016895294\n",
      "Epoch: 532, Loss: 73.08493089675903,  L-Loss: 0.15261456463485956, C-Loss: 14.58646336197853\n",
      "Epoch: 533, Loss: 71.944615483284,  L-Loss: 0.16931069712154567, C-Loss: 14.355060994625092\n",
      "Epoch: 534, Loss: 74.32753324508667,  L-Loss: 0.18487964011728764, C-Loss: 14.828530669212341\n",
      "Epoch: 535, Loss: 97.8627415895462,  L-Loss: 0.26012117555364966, C-Loss: 19.520524233579636\n",
      "Epoch: 536, Loss: 86.70357894897461,  L-Loss: 0.2538656471297145, C-Loss: 17.289942622184753\n",
      "Epoch: 537, Loss: 94.03642547130585,  L-Loss: 0.28267820342443883, C-Loss: 18.750749498605728\n",
      "Epoch: 538, Loss: 88.35744905471802,  L-Loss: 0.2856915039010346, C-Loss: 17.614351719617844\n",
      "Epoch: 539, Loss: 98.50361037254333,  L-Loss: 0.31479191198013723, C-Loss: 19.63776382803917\n",
      "Epoch: 540, Loss: 85.78270208835602,  L-Loss: 0.21746051171794534, C-Loss: 17.113048404455185\n",
      "Epoch: 541, Loss: 78.90934836864471,  L-Loss: 0.1693254702258855, C-Loss: 15.748004645109177\n",
      "Epoch: 542, Loss: 77.26536476612091,  L-Loss: 0.17388732847757638, C-Loss: 15.418295532464981\n",
      "Epoch: 543, Loss: 91.58313035964966,  L-Loss: 0.22761037363670766, C-Loss: 18.27110406756401\n",
      "Epoch: 544, Loss: 82.51930737495422,  L-Loss: 0.21199249755591154, C-Loss: 16.461463153362274\n",
      "Epoch: 545, Loss: 85.66878545284271,  L-Loss: 0.214718462433666, C-Loss: 17.090813398361206\n",
      "Epoch: 546, Loss: 74.22117269039154,  L-Loss: 0.1746365267317742, C-Loss: 14.809307217597961\n",
      "Epoch: 547, Loss: 73.21436893939972,  L-Loss: 0.15376452752389014, C-Loss: 14.61212083697319\n",
      "Epoch: 548, Loss: 71.92295444011688,  L-Loss: 0.20291629945859313, C-Loss: 14.344007581472397\n",
      "Epoch: 549, Loss: 95.37622129917145,  L-Loss: 0.3075420332606882, C-Loss: 19.01373589038849\n",
      "Epoch: 550, Loss: 82.542121052742,  L-Loss: 0.20888663781806827, C-Loss: 16.466646820306778\n",
      "Epoch: 551, Loss: 73.20090734958649,  L-Loss: 0.15304132248274982, C-Loss: 14.6095732152462\n",
      "Epoch: 552, Loss: 72.53635609149933,  L-Loss: 0.14204560685902834, C-Loss: 14.478862017393112\n",
      "Epoch: 553, Loss: 72.05399632453918,  L-Loss: 0.13931485451757908, C-Loss: 14.38293632864952\n",
      "Epoch: 554, Loss: 71.95304012298584,  L-Loss: 0.1292995938565582, C-Loss: 14.3647480905056\n",
      "Epoch: 555, Loss: 71.67686116695404,  L-Loss: 0.13778412505052984, C-Loss: 14.307815432548523\n",
      "Epoch: 556, Loss: 74.2562495470047,  L-Loss: 0.15522445156238973, C-Loss: 14.82020503282547\n",
      "Epoch: 557, Loss: 71.49237751960754,  L-Loss: 0.15214677387848496, C-Loss: 14.268046170473099\n",
      "Epoch: 558, Loss: 74.78973042964935,  L-Loss: 0.14793357462622225, C-Loss: 14.928359299898148\n",
      "Epoch: 559, Loss: 71.80089819431305,  L-Loss: 0.14807096612639725, C-Loss: 14.330565392971039\n",
      "Epoch: 560, Loss: 72.56157374382019,  L-Loss: 0.13473488460294902, C-Loss: 14.485367953777313\n",
      "Epoch: 561, Loss: 71.77779746055603,  L-Loss: 0.1387331048026681, C-Loss: 14.327812910079956\n",
      "Epoch: 562, Loss: 75.70203244686127,  L-Loss: 0.15210360917262733, C-Loss: 15.109985709190369\n",
      "Epoch: 563, Loss: 71.49435138702393,  L-Loss: 0.1469989523757249, C-Loss: 14.269470512866974\n",
      "Epoch: 564, Loss: 73.83254384994507,  L-Loss: 0.13278858154080808, C-Loss: 14.739951133728027\n",
      "Epoch: 565, Loss: 71.93885910511017,  L-Loss: 0.14253300265409052, C-Loss: 14.35926502943039\n",
      "Epoch: 566, Loss: 73.09329569339752,  L-Loss: 0.14585786149837077, C-Loss: 14.5894875228405\n",
      "Epoch: 567, Loss: 71.45496213436127,  L-Loss: 0.12172806123271585, C-Loss: 14.266646891832352\n",
      "Epoch: 568, Loss: 71.76593768596649,  L-Loss: 0.11620305106043816, C-Loss: 14.329946845769882\n",
      "Epoch: 569, Loss: 71.35913801193237,  L-Loss: 0.12001132545992732, C-Loss: 14.247825294733047\n",
      "Epoch: 570, Loss: 71.80384457111359,  L-Loss: 0.12318405555561185, C-Loss: 14.336132019758224\n",
      "Epoch: 571, Loss: 71.45527398586273,  L-Loss: 0.11874333466403186, C-Loss: 14.267306119203568\n",
      "Epoch: 572, Loss: 71.7930234670639,  L-Loss: 0.11825694632716477, C-Loss: 14.33495318889618\n",
      "Epoch: 573, Loss: 71.31513369083405,  L-Loss: 0.11536791315302253, C-Loss: 14.239953011274338\n",
      "Epoch: 574, Loss: 71.49358701705933,  L-Loss: 0.11617062962614, C-Loss: 14.275483340024948\n",
      "Epoch: 575, Loss: 71.25936257839203,  L-Loss: 0.11244480195455253, C-Loss: 14.229383617639542\n",
      "Epoch: 576, Loss: 71.34936261177063,  L-Loss: 0.11335959215648472, C-Loss: 14.247200578451157\n",
      "Epoch: 577, Loss: 71.22433912754059,  L-Loss: 0.11288767168298364, C-Loss: 14.222290307283401\n",
      "Epoch: 578, Loss: 71.33152055740356,  L-Loss: 0.11373353423550725, C-Loss: 14.24355736374855\n",
      "Epoch: 579, Loss: 71.20772397518158,  L-Loss: 0.11351373884826899, C-Loss: 14.218842059373856\n",
      "Epoch: 580, Loss: 71.46479368209839,  L-Loss: 0.11637113499455154, C-Loss: 14.269684553146362\n",
      "Saving model.\n",
      "Epoch: 581, Loss: 71.1833381652832,  L-Loss: 0.1146764496807009, C-Loss: 14.213732242584229\n",
      "Epoch: 582, Loss: 71.54589080810547,  L-Loss: 0.11843860102817416, C-Loss: 14.285490483045578\n",
      "Saving model.\n",
      "Epoch: 583, Loss: 71.16905283927917,  L-Loss: 0.12158373254351318, C-Loss: 14.20949387550354\n",
      "Epoch: 584, Loss: 71.37463355064392,  L-Loss: 0.13021620898507535, C-Loss: 14.2488833963871\n",
      "Saving model.\n",
      "Epoch: 585, Loss: 71.0947744846344,  L-Loss: 0.12162904907017946, C-Loss: 14.194629162549973\n",
      "Epoch: 586, Loss: 71.77280247211456,  L-Loss: 0.11932830861769617, C-Loss: 14.330694824457169\n",
      "Epoch: 587, Loss: 71.1116396188736,  L-Loss: 0.11577547271735966, C-Loss: 14.199172884225845\n",
      "Epoch: 588, Loss: 71.40202105045319,  L-Loss: 0.13397716777399182, C-Loss: 14.253608763217926\n",
      "Epoch: 589, Loss: 71.13852393627167,  L-Loss: 0.12348493561148643, C-Loss: 14.20300778746605\n",
      "Epoch: 590, Loss: 71.7498527765274,  L-Loss: 0.15028541209176183, C-Loss: 14.319913476705551\n",
      "Epoch: 591, Loss: 72.50251853466034,  L-Loss: 0.1821627919562161, C-Loss: 14.464071154594421\n",
      "Epoch: 592, Loss: 75.89837801456451,  L-Loss: 0.16587685863487422, C-Loss: 15.14650022983551\n",
      "Epoch: 593, Loss: 79.5184121131897,  L-Loss: 0.1943639141973108, C-Loss: 15.864809542894363\n",
      "Epoch: 594, Loss: 79.4290611743927,  L-Loss: 0.24132395046763122, C-Loss: 15.837547361850739\n",
      "Epoch: 595, Loss: 74.8175345659256,  L-Loss: 0.17517166677862406, C-Loss: 14.928472697734833\n",
      "Epoch: 596, Loss: 72.16486966609955,  L-Loss: 0.137310589896515, C-Loss: 14.405511796474457\n",
      "Epoch: 597, Loss: 76.37117028236389,  L-Loss: 0.14502434828318655, C-Loss: 15.245229244232178\n",
      "Epoch: 598, Loss: 71.74107575416565,  L-Loss: 0.13737916806712747, C-Loss: 14.320739328861237\n",
      "Epoch: 599, Loss: 71.25310719013214,  L-Loss: 0.1688457946293056, C-Loss: 14.216852307319641\n",
      "Epoch: 600, Loss: 76.56514155864716,  L-Loss: 0.18009225348941982, C-Loss: 15.277009844779968\n",
      "Epoch: 601, Loss: 71.59087252616882,  L-Loss: 0.12803935329429805, C-Loss: 14.2925665974617\n",
      "Epoch: 602, Loss: 71.23819887638092,  L-Loss: 0.13748044101521373, C-Loss: 14.220143675804138\n",
      "Epoch: 603, Loss: 72.1443544626236,  L-Loss: 0.13859714986756444, C-Loss: 14.401151478290558\n",
      "Epoch: 604, Loss: 71.28559339046478,  L-Loss: 0.14536538743413985, C-Loss: 14.228045612573624\n",
      "Epoch: 605, Loss: 74.36484146118164,  L-Loss: 0.16485778335481882, C-Loss: 14.839996755123138\n",
      "Epoch: 606, Loss: 71.658212184906,  L-Loss: 0.15265212440863252, C-Loss: 14.301111966371536\n",
      "Epoch: 607, Loss: 74.89963233470917,  L-Loss: 0.1628701314330101, C-Loss: 14.947352290153503\n",
      "Epoch: 608, Loss: 72.15315735340118,  L-Loss: 0.13909695926122367, C-Loss: 14.402812033891678\n",
      "Epoch: 609, Loss: 71.33802556991577,  L-Loss: 0.1310065274592489, C-Loss: 14.241403818130493\n",
      "Epoch: 610, Loss: 71.97383713722229,  L-Loss: 0.13443701481446624, C-Loss: 14.367880016565323\n",
      "Saving model.\n",
      "Epoch: 611, Loss: 71.0880126953125,  L-Loss: 0.13633674755692482, C-Loss: 14.190335065126419\n",
      "Epoch: 612, Loss: 72.56698477268219,  L-Loss: 0.13762435060925782, C-Loss: 14.485872149467468\n",
      "Epoch: 613, Loss: 72.23820745944977,  L-Loss: 0.13722996483556926, C-Loss: 14.420195460319519\n",
      "Epoch: 614, Loss: 71.20416605472565,  L-Loss: 0.11831136676482856, C-Loss: 14.217170894145966\n",
      "Saving model.\n",
      "Epoch: 615, Loss: 71.07540392875671,  L-Loss: 0.12070709024555981, C-Loss: 14.190939337015152\n",
      "Epoch: 616, Loss: 71.90547621250153,  L-Loss: 0.13427078840322793, C-Loss: 14.354241043329239\n",
      "Saving model.\n",
      "Epoch: 617, Loss: 71.05076551437378,  L-Loss: 0.13354646763764322, C-Loss: 14.183443814516068\n",
      "Epoch: 618, Loss: 75.36531805992126,  L-Loss: 0.14947189227677882, C-Loss: 15.043169260025024\n",
      "Epoch: 619, Loss: 72.34647023677826,  L-Loss: 0.13283122703433037, C-Loss: 14.442727863788605\n",
      "Epoch: 620, Loss: 71.3722813129425,  L-Loss: 0.12343134940601885, C-Loss: 14.249770015478134\n",
      "Epoch: 621, Loss: 71.2338525056839,  L-Loss: 0.11987347132526338, C-Loss: 14.222795844078064\n",
      "Epoch: 622, Loss: 71.76060199737549,  L-Loss: 0.13109963573515415, C-Loss: 14.325900584459305\n",
      "Epoch: 623, Loss: 71.35940027236938,  L-Loss: 0.12737200036644936, C-Loss: 14.246405512094498\n",
      "Epoch: 624, Loss: 72.13397479057312,  L-Loss: 0.13954486488364637, C-Loss: 14.398885875940323\n",
      "Epoch: 625, Loss: 71.26442241668701,  L-Loss: 0.14620036096312106, C-Loss: 14.223644375801086\n",
      "Epoch: 626, Loss: 74.18001055717468,  L-Loss: 0.14638533210381866, C-Loss: 14.806725144386292\n",
      "Epoch: 627, Loss: 78.93988621234894,  L-Loss: 0.16209517954848707, C-Loss: 15.755558252334595\n",
      "Epoch: 628, Loss: 72.47187221050262,  L-Loss: 0.12370790774002671, C-Loss: 14.46963283419609\n",
      "Epoch: 629, Loss: 71.16556942462921,  L-Loss: 0.11537706246599555, C-Loss: 14.210038483142853\n",
      "Epoch: 630, Loss: 71.95872211456299,  L-Loss: 0.12269834848120809, C-Loss: 14.367204636335373\n",
      "Epoch: 631, Loss: 71.21489298343658,  L-Loss: 0.11421269597485662, C-Loss: 14.220136135816574\n",
      "Epoch: 632, Loss: 71.31011712551117,  L-Loss: 0.12804206437431276, C-Loss: 14.23641511797905\n",
      "Saving model.\n",
      "Epoch: 633, Loss: 70.93264770507812,  L-Loss: 0.13865944789722562, C-Loss: 14.158797711133957\n",
      "Epoch: 634, Loss: 74.61038601398468,  L-Loss: 0.16540838172659278, C-Loss: 14.888995558023453\n",
      "Epoch: 635, Loss: 87.10581636428833,  L-Loss: 0.23616194422356784, C-Loss: 17.37393096089363\n",
      "Epoch: 636, Loss: 98.60980641841888,  L-Loss: 0.284451198996976, C-Loss: 19.665070921182632\n",
      "Epoch: 637, Loss: 132.3124564886093,  L-Loss: 0.6326321228407323, C-Loss: 26.33596482872963\n",
      "Epoch: 638, Loss: 150.855903506279,  L-Loss: 0.6649380354210734, C-Loss: 30.038192719221115\n",
      "Epoch: 639, Loss: 155.45536851882935,  L-Loss: 0.785393790807575, C-Loss: 30.933995127677917\n",
      "Epoch: 640, Loss: 120.84110033512115,  L-Loss: 0.45135178603231907, C-Loss: 24.077949702739716\n",
      "Epoch: 641, Loss: 107.86075985431671,  L-Loss: 0.38242426281794906, C-Loss: 21.495666921138763\n",
      "Epoch: 642, Loss: 133.69929802417755,  L-Loss: 0.7138075544498861, C-Loss: 26.59709796309471\n",
      "Epoch: 643, Loss: 160.75763189792633,  L-Loss: 0.9402909828349948, C-Loss: 31.96346828341484\n",
      "Epoch: 644, Loss: 99.38714933395386,  L-Loss: 0.26604094030335546, C-Loss: 19.824221670627594\n",
      "Epoch: 645, Loss: 113.54425990581512,  L-Loss: 0.3945244629867375, C-Loss: 22.62994721531868\n",
      "Epoch: 646, Loss: 91.38143956661224,  L-Loss: 0.3540456867776811, C-Loss: 18.205478817224503\n",
      "Epoch: 647, Loss: 97.83673310279846,  L-Loss: 0.2415058109909296, C-Loss: 19.51904571056366\n",
      "Epoch: 648, Loss: 78.69599533081055,  L-Loss: 0.26043460983783007, C-Loss: 15.687112122774124\n",
      "Epoch: 649, Loss: 96.43953382968903,  L-Loss: 0.2579687836114317, C-Loss: 19.236313074827194\n",
      "Epoch: 650, Loss: 75.37480115890503,  L-Loss: 0.18542441236786544, C-Loss: 15.037875384092331\n",
      "Epoch: 651, Loss: 74.7877424955368,  L-Loss: 0.16886263410560787, C-Loss: 14.9237759411335\n",
      "Epoch: 652, Loss: 73.00148797035217,  L-Loss: 0.18478148593567312, C-Loss: 14.563341289758682\n",
      "Epoch: 653, Loss: 86.74318146705627,  L-Loss: 0.19178358651697636, C-Loss: 17.310279548168182\n",
      "Epoch: 654, Loss: 72.75933527946472,  L-Loss: 0.17019684752449393, C-Loss: 14.51782763004303\n",
      "Epoch: 655, Loss: 74.08077001571655,  L-Loss: 0.14263979764655232, C-Loss: 14.787626057863235\n",
      "Epoch: 656, Loss: 72.07367098331451,  L-Loss: 0.1274899945128709, C-Loss: 14.389236152172089\n",
      "Epoch: 657, Loss: 72.38653004169464,  L-Loss: 0.12199603649787605, C-Loss: 14.45290693640709\n",
      "Epoch: 658, Loss: 72.40009140968323,  L-Loss: 0.11778637929819524, C-Loss: 14.456460982561111\n",
      "Epoch: 659, Loss: 71.8795051574707,  L-Loss: 0.11986451037228107, C-Loss: 14.351928144693375\n",
      "Epoch: 660, Loss: 72.39662110805511,  L-Loss: 0.12078616581857204, C-Loss: 14.455167025327682\n",
      "Epoch: 661, Loss: 71.76689434051514,  L-Loss: 0.12481417786329985, C-Loss: 14.328416168689728\n",
      "Epoch: 662, Loss: 73.53496050834656,  L-Loss: 0.12773644411936402, C-Loss: 14.681444764137268\n",
      "Epoch: 663, Loss: 72.66330456733704,  L-Loss: 0.13206783635541797, C-Loss: 14.506247371435165\n",
      "Epoch: 664, Loss: 72.81978607177734,  L-Loss: 0.13072031713090837, C-Loss: 14.537813067436218\n",
      "Epoch: 665, Loss: 71.70252418518066,  L-Loss: 0.1250667106360197, C-Loss: 14.315491557121277\n",
      "Epoch: 666, Loss: 72.13724517822266,  L-Loss: 0.12764897872693837, C-Loss: 14.401919305324554\n",
      "Epoch: 667, Loss: 72.73131728172302,  L-Loss: 0.13392586237750947, C-Loss: 14.519478350877762\n",
      "Epoch: 668, Loss: 72.49887764453888,  L-Loss: 0.13170924317091703, C-Loss: 14.473433762788773\n",
      "Epoch: 669, Loss: 74.05718159675598,  L-Loss: 0.13850815710611641, C-Loss: 14.783734768629074\n",
      "Epoch: 670, Loss: 72.67228639125824,  L-Loss: 0.12491611251607537, C-Loss: 14.509474039077759\n",
      "Epoch: 671, Loss: 71.73051261901855,  L-Loss: 0.12502350169233978, C-Loss: 14.321097791194916\n",
      "Epoch: 672, Loss: 72.44140863418579,  L-Loss: 0.12516011158004403, C-Loss: 14.46324959397316\n",
      "Epoch: 673, Loss: 72.19409418106079,  L-Loss: 0.12987267994321883, C-Loss: 14.412844330072403\n",
      "Epoch: 674, Loss: 72.6327588558197,  L-Loss: 0.14114334271289408, C-Loss: 14.498323202133179\n",
      "Epoch: 675, Loss: 72.18467581272125,  L-Loss: 0.13230365538038313, C-Loss: 14.410474419593811\n",
      "Epoch: 676, Loss: 72.67888128757477,  L-Loss: 0.1453963196836412, C-Loss: 14.506697058677673\n",
      "Epoch: 677, Loss: 75.25756573677063,  L-Loss: 0.14611125621013343, C-Loss: 15.022290885448456\n",
      "Epoch: 678, Loss: 72.36659228801727,  L-Loss: 0.14060335513204336, C-Loss: 14.445197850465775\n",
      "Epoch: 679, Loss: 74.45500898361206,  L-Loss: 0.1457529857289046, C-Loss: 14.861851245164871\n",
      "Epoch: 680, Loss: 73.34950613975525,  L-Loss: 0.1343870311975479, C-Loss: 14.643023759126663\n",
      "Epoch: 681, Loss: 72.15786719322205,  L-Loss: 0.12543762708082795, C-Loss: 14.406485885381699\n",
      "Epoch: 682, Loss: 71.56418466567993,  L-Loss: 0.12141632963903248, C-Loss: 14.288553714752197\n",
      "Epoch: 683, Loss: 71.09342217445374,  L-Loss: 0.1168673257343471, C-Loss: 14.195310950279236\n",
      "Epoch: 684, Loss: 72.44383192062378,  L-Loss: 0.11764890910126269, C-Loss: 14.465236693620682\n",
      "Epoch: 685, Loss: 70.98876905441284,  L-Loss: 0.11998322978615761, C-Loss: 14.173757165670395\n",
      "Epoch: 686, Loss: 73.15102469921112,  L-Loss: 0.13655982702039182, C-Loss: 14.602892905473709\n",
      "Epoch: 687, Loss: 71.03090476989746,  L-Loss: 0.14595257095061243, C-Loss: 14.176990389823914\n",
      "Epoch: 688, Loss: 73.08053135871887,  L-Loss: 0.1361531363800168, C-Loss: 14.588875710964203\n",
      "Epoch: 689, Loss: 71.08716237545013,  L-Loss: 0.14795911964029074, C-Loss: 14.187840640544891\n",
      "Epoch: 690, Loss: 74.04564249515533,  L-Loss: 0.14887967566028237, C-Loss: 14.779352605342865\n",
      "Epoch: 691, Loss: 71.62211954593658,  L-Loss: 0.15980275999754667, C-Loss: 14.292463421821594\n",
      "Epoch: 692, Loss: 74.704310297966,  L-Loss: 0.14624721952714026, C-Loss: 14.911612451076508\n",
      "Epoch: 693, Loss: 72.33664548397064,  L-Loss: 0.14666553167626262, C-Loss: 14.437996059656143\n",
      "Epoch: 694, Loss: 72.69491302967072,  L-Loss: 0.127673250855878, C-Loss: 14.513448059558868\n",
      "Epoch: 695, Loss: 71.16523170471191,  L-Loss: 0.13367614592425525, C-Loss: 14.206311076879501\n",
      "Epoch: 696, Loss: 71.88432288169861,  L-Loss: 0.13165886513888836, C-Loss: 14.350532710552216\n",
      "Epoch: 697, Loss: 71.53622996807098,  L-Loss: 0.1257854679133743, C-Loss: 14.282088845968246\n",
      "Epoch: 698, Loss: 71.40324139595032,  L-Loss: 0.1229134164750576, C-Loss: 14.256065547466278\n",
      "Epoch: 699, Loss: 71.16402268409729,  L-Loss: 0.13566843490116298, C-Loss: 14.205670893192291\n",
      "Epoch: 700, Loss: 71.56593811511993,  L-Loss: 0.12235750583931804, C-Loss: 14.288716077804565\n",
      "Saving model.\n",
      "Epoch: 701, Loss: 70.90725791454315,  L-Loss: 0.12611807161010802, C-Loss: 14.156228005886078\n",
      "Epoch: 702, Loss: 71.38413083553314,  L-Loss: 0.12226039776578546, C-Loss: 14.252374053001404\n",
      "Epoch: 703, Loss: 71.14266240596771,  L-Loss: 0.12989779212512076, C-Loss: 14.202553004026413\n",
      "Epoch: 704, Loss: 72.0742951631546,  L-Loss: 0.12956294580362737, C-Loss: 14.388946503400803\n",
      "Epoch: 705, Loss: 71.3298202753067,  L-Loss: 0.12956261192448437, C-Loss: 14.240051507949829\n",
      "Epoch: 706, Loss: 71.73261034488678,  L-Loss: 0.1327441402245313, C-Loss: 14.31997326016426\n",
      "Epoch: 707, Loss: 71.57020461559296,  L-Loss: 0.13290803879499435, C-Loss: 14.287459254264832\n",
      "Epoch: 708, Loss: 73.5632815361023,  L-Loss: 0.1353739022742957, C-Loss: 14.685581624507904\n",
      "Epoch: 709, Loss: 72.98903524875641,  L-Loss: 0.13819186040200293, C-Loss: 14.570168644189835\n",
      "Epoch: 710, Loss: 71.71868908405304,  L-Loss: 0.12500518513843417, C-Loss: 14.318736672401428\n",
      "Epoch: 711, Loss: 71.16430616378784,  L-Loss: 0.11461795843206346, C-Loss: 14.209937781095505\n",
      "Epoch: 712, Loss: 70.95683062076569,  L-Loss: 0.11740529909729958, C-Loss: 14.167884975671768\n",
      "Epoch: 713, Loss: 73.66487944126129,  L-Loss: 0.14401857974007726, C-Loss: 14.704172253608704\n",
      "Epoch: 714, Loss: 71.19364905357361,  L-Loss: 0.14461243548430502, C-Loss: 14.209807455539703\n",
      "Epoch: 715, Loss: 73.55028188228607,  L-Loss: 0.13120369333773851, C-Loss: 14.683815628290176\n",
      "Epoch: 716, Loss: 71.00581300258636,  L-Loss: 0.11917233746498823, C-Loss: 14.177328139543533\n",
      "Epoch: 717, Loss: 71.05270278453827,  L-Loss: 0.12470894027501345, C-Loss: 14.185598731040955\n",
      "Epoch: 718, Loss: 71.01721799373627,  L-Loss: 0.11508159292861819, C-Loss: 14.180427312850952\n",
      "Epoch: 719, Loss: 71.37971687316895,  L-Loss: 0.11587703344412148, C-Loss: 14.252767890691757\n",
      "Epoch: 720, Loss: 71.48623657226562,  L-Loss: 0.11369409994222224, C-Loss: 14.274508476257324\n",
      "Epoch: 721, Loss: 71.20044648647308,  L-Loss: 0.11924961698241532, C-Loss: 14.216239482164383\n",
      "Epoch: 722, Loss: 71.36014568805695,  L-Loss: 0.11501511675305665, C-Loss: 14.24902617931366\n",
      "Epoch: 723, Loss: 70.9188961982727,  L-Loss: 0.1213528010994196, C-Loss: 14.159508645534515\n",
      "Epoch: 724, Loss: 71.23638665676117,  L-Loss: 0.11775892390869558, C-Loss: 14.223725616931915\n",
      "Epoch: 725, Loss: 71.05039238929749,  L-Loss: 0.11327600432559848, C-Loss: 14.18742322921753\n",
      "Saving model.\n",
      "Epoch: 726, Loss: 70.88283550739288,  L-Loss: 0.11398754338733852, C-Loss: 14.153769582509995\n",
      "Epoch: 727, Loss: 70.89243948459625,  L-Loss: 0.11679934826679528, C-Loss: 14.155128002166748\n",
      "Epoch: 728, Loss: 71.86181259155273,  L-Loss: 0.1413959595374763, C-Loss: 14.344083279371262\n",
      "Epoch: 729, Loss: 71.25630819797516,  L-Loss: 0.14590464672073722, C-Loss: 14.222080677747726\n",
      "Epoch: 730, Loss: 72.21230721473694,  L-Loss: 0.1279586786404252, C-Loss: 14.416869699954987\n",
      "Epoch: 731, Loss: 71.10092258453369,  L-Loss: 0.11517122108489275, C-Loss: 14.197150230407715\n",
      "Epoch: 732, Loss: 70.97673559188843,  L-Loss: 0.11121265706606209, C-Loss: 14.173104405403137\n",
      "Epoch: 733, Loss: 71.3484023809433,  L-Loss: 0.1204255057964474, C-Loss: 14.245595306158066\n",
      "Epoch: 734, Loss: 70.8952089548111,  L-Loss: 0.1215207006316632, C-Loss: 14.154737681150436\n",
      "Epoch: 735, Loss: 72.16300642490387,  L-Loss: 0.1334429436828941, C-Loss: 14.405912578105927\n",
      "Epoch: 736, Loss: 71.29247796535492,  L-Loss: 0.13291321881115437, C-Loss: 14.231912910938263\n",
      "Epoch: 737, Loss: 75.77041816711426,  L-Loss: 0.18251490057446063, C-Loss: 15.117580622434616\n",
      "Epoch: 738, Loss: 77.12894093990326,  L-Loss: 0.16861417889595032, C-Loss: 15.392065167427063\n",
      "Epoch: 739, Loss: 77.81697130203247,  L-Loss: 0.1747459175530821, C-Loss: 15.528445154428482\n",
      "Epoch: 740, Loss: 74.49569058418274,  L-Loss: 0.13836941425688565, C-Loss: 14.871464341878891\n",
      "Epoch: 741, Loss: 71.09910666942596,  L-Loss: 0.12648523738607764, C-Loss: 14.19452428817749\n",
      "Epoch: 742, Loss: 72.50983488559723,  L-Loss: 0.13350274530239403, C-Loss: 14.475266456604004\n",
      "Epoch: 743, Loss: 75.51088190078735,  L-Loss: 0.16236269753426313, C-Loss: 15.069704055786133\n",
      "Epoch: 744, Loss: 74.1106561422348,  L-Loss: 0.12561935442499816, C-Loss: 14.797007292509079\n",
      "Epoch: 745, Loss: 71.25769448280334,  L-Loss: 0.13343880232423544, C-Loss: 14.22485101222992\n",
      "Epoch: 746, Loss: 73.7712174654007,  L-Loss: 0.12690439959987998, C-Loss: 14.72886261343956\n",
      "Epoch: 747, Loss: 71.61832594871521,  L-Loss: 0.12980924546718597, C-Loss: 14.297703385353088\n",
      "Epoch: 748, Loss: 74.66649508476257,  L-Loss: 0.14769691554829478, C-Loss: 14.903759777545929\n",
      "Epoch: 749, Loss: 72.32581198215485,  L-Loss: 0.14649282093159854, C-Loss: 14.435864001512527\n",
      "Epoch: 750, Loss: 71.94873046875,  L-Loss: 0.12094448902644217, C-Loss: 14.365557253360748\n",
      "Epoch: 751, Loss: 71.24392020702362,  L-Loss: 0.11953313904814422, C-Loss: 14.22487735748291\n",
      "Epoch: 752, Loss: 72.0055228471756,  L-Loss: 0.1309632535558194, C-Loss: 14.374911934137344\n",
      "Saving model.\n",
      "Epoch: 753, Loss: 70.85681581497192,  L-Loss: 0.12637437041848898, C-Loss: 14.146088272333145\n",
      "Epoch: 754, Loss: 73.8793214559555,  L-Loss: 0.13772557326592505, C-Loss: 14.748319268226624\n",
      "Epoch: 755, Loss: 72.14718949794769,  L-Loss: 0.12567198765464127, C-Loss: 14.40430337190628\n",
      "Epoch: 756, Loss: 75.5353524684906,  L-Loss: 0.13623172813095152, C-Loss: 15.079824209213257\n",
      "Epoch: 757, Loss: 71.16395461559296,  L-Loss: 0.12982576503418386, C-Loss: 14.20682579278946\n",
      "Epoch: 758, Loss: 71.34121561050415,  L-Loss: 0.12148150219582021, C-Loss: 14.243946820497513\n",
      "Epoch: 759, Loss: 71.43516159057617,  L-Loss: 0.1092172043863684, C-Loss: 14.265188813209534\n",
      "Epoch: 760, Loss: 71.09490823745728,  L-Loss: 0.10608466295525432, C-Loss: 14.197764724493027\n",
      "Epoch: 761, Loss: 71.04571950435638,  L-Loss: 0.10393037274479866, C-Loss: 14.188357710838318\n",
      "Epoch: 762, Loss: 70.8855437040329,  L-Loss: 0.10429681930691004, C-Loss: 14.15624949336052\n",
      "Epoch: 763, Loss: 71.1475899219513,  L-Loss: 0.10633719037286937, C-Loss: 14.208250552415848\n",
      "Saving model.\n",
      "Epoch: 764, Loss: 70.67102813720703,  L-Loss: 0.1109008917119354, C-Loss: 14.112025499343872\n",
      "Epoch: 765, Loss: 71.52204060554504,  L-Loss: 0.11585622583515942, C-Loss: 14.281236946582794\n",
      "Epoch: 766, Loss: 70.84801542758942,  L-Loss: 0.12709403922781348, C-Loss: 14.144184201955795\n",
      "Epoch: 767, Loss: 71.86907732486725,  L-Loss: 0.1264981236308813, C-Loss: 14.348515897989273\n",
      "Epoch: 768, Loss: 72.562460064888,  L-Loss: 0.13417912484146655, C-Loss: 14.48565611243248\n",
      "Epoch: 769, Loss: 75.99670720100403,  L-Loss: 0.14462792337872088, C-Loss: 15.170415997505188\n",
      "Epoch: 770, Loss: 71.37606179714203,  L-Loss: 0.13600041344761848, C-Loss: 14.24801230430603\n",
      "Epoch: 771, Loss: 71.5906890630722,  L-Loss: 0.11761113302782178, C-Loss: 14.294615536928177\n",
      "Epoch: 772, Loss: 70.88771486282349,  L-Loss: 0.12766197277233005, C-Loss: 14.152010649442673\n",
      "Epoch: 773, Loss: 72.21963477134705,  L-Loss: 0.13557367329485714, C-Loss: 14.416812270879745\n",
      "Epoch: 774, Loss: 71.04859066009521,  L-Loss: 0.1447321050800383, C-Loss: 14.180771708488464\n",
      "Epoch: 775, Loss: 75.57530462741852,  L-Loss: 0.13744072429835796, C-Loss: 15.087572872638702\n",
      "Epoch: 776, Loss: 70.83634686470032,  L-Loss: 0.1163850596640259, C-Loss: 14.143992364406586\n",
      "Epoch: 777, Loss: 70.82375264167786,  L-Loss: 0.11345567530952394, C-Loss: 14.142059355974197\n",
      "Epoch: 778, Loss: 70.88608264923096,  L-Loss: 0.11183815402910113, C-Loss: 14.154848963022232\n",
      "Epoch: 779, Loss: 71.36006307601929,  L-Loss: 0.11137319938279688, C-Loss: 14.249737918376923\n",
      "Epoch: 780, Loss: 70.91247010231018,  L-Loss: 0.11722903372719884, C-Loss: 14.159048110246658\n",
      "Epoch: 781, Loss: 72.1503757238388,  L-Loss: 0.12059417855925858, C-Loss: 14.40595617890358\n",
      "Epoch: 782, Loss: 70.79535472393036,  L-Loss: 0.14157151500694454, C-Loss: 14.130756735801697\n",
      "Epoch: 783, Loss: 77.83169782161713,  L-Loss: 0.15840901457704604, C-Loss: 15.534657716751099\n",
      "Epoch: 784, Loss: 101.0828343629837,  L-Loss: 0.23247184068895876, C-Loss: 20.17007267475128\n",
      "Epoch: 785, Loss: 83.58010411262512,  L-Loss: 0.21154813328757882, C-Loss: 16.673711210489273\n",
      "Epoch: 786, Loss: 99.29154205322266,  L-Loss: 0.25623112288303673, C-Loss: 19.807062208652496\n",
      "Epoch: 787, Loss: 84.43922674655914,  L-Loss: 0.21200177096761763, C-Loss: 16.845445036888123\n",
      "Epoch: 788, Loss: 98.4760434627533,  L-Loss: 0.24227321660146117, C-Loss: 19.646753937005997\n",
      "Epoch: 789, Loss: 100.40585100650787,  L-Loss: 0.25688529619947076, C-Loss: 20.029793053865433\n",
      "Epoch: 790, Loss: 92.76914119720459,  L-Loss: 0.17958508525043726, C-Loss: 18.517911165952682\n",
      "Epoch: 791, Loss: 102.46064639091492,  L-Loss: 0.2598622404038906, C-Loss: 20.44015645980835\n",
      "Epoch: 792, Loss: 87.44211709499359,  L-Loss: 0.16880692401900887, C-Loss: 17.454662144184113\n",
      "Epoch: 793, Loss: 75.58208417892456,  L-Loss: 0.1548553507309407, C-Loss: 15.085445672273636\n",
      "Epoch: 794, Loss: 88.15750443935394,  L-Loss: 0.17384550580754876, C-Loss: 17.59673175215721\n",
      "Epoch: 795, Loss: 73.89770841598511,  L-Loss: 0.18736515007913113, C-Loss: 14.742068767547607\n",
      "Epoch: 796, Loss: 87.72913038730621,  L-Loss: 0.20505787222646177, C-Loss: 17.50481426715851\n",
      "Epoch: 797, Loss: 72.61898756027222,  L-Loss: 0.1523115064483136, C-Loss: 14.493335217237473\n",
      "Epoch: 798, Loss: 72.77294480800629,  L-Loss: 0.1233754656277597, C-Loss: 14.529913872480392\n",
      "Epoch: 799, Loss: 71.0945793390274,  L-Loss: 0.12320841895416379, C-Loss: 14.194274246692657\n",
      "Epoch: 800, Loss: 75.72788226604462,  L-Loss: 0.16236837184987962, C-Loss: 15.113102793693542\n",
      "Epoch: 801, Loss: 71.5313880443573,  L-Loss: 0.14588375110179186, C-Loss: 14.277100831270218\n",
      "Epoch: 802, Loss: 74.41775035858154,  L-Loss: 0.1148313507437706, C-Loss: 14.860583782196045\n",
      "Epoch: 803, Loss: 71.04779314994812,  L-Loss: 0.11242505954578519, C-Loss: 14.187073528766632\n",
      "Epoch: 804, Loss: 71.7303181886673,  L-Loss: 0.10806297953240573, C-Loss: 14.324451088905334\n",
      "Epoch: 805, Loss: 70.94685220718384,  L-Loss: 0.10918473242782056, C-Loss: 14.167533576488495\n",
      "Epoch: 806, Loss: 71.80750334262848,  L-Loss: 0.10917132976464927, C-Loss: 14.339666247367859\n",
      "Epoch: 807, Loss: 70.77054476737976,  L-Loss: 0.1099861329421401, C-Loss: 14.132111757993698\n",
      "Epoch: 808, Loss: 72.18572318553925,  L-Loss: 0.10862535680644214, C-Loss: 14.415419667959213\n",
      "Epoch: 809, Loss: 70.75291574001312,  L-Loss: 0.10798760666511953, C-Loss: 14.128985613584518\n",
      "Epoch: 810, Loss: 71.83192825317383,  L-Loss: 0.10586885293014348, C-Loss: 14.34521198272705\n",
      "Epoch: 811, Loss: 70.75527942180634,  L-Loss: 0.10464650928042829, C-Loss: 14.130126506090164\n",
      "Epoch: 812, Loss: 71.37986028194427,  L-Loss: 0.10329969110898674, C-Loss: 14.25531217455864\n",
      "Epoch: 813, Loss: 70.747873544693,  L-Loss: 0.10213332017883658, C-Loss: 14.129147976636887\n",
      "Epoch: 814, Loss: 71.37333679199219,  L-Loss: 0.10210239700973034, C-Loss: 14.254246771335602\n",
      "Epoch: 815, Loss: 70.7038904428482,  L-Loss: 0.10147811193019152, C-Loss: 14.120482355356216\n",
      "Epoch: 816, Loss: 71.19710731506348,  L-Loss: 0.10140390251763165, C-Loss: 14.21914067864418\n",
      "Epoch: 817, Loss: 70.69963765144348,  L-Loss: 0.10133858397603035, C-Loss: 14.119659811258316\n",
      "Epoch: 818, Loss: 71.13770055770874,  L-Loss: 0.10181667981669307, C-Loss: 14.207176804542542\n",
      "Epoch: 819, Loss: 70.67885625362396,  L-Loss: 0.10235295630991459, C-Loss: 14.115300565958023\n",
      "Epoch: 820, Loss: 71.33540213108063,  L-Loss: 0.10335543937981129, C-Loss: 14.246409267187119\n",
      "Saving model.\n",
      "Epoch: 821, Loss: 70.65707516670227,  L-Loss: 0.10435487958602607, C-Loss: 14.110543966293335\n",
      "Epoch: 822, Loss: 71.65683245658875,  L-Loss: 0.10623066406697035, C-Loss: 14.310120403766632\n",
      "Epoch: 823, Loss: 70.66961967945099,  L-Loss: 0.10878162458539009, C-Loss: 14.112167626619339\n",
      "Epoch: 824, Loss: 72.25921583175659,  L-Loss: 0.11206434504128993, C-Loss: 14.429430335760117\n",
      "Epoch: 825, Loss: 70.70463919639587,  L-Loss: 0.11091172019951046, C-Loss: 14.11874544620514\n",
      "Epoch: 826, Loss: 74.02518081665039,  L-Loss: 0.12615888426080346, C-Loss: 14.779804438352585\n",
      "Epoch: 827, Loss: 70.72412693500519,  L-Loss: 0.12491076812148094, C-Loss: 14.119843244552612\n",
      "Epoch: 828, Loss: 71.32297623157501,  L-Loss: 0.11400885204784572, C-Loss: 14.241793543100357\n",
      "Epoch: 829, Loss: 70.704843044281,  L-Loss: 0.11479427036829293, C-Loss: 14.118009865283966\n",
      "Epoch: 830, Loss: 72.6785386800766,  L-Loss: 0.12560624303296208, C-Loss: 14.510586380958557\n",
      "Epoch: 831, Loss: 70.68775749206543,  L-Loss: 0.12476979685015976, C-Loss: 14.112597465515137\n",
      "Epoch: 832, Loss: 72.85333633422852,  L-Loss: 0.11922341911122203, C-Loss: 14.546822786331177\n",
      "Epoch: 833, Loss: 71.46327757835388,  L-Loss: 0.12183459009975195, C-Loss: 14.268288612365723\n",
      "Epoch: 834, Loss: 71.0695128440857,  L-Loss: 0.11369073623791337, C-Loss: 14.191164553165436\n",
      "Epoch: 835, Loss: 70.96334755420685,  L-Loss: 0.12092845258302987, C-Loss: 14.168483853340149\n",
      "Epoch: 836, Loss: 72.39993476867676,  L-Loss: 0.1235769612248987, C-Loss: 14.455271512269974\n",
      "Epoch: 837, Loss: 70.69394671916962,  L-Loss: 0.11634673876687884, C-Loss: 14.115520149469376\n",
      "Epoch: 838, Loss: 72.5443422794342,  L-Loss: 0.1167599365580827, C-Loss: 14.485516488552094\n",
      "Epoch: 839, Loss: 70.71517324447632,  L-Loss: 0.1266336019616574, C-Loss: 14.117707997560501\n",
      "Epoch: 840, Loss: 71.72972893714905,  L-Loss: 0.11528976238332689, C-Loss: 14.32288783788681\n",
      "Epoch: 841, Loss: 72.14186751842499,  L-Loss: 0.1181275809649378, C-Loss: 14.404747992753983\n",
      "Epoch: 842, Loss: 73.49990797042847,  L-Loss: 0.13056384189985693, C-Loss: 14.673868745565414\n",
      "Epoch: 843, Loss: 70.81198608875275,  L-Loss: 0.13843335094861686, C-Loss: 14.13471058011055\n",
      "Epoch: 844, Loss: 71.63546800613403,  L-Loss: 0.1182628117967397, C-Loss: 14.30344107747078\n",
      "Epoch: 845, Loss: 71.20810115337372,  L-Loss: 0.1274128088261932, C-Loss: 14.216137707233429\n",
      "Epoch: 846, Loss: 71.7524905204773,  L-Loss: 0.12709759338758886, C-Loss: 14.325078547000885\n",
      "Epoch: 847, Loss: 71.23343586921692,  L-Loss: 0.12338966270908713, C-Loss: 14.222009271383286\n",
      "Epoch: 848, Loss: 71.18132793903351,  L-Loss: 0.11263635661453009, C-Loss: 14.21373838186264\n",
      "Epoch: 849, Loss: 70.76090061664581,  L-Loss: 0.1106353122740984, C-Loss: 14.130053251981735\n",
      "Epoch: 850, Loss: 71.71819353103638,  L-Loss: 0.10820325557142496, C-Loss: 14.321997970342636\n",
      "Epoch: 851, Loss: 70.66861093044281,  L-Loss: 0.11049623554572463, C-Loss: 14.111622959375381\n",
      "Epoch: 852, Loss: 71.64810812473297,  L-Loss: 0.1143092424608767, C-Loss: 14.306759804487228\n",
      "Epoch: 853, Loss: 71.00552797317505,  L-Loss: 0.11954583483748138, C-Loss: 14.17719641327858\n",
      "Epoch: 854, Loss: 74.14346599578857,  L-Loss: 0.12977849086746573, C-Loss: 14.802737444639206\n",
      "Epoch: 855, Loss: 70.91151320934296,  L-Loss: 0.11948080430738628, C-Loss: 14.158406436443329\n",
      "Epoch: 856, Loss: 71.48002696037292,  L-Loss: 0.10905348812229931, C-Loss: 14.274194717407227\n",
      "Epoch: 857, Loss: 70.76855826377869,  L-Loss: 0.11387083819136024, C-Loss: 14.130937427282333\n",
      "Epoch: 858, Loss: 73.80169451236725,  L-Loss: 0.13064997224137187, C-Loss: 14.734208852052689\n",
      "Epoch: 859, Loss: 70.88837110996246,  L-Loss: 0.12826374918222427, C-Loss: 14.152021437883377\n",
      "Epoch: 860, Loss: 71.91547560691833,  L-Loss: 0.12613497069105506, C-Loss: 14.357868134975433\n",
      "Epoch: 861, Loss: 72.79830801486969,  L-Loss: 0.13661839882843196, C-Loss: 14.532338082790375\n",
      "Epoch: 862, Loss: 72.40124297142029,  L-Loss: 0.13675500079989433, C-Loss: 14.452897429466248\n",
      "Epoch: 863, Loss: 70.91135394573212,  L-Loss: 0.13285056897439063, C-Loss: 14.155700653791428\n",
      "Epoch: 864, Loss: 73.41850733757019,  L-Loss: 0.13004517112858593, C-Loss: 14.657692283391953\n",
      "Epoch: 865, Loss: 76.28405439853668,  L-Loss: 0.17766833771020174, C-Loss: 15.221277058124542\n",
      "Epoch: 866, Loss: 90.25023329257965,  L-Loss: 0.19556340598501265, C-Loss: 18.010933756828308\n",
      "Epoch: 867, Loss: 112.41130375862122,  L-Loss: 0.3107190600130707, C-Loss: 22.420116990804672\n",
      "Epoch: 868, Loss: 118.7295126914978,  L-Loss: 0.3275113822892308, C-Loss: 23.680400401353836\n",
      "Epoch: 869, Loss: 121.72446000576019,  L-Loss: 0.392570927971974, C-Loss: 24.266377359628677\n",
      "Epoch: 870, Loss: 167.91047382354736,  L-Loss: 0.7694544345140457, C-Loss: 33.42820402979851\n",
      "Epoch: 871, Loss: 137.55438208580017,  L-Loss: 0.4312695749104023, C-Loss: 27.4246224462986\n",
      "Epoch: 872, Loss: 155.2605218887329,  L-Loss: 1.0696146539412439, C-Loss: 30.83818143606186\n",
      "Epoch: 873, Loss: 133.57461655139923,  L-Loss: 0.642779293935746, C-Loss: 26.586367398500443\n",
      "Epoch: 874, Loss: 115.83545565605164,  L-Loss: 0.5151871829293668, C-Loss: 23.064053684473038\n",
      "Epoch: 875, Loss: 118.52713167667389,  L-Loss: 0.43471109168604016, C-Loss: 23.618483781814575\n",
      "Epoch: 876, Loss: 116.8038147687912,  L-Loss: 0.3913665567524731, C-Loss: 23.28248965740204\n",
      "Epoch: 877, Loss: 130.17156147956848,  L-Loss: 0.4665414225310087, C-Loss: 25.94100385904312\n",
      "Epoch: 878, Loss: 87.26138842105865,  L-Loss: 0.23528941767290235, C-Loss: 17.405219823122025\n",
      "Epoch: 879, Loss: 86.39642333984375,  L-Loss: 0.20056640706025064, C-Loss: 17.239171504974365\n",
      "Epoch: 880, Loss: 80.00039982795715,  L-Loss: 0.208272636635229, C-Loss: 15.958425283432007\n",
      "Epoch: 881, Loss: 73.93158257007599,  L-Loss: 0.14302113745361567, C-Loss: 14.757712364196777\n",
      "Epoch: 882, Loss: 73.75013518333435,  L-Loss: 0.15537805459462106, C-Loss: 14.718951493501663\n",
      "Epoch: 883, Loss: 72.80441677570343,  L-Loss: 0.13869291986338794, C-Loss: 14.533144772052765\n",
      "Epoch: 884, Loss: 72.04323947429657,  L-Loss: 0.1370271285995841, C-Loss: 14.381242424249649\n",
      "Epoch: 885, Loss: 71.57752013206482,  L-Loss: 0.13313243025913835, C-Loss: 14.288877457380295\n",
      "Epoch: 886, Loss: 71.65096783638,  L-Loss: 0.13194700074382126, C-Loss: 14.303804188966751\n",
      "Epoch: 887, Loss: 71.59197354316711,  L-Loss: 0.13147192727774382, C-Loss: 14.292100369930267\n",
      "Epoch: 888, Loss: 75.83359158039093,  L-Loss: 0.13734017102979124, C-Loss: 15.139250308275223\n",
      "Epoch: 889, Loss: 71.6956090927124,  L-Loss: 0.15507500292733312, C-Loss: 14.308106809854507\n",
      "Epoch: 890, Loss: 77.16278505325317,  L-Loss: 0.13892116979695857, C-Loss: 15.40477266907692\n",
      "Epoch: 891, Loss: 71.10075116157532,  L-Loss: 0.1743482956662774, C-Loss: 14.185280621051788\n",
      "Epoch: 892, Loss: 87.89249920845032,  L-Loss: 0.1713023241609335, C-Loss: 17.544239312410355\n",
      "Epoch: 893, Loss: 71.48473787307739,  L-Loss: 0.12975394795648754, C-Loss: 14.270996749401093\n",
      "Epoch: 894, Loss: 71.8554675579071,  L-Loss: 0.12618511426262558, C-Loss: 14.34585651755333\n",
      "Epoch: 895, Loss: 70.90824270248413,  L-Loss: 0.12935051624663174, C-Loss: 14.15577843785286\n",
      "Epoch: 896, Loss: 75.28585040569305,  L-Loss: 0.1337643819861114, C-Loss: 15.030417203903198\n",
      "Epoch: 897, Loss: 71.06421947479248,  L-Loss: 0.13943055947311223, C-Loss: 14.184957951307297\n",
      "Epoch: 898, Loss: 81.65724337100983,  L-Loss: 0.14759051240980625, C-Loss: 16.30193054676056\n",
      "Epoch: 899, Loss: 71.09888529777527,  L-Loss: 0.1397259864024818, C-Loss: 14.191831886768341\n",
      "Epoch: 900, Loss: 73.84622740745544,  L-Loss: 0.12593907583504915, C-Loss: 14.744057714939117\n",
      "Epoch: 901, Loss: 71.15288007259369,  L-Loss: 0.1380037555936724, C-Loss: 14.202975332736969\n",
      "Epoch: 902, Loss: 80.13332331180573,  L-Loss: 0.1432719025760889, C-Loss: 15.998010158538818\n",
      "Epoch: 903, Loss: 71.75123751163483,  L-Loss: 0.1426683182362467, C-Loss: 14.321713775396347\n",
      "Epoch: 904, Loss: 76.70771908760071,  L-Loss: 0.13577701547183096, C-Loss: 15.314388394355774\n",
      "Epoch: 905, Loss: 71.82859647274017,  L-Loss: 0.1366774863563478, C-Loss: 14.338383883237839\n",
      "Epoch: 906, Loss: 71.10758781433105,  L-Loss: 0.12097524269483984, C-Loss: 14.197322726249695\n",
      "Epoch: 907, Loss: 71.13183391094208,  L-Loss: 0.12251648819074035, C-Loss: 14.201863467693329\n",
      "Epoch: 908, Loss: 71.15563488006592,  L-Loss: 0.118607779731974, C-Loss: 14.207405418157578\n",
      "Epoch: 909, Loss: 70.86904048919678,  L-Loss: 0.11781895998865366, C-Loss: 14.150244355201721\n",
      "Epoch: 910, Loss: 71.01544570922852,  L-Loss: 0.12133344309404492, C-Loss: 14.178822547197342\n",
      "Epoch: 911, Loss: 71.65935730934143,  L-Loss: 0.1312106605619192, C-Loss: 14.305629223585129\n",
      "Epoch: 912, Loss: 70.70693922042847,  L-Loss: 0.12186445505358279, C-Loss: 14.117014855146408\n",
      "Epoch: 913, Loss: 71.79101312160492,  L-Loss: 0.12371181161142886, C-Loss: 14.333460181951523\n",
      "Epoch: 914, Loss: 70.68554508686066,  L-Loss: 0.11782589391805232, C-Loss: 14.113543778657913\n",
      "Epoch: 915, Loss: 70.76081812381744,  L-Loss: 0.11263942089863122, C-Loss: 14.129635721445084\n",
      "Epoch: 916, Loss: 70.90828657150269,  L-Loss: 0.11446385481394827, C-Loss: 14.15876454114914\n",
      "Epoch: 917, Loss: 71.05366063117981,  L-Loss: 0.11848454573191702, C-Loss: 14.18703517317772\n",
      "Epoch: 918, Loss: 70.68171465396881,  L-Loss: 0.1268379793036729, C-Loss: 14.110975295305252\n",
      "Epoch: 919, Loss: 71.46827018260956,  L-Loss: 0.12426988151855767, C-Loss: 14.268800139427185\n",
      "Epoch: 920, Loss: 70.72880482673645,  L-Loss: 0.1328317855950445, C-Loss: 14.119194567203522\n",
      "Epoch: 921, Loss: 80.51081550121307,  L-Loss: 0.15163961541838944, C-Loss: 16.0718352496624\n",
      "Epoch: 922, Loss: 72.87985336780548,  L-Loss: 0.14167042076587677, C-Loss: 14.547636687755585\n",
      "Epoch: 923, Loss: 77.25854396820068,  L-Loss: 0.12564183119684458, C-Loss: 15.426580399274826\n",
      "Epoch: 924, Loss: 71.6679276227951,  L-Loss: 0.133272249950096, C-Loss: 14.306931018829346\n",
      "Epoch: 925, Loss: 70.97032356262207,  L-Loss: 0.11166207701899111, C-Loss: 14.171732306480408\n",
      "Epoch: 926, Loss: 70.76173877716064,  L-Loss: 0.11258878069929779, C-Loss: 14.129829972982407\n",
      "Epoch: 927, Loss: 71.79481959342957,  L-Loss: 0.11863958090543747, C-Loss: 14.335235983133316\n",
      "Epoch: 928, Loss: 70.70727491378784,  L-Loss: 0.12615976389497519, C-Loss: 14.116223067045212\n",
      "Epoch: 929, Loss: 73.52995800971985,  L-Loss: 0.13235352630726993, C-Loss: 14.67952087521553\n",
      "Epoch: 930, Loss: 71.57791221141815,  L-Loss: 0.1438628954347223, C-Loss: 14.286809921264648\n",
      "Epoch: 931, Loss: 75.19267344474792,  L-Loss: 0.1297608818858862, C-Loss: 15.012582510709763\n",
      "Epoch: 932, Loss: 70.86500012874603,  L-Loss: 0.12065435526892543, C-Loss: 14.148869127035141\n",
      "Epoch: 933, Loss: 72.21504139900208,  L-Loss: 0.1153844513464719, C-Loss: 14.419931381940842\n",
      "Epoch: 934, Loss: 71.0033028125763,  L-Loss: 0.11232699267566204, C-Loss: 14.178195178508759\n",
      "Epoch: 935, Loss: 70.75449109077454,  L-Loss: 0.11220277030952275, C-Loss: 14.128457635641098\n",
      "Epoch: 936, Loss: 70.6879552602768,  L-Loss: 0.1121174399740994, C-Loss: 14.115167438983917\n",
      "Saving model.\n",
      "Epoch: 937, Loss: 70.62814509868622,  L-Loss: 0.11808783956803381, C-Loss: 14.10201147198677\n",
      "Epoch: 938, Loss: 70.9090987443924,  L-Loss: 0.12181893107481301, C-Loss: 14.15745598077774\n",
      "Epoch: 939, Loss: 70.68217635154724,  L-Loss: 0.1256073445547372, C-Loss: 14.111313849687576\n",
      "Epoch: 940, Loss: 71.39702725410461,  L-Loss: 0.1222229793202132, C-Loss: 14.254960834980011\n",
      "Epoch: 941, Loss: 70.74536788463593,  L-Loss: 0.1235177235212177, C-Loss: 14.124370068311691\n",
      "Epoch: 942, Loss: 74.50735139846802,  L-Loss: 0.13316109706647694, C-Loss: 14.874838024377823\n",
      "Epoch: 943, Loss: 70.7994042634964,  L-Loss: 0.12981186341494322, C-Loss: 14.133918434381485\n",
      "Epoch: 944, Loss: 71.55406785011292,  L-Loss: 0.11604159092530608, C-Loss: 14.287605196237564\n",
      "Epoch: 945, Loss: 70.91648507118225,  L-Loss: 0.1197764384560287, C-Loss: 14.1593416929245\n",
      "Epoch: 946, Loss: 71.57869875431061,  L-Loss: 0.12294492172077298, C-Loss: 14.291150748729706\n",
      "Epoch: 947, Loss: 70.81861627101898,  L-Loss: 0.13641997752711177, C-Loss: 14.136439234018326\n",
      "Epoch: 948, Loss: 73.5827488899231,  L-Loss: 0.13508019037544727, C-Loss: 14.689533680677414\n",
      "Epoch: 949, Loss: 71.53731107711792,  L-Loss: 0.12650867667980492, C-Loss: 14.282160520553589\n",
      "Epoch: 950, Loss: 72.36302053928375,  L-Loss: 0.119674286339432, C-Loss: 14.448669224977493\n",
      "Epoch: 951, Loss: 70.94563019275665,  L-Loss: 0.125226438511163, C-Loss: 14.164080709218979\n",
      "Epoch: 952, Loss: 70.86842966079712,  L-Loss: 0.1137941237539053, C-Loss: 14.150927066802979\n",
      "Epoch: 953, Loss: 70.64641845226288,  L-Loss: 0.1097294744104147, C-Loss: 14.107337802648544\n",
      "Epoch: 954, Loss: 71.42501509189606,  L-Loss: 0.11604498652741313, C-Loss: 14.261793971061707\n",
      "Epoch: 955, Loss: 70.85817778110504,  L-Loss: 0.10998264630325139, C-Loss: 14.14963886141777\n",
      "Epoch: 956, Loss: 70.66390693187714,  L-Loss: 0.10994847468100488, C-Loss: 14.110791623592377\n",
      "Epoch: 957, Loss: 71.12556028366089,  L-Loss: 0.11116884392686188, C-Loss: 14.202878206968307\n",
      "Saving model.\n",
      "Epoch: 958, Loss: 70.52807199954987,  L-Loss: 0.11181511939503253, C-Loss: 14.083251297473907\n",
      "Epoch: 959, Loss: 71.06159472465515,  L-Loss: 0.11658821278251708, C-Loss: 14.18900129199028\n",
      "Epoch: 960, Loss: 70.61976826190948,  L-Loss: 0.11646772897802293, C-Loss: 14.100660145282745\n",
      "Epoch: 961, Loss: 70.87143933773041,  L-Loss: 0.11604715068824589, C-Loss: 14.151078522205353\n",
      "Epoch: 962, Loss: 70.7708512544632,  L-Loss: 0.11387352831661701, C-Loss: 14.131395637989044\n",
      "Epoch: 963, Loss: 70.72993266582489,  L-Loss: 0.10533696878701448, C-Loss: 14.124919176101685\n",
      "Epoch: 964, Loss: 70.72062599658966,  L-Loss: 0.10285975993610919, C-Loss: 14.12355324625969\n",
      "Epoch: 965, Loss: 70.68928074836731,  L-Loss: 0.1038179243914783, C-Loss: 14.11709263920784\n",
      "Epoch: 966, Loss: 70.65732431411743,  L-Loss: 0.10522869019769132, C-Loss: 14.110419243574142\n",
      "Epoch: 967, Loss: 70.65256524085999,  L-Loss: 0.10932658496312797, C-Loss: 14.10864770412445\n",
      "Epoch: 968, Loss: 71.1108273267746,  L-Loss: 0.11860078806057572, C-Loss: 14.198445320129395\n",
      "Epoch: 969, Loss: 94.25221884250641,  L-Loss: 0.2026515188626945, C-Loss: 18.809913456439972\n",
      "Epoch: 970, Loss: 95.72559344768524,  L-Loss: 0.21047214954160154, C-Loss: 19.10302397608757\n",
      "Epoch: 971, Loss: 81.50849044322968,  L-Loss: 0.15769922267645597, C-Loss: 16.27015808224678\n",
      "Epoch: 972, Loss: 75.0712012052536,  L-Loss: 0.1512085844296962, C-Loss: 14.983998447656631\n",
      "Epoch: 973, Loss: 75.90990662574768,  L-Loss: 0.14336206787265837, C-Loss: 15.153308928012848\n",
      "Epoch: 974, Loss: 72.52757287025452,  L-Loss: 0.14570271549746394, C-Loss: 14.47637403011322\n",
      "Epoch: 975, Loss: 73.10487592220306,  L-Loss: 0.12984776333905756, C-Loss: 14.595005601644516\n",
      "Epoch: 976, Loss: 71.50637423992157,  L-Loss: 0.12944192648865283, C-Loss: 14.27538651227951\n",
      "Epoch: 977, Loss: 72.8644472360611,  L-Loss: 0.12703566695563495, C-Loss: 14.547482311725616\n",
      "Epoch: 978, Loss: 72.9612683057785,  L-Loss: 0.12154022417962551, C-Loss: 14.567945539951324\n",
      "Epoch: 979, Loss: 71.39342772960663,  L-Loss: 0.11085925484076142, C-Loss: 14.256513714790344\n",
      "Epoch: 980, Loss: 71.94979643821716,  L-Loss: 0.10877306433394551, C-Loss: 14.368204802274704\n",
      "Epoch: 981, Loss: 70.65011608600616,  L-Loss: 0.1055932892486453, C-Loss: 14.108904540538788\n",
      "Epoch: 982, Loss: 70.97424256801605,  L-Loss: 0.11056674993596971, C-Loss: 14.172735124826431\n",
      "Epoch: 983, Loss: 70.65227472782135,  L-Loss: 0.12182152201421559, C-Loss: 14.106090635061264\n",
      "Epoch: 984, Loss: 71.26396334171295,  L-Loss: 0.12528960360214114, C-Loss: 14.227734744548798\n",
      "Epoch: 985, Loss: 70.8350738286972,  L-Loss: 0.13222568319179118, C-Loss: 14.140569597482681\n",
      "Epoch: 986, Loss: 71.28396713733673,  L-Loss: 0.11056806449778378, C-Loss: 14.234679818153381\n",
      "Epoch: 987, Loss: 70.8878698348999,  L-Loss: 0.10816450812853873, C-Loss: 14.155941158533096\n",
      "Epoch: 988, Loss: 71.0820722579956,  L-Loss: 0.10502511053346097, C-Loss: 14.195409417152405\n",
      "Epoch: 989, Loss: 70.86528944969177,  L-Loss: 0.10874839359894395, C-Loss: 14.15130826830864\n",
      "Epoch: 990, Loss: 71.42881309986115,  L-Loss: 0.10912214498966932, C-Loss: 14.263938158750534\n",
      "Epoch: 991, Loss: 70.65926933288574,  L-Loss: 0.1098777474835515, C-Loss: 14.10987839102745\n",
      "Epoch: 992, Loss: 70.57489800453186,  L-Loss: 0.10370286065153778, C-Loss: 14.094239085912704\n",
      "Epoch: 993, Loss: 71.19213461875916,  L-Loss: 0.10432217409834266, C-Loss: 14.217562437057495\n",
      "Epoch: 994, Loss: 70.64165842533112,  L-Loss: 0.10756965703330934, C-Loss: 14.106817662715912\n",
      "Epoch: 995, Loss: 70.62759757041931,  L-Loss: 0.11175922979600728, C-Loss: 14.103167712688446\n",
      "Epoch: 996, Loss: 70.75371098518372,  L-Loss: 0.11471606185659766, C-Loss: 14.127799063920975\n",
      "Epoch: 997, Loss: 70.84358978271484,  L-Loss: 0.1200652476400137, C-Loss: 14.144704908132553\n",
      "Epoch: 998, Loss: 70.93821680545807,  L-Loss: 0.1189978753682226, C-Loss: 14.163843750953674\n",
      "Epoch: 999, Loss: 70.65518057346344,  L-Loss: 0.12864450155757368, C-Loss: 14.105307310819626\n",
      "Epoch: 1000, Loss: 73.2013372182846,  L-Loss: 0.132993352599442, C-Loss: 14.613668739795685\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training!\")\n",
    "best_loss = np.infty\n",
    "for epoch in range(num_epochs+1): \n",
    "    # Training.\n",
    "    net.train()\n",
    "    loss_tracker = 0.0\n",
    "    latent_loss_tracker = 0.0\n",
    "    cor_loss_tracker = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()      \n",
    "\n",
    "        # Pass x, y to network. Retrieve both encodings, and decoding of ys encoding.\n",
    "        fx_x, fe_y, fd_z = net(x, y)\n",
    "        # Calc loss.\n",
    "        l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "        # Normalize losses by batch.\n",
    "        l_loss /= x.shape[0]\n",
    "        c_loss /= x.shape[0]\n",
    "        loss = l_loss + net.alpha*c_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_tracker+=loss.item()\n",
    "        latent_loss_tracker+=l_loss.item()\n",
    "        cor_loss_tracker+=c_loss.item()\n",
    "    writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "    writer.add_scalar('train/latent_loss', latent_loss_tracker, epoch)\n",
    "    writer.add_scalar('train/corr_loss', cor_loss_tracker, epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    net.eval()\n",
    "    loss_tracker = 0.0\n",
    "    latent_loss_tracker = 0.0\n",
    "    cor_loss_tracker = 0.0\n",
    "    acc_track = 0.0\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # evaluation only requires x. As its just Fd(Fx(x))\n",
    "        fx_x, fe_y = net.Fx(x), net.Fe(y)\n",
    "        fd_z = net.Fd(fx_x)\n",
    "\n",
    "        l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "        # Normalize losses by batch.\n",
    "        l_loss /= x.shape[0]\n",
    "        c_loss /= x.shape[0]\n",
    "        loss = l_loss + net.alpha*c_loss\n",
    "        \n",
    "        latent_loss_tracker += l_loss.item()\n",
    "        cor_loss_tracker += c_loss.item()\n",
    "        loss_tracker += loss.item()\n",
    "        lab_preds = torch.round(net.Fd(net.Fx(x))).cpu().detach().numpy()\n",
    "        acc_track += accuracy_score(y.cpu().detach().numpy(), lab_preds)\n",
    "        \n",
    "    if loss_tracker < best_loss:\n",
    "        best_loss = loss_tracker\n",
    "        print(\"Saving model.\")\n",
    "        torch.save(net.state_dict(), f'./models/scene_best/best.pt')\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss_tracker},  L-Loss: {latent_loss_tracker}, C-Loss: {cor_loss_tracker}\")\n",
    "    writer.add_scalar('val/loss', loss_tracker, epoch)\n",
    "    writer.add_scalar('val/latent_loss', latent_loss_tracker, epoch)\n",
    "    writer.add_scalar('val/corr_loss', cor_loss_tracker, epoch)\n",
    "    writer.add_scalar('val/acc', acc_track, epoch)\n",
    "writer.add_hparams(hparam_dict=eval_metrics(net, [hamming_loss, accuracy_score], [train_dataset], device)['dataset_0'], metric_dict={})\n",
    "writer.add_hparams(hparam_dict=eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset], device)['dataset_0'], metric_dict={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('./models/scene_best/best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.00390625, 'accuracy_score': 0.9765625},\n",
       " 'dataset_1': {'hamming_loss': 0.00390625, 'accuracy_score': 0.9765625}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.00390625, 'accuracy_score': 0.9765625},\n",
       " 'dataset_1': {'hamming_loss': 0.00390625, 'accuracy_score': 0.9765625}}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.0013762730525736307,\n",
       "  'accuracy_score': 0.9917423616845582},\n",
       " 'dataset_1': {'hamming_loss': 0.0013762730525736307,\n",
       "  'accuracy_score': 0.9917423616845582}}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.010416666666666666, 'accuracy_score': 0.9375},\n",
       " 'dataset_1': {'hamming_loss': 0.010416666666666666, 'accuracy_score': 0.9375}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.00390625, 'accuracy_score': 0.9765625},\n",
       " 'dataset_1': {'hamming_loss': 0.00390625, 'accuracy_score': 0.9765625}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [test_dataset, train_dataset], torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.016712834718374883,\n",
       "  'accuracy_score': 0.9268698060941828},\n",
       " 'dataset_1': {'hamming_loss': 0.09302325581395349,\n",
       "  'accuracy_score': 0.6827242524916943}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.01680517082179132,\n",
       "  'accuracy_score': 0.9257617728531856},\n",
       " 'dataset_1': {'hamming_loss': 0.08665559246954596,\n",
       "  'accuracy_score': 0.6827242524916943}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval_metrics() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-6cdcf7d1d4b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mhamming_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: eval_metrics() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.029916897506925208,\n",
       "  'accuracy_score': 0.8626038781163435},\n",
       " 'dataset_1': {'hamming_loss': 0.0858250276854928,\n",
       "  'accuracy_score': 0.6760797342192691}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.029916897506925208,\n",
       "  'accuracy_score': 0.8626038781163435}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'hamming_loss': 0.010156971375807941,\n",
       "  'accuracy_score': 0.9484764542936288},\n",
       " 'dataset_1': {'hamming_loss': 0.07419712070874862,\n",
       "  'accuracy_score': 0.7093023255813954}}"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(net, [hamming_loss, accuracy_score], [train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, 1814.358543395996, 603.0643725395203\n",
      "Epoch: 1, 1806.555687904358, 601.974002957344\n",
      "Epoch: 2, 1802.5488176345825, 601.4645870923996\n",
      "Epoch: 3, 1800.9454917907715, 601.294159412384\n",
      "Epoch: 4, 1801.1399192810059, 601.2288442850113\n",
      "Epoch: 5, 1800.7324028015137, 601.224426150322\n",
      "Epoch: 6, 1800.1720390319824, 601.2312300205231\n",
      "Epoch: 7, 1800.2306661605835, 601.223198890686\n",
      "Epoch: 8, 1800.21125125885, 601.2118003368378\n",
      "Epoch: 9, 1800.0681238174438, 601.1996719837189\n",
      "Epoch: 10, 1800.1249866485596, 601.1949944496155\n",
      "Epoch: 11, 1799.9471454620361, 601.1941817998886\n",
      "Epoch: 12, 1799.929599761963, 601.1943576335907\n",
      "Epoch: 13, 1799.9361963272095, 601.1952466964722\n",
      "Epoch: 14, 1799.907998085022, 601.1959321498871\n",
      "Epoch: 15, 1799.9003524780273, 601.1960427761078\n",
      "Epoch: 16, 1799.898452758789, 601.1952675580978\n",
      "Epoch: 17, 1799.882776260376, 601.1945011615753\n",
      "Epoch: 18, 1799.884165763855, 601.1939296722412\n",
      "Epoch: 19, 1799.8806791305542, 601.1934466362\n",
      "Epoch: 20, 1799.880187034607, 601.1931867599487\n",
      "Epoch: 21, 1799.8792781829834, 601.1930513381958\n",
      "Epoch: 22, 1799.8783626556396, 601.1930272579193\n",
      "Epoch: 23, 1799.8788614273071, 601.1930593252182\n",
      "Epoch: 24, 1799.8789415359497, 601.1930468082428\n",
      "Epoch: 25, 1799.879111289978, 601.1930689811707\n"
     ]
    }
   ],
   "source": [
    "# Decoder.\n",
    "Fd_scene = SFC(latent_dim, 6, num_labels).to(device)\n",
    "\n",
    "# Turn off gradient tracking for encoder networks.\n",
    "for param in Fx_scene.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in Fe_scene.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initializing net.\n",
    "net = C2AE(Fx_scene, Fe_scene, Fd_scene, alpha=0.5, emb_lambda=0.5)\n",
    "net = net.to(device)\n",
    "\n",
    "# Doing weight_decay here is eqiv to adding the L2 norm.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs+1): \n",
    "    # Training.\n",
    "    net.train()\n",
    "    loss_tracker = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()      \n",
    "        # Pass x, y to network. Retrieve both encodings, and decoding of ys encoding.\n",
    "        _, _, fd_z = net(x, y)\n",
    "        loss = net.corr_loss(torch.sigmoid(fd_z), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_tracker+=loss.item()\n",
    "    writer.add_scalar('loss/cor', loss_tracker, epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    net.eval()\n",
    "    test_tracker = 0.0\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # evaluation only requires x. As its just Fd(Fx(x))\n",
    "        loss = net.corr_loss(torch.sigmoid(net.Fd(net.Fe(y))), y)\n",
    "        test_tracker += loss.item()\n",
    "    writer.add_scalar('loss/cor_val', test_tracker, epoch)\n",
    "\n",
    "    print(\"Epoch: {}, {}, {}\".format(epoch, loss_tracker, test_tracker))\n",
    "    save_model(net, './models/c2a/{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_cls, path, *args, **kwargs):\n",
    "    model = model_cls(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFC(\n",
      "  (fc1): Linear(in_features=294, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=6, bias=True)\n",
      ") SFC(\n",
      "  (fc1): Linear(in_features=6, out_features=6, bias=True)\n",
      "  (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
      ") SFC(\n",
      "  (fc1): Linear(in_features=6, out_features=6, bias=True)\n",
      "  (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Fx_scene = SFC(feat_dim, 50, latent_dim).to(device)\n",
    "Fe_scene = SFC(num_labels, 6, latent_dim).to(device)\n",
    "# Decoder.\n",
    "Fd_scene = SFC(latent_dim, 6, num_labels).to(device)\n",
    "\n",
    "net = load_model(C2AE, './models/c2a/36.pt', Fx_scene, Fe_scene, Fd_scene, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxs = net.Fx(X.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fxs = net.Fx(X.to(device))\n",
    "logit_preds = net.Fd(fxs)\n",
    "preds = torch.round(torch.sigmoid(logit_preds)).cpu().detach()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sum(Y == preds, axis=1) == 6).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2407, 6])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,    0.,    0.,    0., 2407., 2407.])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.9726, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.972551>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output_loss(preds, labels):\n",
    "    \"\"\"Computational error function,kYlYck - cl\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : tensorflow tensor {0,1}\n",
    "        binary indicator matrix with label assignments.\n",
    "    output : tensorflow tensor [0,1]\n",
    "    neural network output value\n",
    "    Returns\n",
    "\n",
    "\n",
    "    labels = (N, l)\n",
    "    -------\n",
    "    tensorflow tensor\n",
    "    \"\"\"\n",
    "    # Generate masks for [0,1] elements.\n",
    "    ones = (labels == 1)\n",
    "    zeros = (labels == 0)\n",
    "    # Use broadcasting to apply logical and between mask arrays.\n",
    "    # This will only indicate locations where both masks are 1.\n",
    "    # For us this corresponds to set we are enumerating in eq (3) in Yah et al.\n",
    "    ix_matrix = ones[:,:, None] & zeros[:, None, :]\n",
    "    # print(ix_matrix)\n",
    "    # Use same broadcasting logic to generate exponetial differences.\n",
    "    # This like the above broadcast will do so between all pairs of points\n",
    "    # for every datapoint.\n",
    "    diff_matrix = torch.exp(-(preds[:, :, None] - preds[:, None, :]))\n",
    "    # print(diff_matrix)\n",
    "\n",
    "    # print(diff_matrix*ix_matrix, (diff_matrix*ix_matrix).shape)\n",
    "    # This will sum all contributes to loss for each datapoint.\n",
    "    losses = torch.flatten(diff_matrix*ix_matrix, start_dim=1).sum(dim=1)\n",
    "    # print(losses)\n",
    "    # print(ones.sum(dim=1), zeros.sum(dim=1))\n",
    "    # print((ones.sum(dim=1)*zeros.sum(dim=1)).shape)\n",
    "    # print((ones.sum(dim=1)*zeros.sum(dim=1)))\n",
    "    # Normalize each loss\n",
    "    losses /= (ones.sum(dim=1)*zeros.sum(dim=1))\n",
    "    # print(losses.shape)\n",
    "    # Combine all losses to retrieve final loss.\n",
    "    return  losses.sum()\n",
    "\n",
    "\n",
    "def output_loss(predictions, labels):\n",
    "        \"\"\"Computational error function,kYlYck - cl\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : tensorflow tensor {0,1}\n",
    "            binary indicator matrix with label assignments.\n",
    "        output : tensorflow tensor [0,1]\n",
    "            neural network output value\n",
    "        Returns\n",
    "        -------\n",
    "        tensorflow tensor\n",
    "        \"\"\"\n",
    "        shape = tf.shape(labels)\n",
    "\n",
    "        y_i = tf.equal(labels, tf.ones(shape))\n",
    "        y_not_i = tf.equal(labels, tf.zeros(shape))\n",
    "\n",
    "        # get indices to check\n",
    "        truth_matrix = tf.cast(pairwise_and(y_i, y_not_i), float)\n",
    "\n",
    "        # calculate all exp'd differences\n",
    "        # through and with truth_matrix, we can get all c_i - c_k(appear in the paper)\n",
    "        sub_matrix = pairwise_sub(predictions, predictions)\n",
    "        exp_matrix = tf.exp(tf.negative(sub_matrix))\n",
    "\n",
    "        # check which differences to consider and sum them\n",
    "        sparse_matrix = tf.multiply(exp_matrix, truth_matrix)\n",
    "        sums = tf.reduce_sum(sparse_matrix, axis=[1, 2])\n",
    "\n",
    "        # get normalizing terms and apply them\n",
    "        y_i_sizes = tf.reduce_sum(tf.cast(y_i, float), axis=1)\n",
    "        y_i_bar_sizes = tf.reduce_sum(tf.cast(y_not_i, float), axis=1)\n",
    "        normalizers = tf.multiply(y_i_sizes, y_i_bar_sizes)\n",
    "\n",
    "        loss = tf.divide(sums, normalizers)\n",
    "        zero = tf.zeros_like(loss)\n",
    "        loss = tf.where(tf.logical_or(tf.math.is_inf(loss), tf.math.is_nan(loss)), x=zero, y=loss)\n",
    "        loss = tf.reduce_sum(loss)\n",
    "        return loss\n",
    "    \n",
    "def pairwise_and(a, b):\n",
    "        \"\"\"compute pairwise logical and between elements of the tensors a and b\n",
    "        Description\n",
    "        -----\n",
    "        if y shape is [3,3], y_i would be translate to [3,3,1], y_not_i is would be [3,1,3]\n",
    "        and return [3,3,3],through the matrix ,we can easy to caculate c_k - c_i(appear in the paper)\n",
    "        \"\"\"\n",
    "        column = tf.expand_dims(a, 2)\n",
    "        row = tf.expand_dims(b, 1)\n",
    "        return tf.logical_and(column, row)\n",
    "\n",
    "def pairwise_sub(a, b):\n",
    "        \"\"\"compute pairwise differences between elements of the tensors a and b\n",
    "        :param a:\n",
    "        :param b:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        column = tf.expand_dims(a, 2)\n",
    "        row = tf.expand_dims(b, 1)\n",
    "        return tf.subtract(column, row)\n",
    "    \n",
    "    \n",
    "def embedding_loss(Fx, Fe):\n",
    "        \"\"\"\n",
    "        caculate embedding loss\n",
    "        min(||Fx(X) - Fe(Y)||^2), subject to Fx(X)Fx(X)^T = Fe(Y)Fe(Y)^T = I\n",
    "        use Lagrange method and lagrange coefficient equeal to 0.5\n",
    "        :param Fx: tensor {n_intances, n_latent_embedding_dim}\n",
    "            Fx latent embedding data\n",
    "        :param Fe: tensor {n_intances, n_latent_embedding_dim}\n",
    "            Fe latent embedding data\n",
    "        :return: tensor\n",
    "            all n_insances loss\n",
    "        \"\"\"\n",
    "        I = tf.eye(tf.shape(Fx)[1])\n",
    "        C1, C2, C3 = Fx - Fe, tf.matmul(tf.transpose(Fx), Fx) - I, tf.matmul(tf.transpose(Fe), Fe) - I\n",
    "        loss = tf.linalg.trace(tf.matmul(C1, tf.transpose(C1))) + tf.linalg.trace(tf.matmul(C2, tf.transpose(C2)) + tf.matmul(C3, tf.transpose(C3)))\n",
    "        return loss\n",
    "net.eval()\n",
    "X_train, y_train = train_dataset[:5][0].to(device), train_dataset[:5][1].to(device)\n",
    "X_train.shape\n",
    "\n",
    "torch_fx = Fx_scene(X_train[:5])\n",
    "torch_fe = Fe_scene(y_train[:5])\n",
    "tf_fx = tf.convert_to_tensor(torch_fx.detach().cpu().numpy())\n",
    "tf_fe = tf.convert_to_tensor(torch_fe.detach().cpu().numpy())\n",
    "\n",
    "net.latent_loss(torch_fx, torch_fe), embedding_loss(tf_fx, tf_fe)\n",
    "\n",
    "\n",
    "preds = torch.sigmoid(Fd_scene(torch_fx))\n",
    "tf_preds = tf.convert_to_tensor(preds.detach().cpu().numpy())\n",
    "tf_ys = tf.convert_to_tensor(y_train[:5].detach().cpu().numpy())\n",
    "net.corr_loss(preds, y_train[:5]), output_loss(tf_preds, tf_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [4]]) tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5]])\n",
      "tensor([[0],\n",
      "        [5]]) tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[0]]) tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n",
      "tensor([[0]]) tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n",
      "tensor([[0]]) tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.0206], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_loss(preds, Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_loss(preds, y):\n",
    "    \"\"\"\n",
    "    Loss between output of decoder, and actual y label of example.\n",
    "    \"\"\"\n",
    "    E_i = 0.0\n",
    "    for i in range(y.shape[0]):\n",
    "        pos_ixs = torch.nonzero(y[i, :] != 0)\n",
    "        neg_ixs = torch.nonzero(y[i, :] == 0)\n",
    "        e_i = 0.0\n",
    "        for p_ix in pos_ixs:\n",
    "            for n_ix in neg_ixs:\n",
    "                e_i += torch.exp(-(preds[i, p_ix]-preds[i, n_ix]))\n",
    "        E_i += e_i/(len(pos_ixs)*len(neg_ixs))\n",
    "\n",
    "    return E_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('torch_sp_env': virtualenv)",
   "language": "python",
   "name": "python37264bittorchspenvvirtualenv64bc8108ce9c407e99ec601d0f3b78f0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
