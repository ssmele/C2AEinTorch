{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from C2AE import save_model, eval_metrics, load_model\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mediamill:train - exists, not redownloading\n",
      "mediamill:test - exists, not redownloading\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "train_x, train_y, feat_names, label_names = load_dataset('mediamill', 'train')\n",
    "test_x, test_y, _, _ = load_dataset('mediamill', 'test')\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_x.todense(), device=device, dtype=torch.float),torch.tensor(train_y.todense(), device=device,dtype=torch.float))\n",
    "test_dataset = TensorDataset(torch.tensor(test_x.todense(), device=device, dtype=torch.float), torch.tensor(test_y.todense(), device=device, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='micro')\n",
    "def macro_r(y_t, y_p):\n",
    "    return recall_score(y_t, y_p, average='macro')\n",
    "def micro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='micro')\n",
    "def macro_p(y_t, y_p):\n",
    "    return precision_score(y_t, y_p, average='macro')\n",
    "def micro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='micro')\n",
    "def macro_f1(y_t, y_p):\n",
    "    return f1_score(y_t, y_p, average='macro')\n",
    "def ham_los(*args, **kwargs):\n",
    "    return hamming_loss(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making TensorDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30993, 120), (30993, 101), (12914, 120), (12914, 101))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43907"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[0] + test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediamillModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MediamillModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(120, 101)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "lr = 0.001\n",
    "batch_size=256\n",
    "\n",
    "net = MediamillModel().to(device)\n",
    "writer = SummaryWriter(comment='mediamill_fc')\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000, Train loss: 0.08613247754143886, Test loss: 0.09163471033760145\n",
      "Epoch: 4005, Train loss: 0.08620843718774983, Test loss: 0.091657734969083\n",
      "Epoch: 4010, Train loss: 0.08614608046949887, Test loss: 0.09172556198695127\n",
      "Epoch: 4015, Train loss: 0.08625598538850175, Test loss: 0.09170330082084618\n",
      "Epoch: 4020, Train loss: 0.08612408409597444, Test loss: 0.09185854681566649\n",
      "Epoch: 4025, Train loss: 0.08617285584084323, Test loss: 0.09190288595124788\n",
      "Epoch: 4030, Train loss: 0.08611986883839623, Test loss: 0.09186678730389651\n",
      "Epoch: 4035, Train loss: 0.0860568668510093, Test loss: 0.09190276265144348\n",
      "Epoch: 4040, Train loss: 0.08621608239949727, Test loss: 0.09193398204504274\n",
      "Epoch: 4045, Train loss: 0.08611884255145417, Test loss: 0.0919204019740516\n",
      "Epoch: 4050, Train loss: 0.08608640382280115, Test loss: 0.09185086687405904\n",
      "Epoch: 4055, Train loss: 0.08604107162014382, Test loss: 0.09175187729152978\n",
      "Epoch: 4060, Train loss: 0.08597204113592867, Test loss: 0.09188787478442285\n",
      "Epoch: 4065, Train loss: 0.08621709567845845, Test loss: 0.09200018381371218\n",
      "Epoch: 4070, Train loss: 0.08601604658560674, Test loss: 0.09194541824798957\n",
      "Epoch: 4075, Train loss: 0.08606008824999214, Test loss: 0.0918726864106515\n",
      "Epoch: 4080, Train loss: 0.08593139840198345, Test loss: 0.09188195756253074\n",
      "Epoch: 4085, Train loss: 0.08608514149902297, Test loss: 0.09202459995068756\n",
      "Epoch: 4090, Train loss: 0.0861891867806677, Test loss: 0.09167843574986738\n",
      "Epoch: 4095, Train loss: 0.08607526880795838, Test loss: 0.09180065507397932\n",
      "Epoch: 4100, Train loss: 0.08607144104164155, Test loss: 0.0918892860120418\n",
      "Epoch: 4105, Train loss: 0.08597040133642368, Test loss: 0.0919661802404067\n",
      "Epoch: 4110, Train loss: 0.08604568230812668, Test loss: 0.09186326989940569\n",
      "Epoch: 4115, Train loss: 0.08621409278912623, Test loss: 0.09209569397510267\n",
      "Epoch: 4120, Train loss: 0.08586649413480134, Test loss: 0.0917205287545335\n",
      "Epoch: 4125, Train loss: 0.08619708989243038, Test loss: 0.09186366127402175\n",
      "Epoch: 4130, Train loss: 0.08610452546692285, Test loss: 0.09183835486570995\n",
      "Epoch: 4135, Train loss: 0.0860053339942557, Test loss: 0.09170213502411749\n",
      "Epoch: 4140, Train loss: 0.08604976437130912, Test loss: 0.09170883119690652\n",
      "Epoch: 4145, Train loss: 0.08602229890520455, Test loss: 0.09171584204715841\n",
      "Epoch: 4150, Train loss: 0.08589820624863514, Test loss: 0.09181398226349961\n",
      "Epoch: 4155, Train loss: 0.08605830420236119, Test loss: 0.0920398609310973\n",
      "Epoch: 4160, Train loss: 0.0860775856942427, Test loss: 0.09175280817583495\n",
      "Epoch: 4165, Train loss: 0.08608143135416703, Test loss: 0.09181053074551564\n",
      "Epoch: 4170, Train loss: 0.0860353296653169, Test loss: 0.09187310115963805\n",
      "Epoch: 4175, Train loss: 0.08600330926844331, Test loss: 0.09197969544751972\n",
      "Epoch: 4180, Train loss: 0.08607715246130208, Test loss: 0.09191267922812817\n",
      "Epoch: 4185, Train loss: 0.08612755252445331, Test loss: 0.09190492521898419\n",
      "Epoch: 4190, Train loss: 0.08617188866998328, Test loss: 0.09181136491836286\n",
      "Epoch: 4195, Train loss: 0.08609265205068667, Test loss: 0.09166276045874053\n",
      "Epoch: 4200, Train loss: 0.08594363017893228, Test loss: 0.09182937793871936\n",
      "Epoch: 4205, Train loss: 0.08600546065412584, Test loss: 0.09174346266423955\n",
      "Epoch: 4210, Train loss: 0.08585887701540697, Test loss: 0.09164848076362236\n",
      "Epoch: 4215, Train loss: 0.08608936200864979, Test loss: 0.09181632159971724\n",
      "Epoch: 4220, Train loss: 0.08613230062068486, Test loss: 0.09199054583030589\n",
      "Epoch: 4225, Train loss: 0.0861631732006542, Test loss: 0.09197822899795045\n",
      "Epoch: 4230, Train loss: 0.08598317431866145, Test loss: 0.09180552906849805\n",
      "Epoch: 4235, Train loss: 0.08629646573643215, Test loss: 0.09198782154742409\n",
      "Epoch: 4240, Train loss: 0.08605173327883736, Test loss: 0.09192735847889208\n",
      "Epoch: 4245, Train loss: 0.08602055882821318, Test loss: 0.09178335029704898\n",
      "Epoch: 4250, Train loss: 0.08594923222162684, Test loss: 0.09203058364344578\n",
      "Epoch: 4255, Train loss: 0.08601041180921383, Test loss: 0.09179524288457983\n",
      "Epoch: 4260, Train loss: 0.0859686269379053, Test loss: 0.09154766754192464\n",
      "Epoch: 4265, Train loss: 0.08595426502774973, Test loss: 0.09197100646355573\n",
      "Epoch: 4270, Train loss: 0.08593114379976617, Test loss: 0.09179646507197735\n",
      "Epoch: 4275, Train loss: 0.08601949101344483, Test loss: 0.0917619268099467\n",
      "Epoch: 4280, Train loss: 0.08610090277478342, Test loss: 0.09179604915427227\n",
      "Epoch: 4285, Train loss: 0.08598017161254023, Test loss: 0.09194071371765698\n",
      "Epoch: 4290, Train loss: 0.08578531135667543, Test loss: 0.0917941870934823\n",
      "Epoch: 4295, Train loss: 0.0858741427664874, Test loss: 0.0917426264461349\n",
      "Epoch: 4300, Train loss: 0.08600127043538407, Test loss: 0.09181894624934477\n",
      "Epoch: 4305, Train loss: 0.08594630127314662, Test loss: 0.09187792592188891\n",
      "Epoch: 4310, Train loss: 0.0861441400207457, Test loss: 0.09180380623130237\n",
      "Epoch: 4315, Train loss: 0.0859122603643136, Test loss: 0.09190511995670843\n",
      "Epoch: 4320, Train loss: 0.08597084330242188, Test loss: 0.09179778820743748\n",
      "Epoch: 4325, Train loss: 0.08602406970057332, Test loss: 0.09187204858251646\n",
      "Epoch: 4330, Train loss: 0.08600252500322998, Test loss: 0.09177751167147767\n",
      "Epoch: 4335, Train loss: 0.08580712033588378, Test loss: 0.09177402509193794\n",
      "Epoch: 4340, Train loss: 0.08601102666532406, Test loss: 0.09186583143823288\n",
      "Epoch: 4345, Train loss: 0.08621027461085164, Test loss: 0.09162356412293864\n",
      "Epoch: 4350, Train loss: 0.08607156226625208, Test loss: 0.09177746273138944\n",
      "Epoch: 4355, Train loss: 0.08609503525935236, Test loss: 0.09180517830685073\n",
      "Epoch: 4360, Train loss: 0.08589409322279398, Test loss: 0.09156898365301244\n",
      "Epoch: 4365, Train loss: 0.0860802442690388, Test loss: 0.09160129798977983\n",
      "Epoch: 4370, Train loss: 0.08613580654634805, Test loss: 0.09177936540514815\n",
      "Epoch: 4375, Train loss: 0.08583494959796062, Test loss: 0.09170609566510893\n",
      "Epoch: 4380, Train loss: 0.08602949463930286, Test loss: 0.0917306913464677\n",
      "Epoch: 4385, Train loss: 0.08582712459515353, Test loss: 0.09182758731584922\n",
      "Epoch: 4390, Train loss: 0.08592998786050765, Test loss: 0.09175016175882489\n",
      "Epoch: 4395, Train loss: 0.08595559171965865, Test loss: 0.09173932174841563\n",
      "Epoch: 4400, Train loss: 0.08585066110139987, Test loss: 0.09149355441331863\n",
      "Epoch: 4405, Train loss: 0.08597639935915588, Test loss: 0.0918724285621269\n",
      "Epoch: 4410, Train loss: 0.08577197460366077, Test loss: 0.09182796846417819\n",
      "Epoch: 4415, Train loss: 0.08581736708273653, Test loss: 0.09188690357932858\n",
      "Epoch: 4420, Train loss: 0.08591644948378938, Test loss: 0.09149906097673903\n",
      "Epoch: 4425, Train loss: 0.08603448292515317, Test loss: 0.09164483480009378\n",
      "Epoch: 4430, Train loss: 0.08582399235885652, Test loss: 0.09180677331545774\n",
      "Epoch: 4435, Train loss: 0.08597324177867076, Test loss: 0.09182309184004278\n",
      "Epoch: 4440, Train loss: 0.08621010764456186, Test loss: 0.09173229877270904\n",
      "Epoch: 4445, Train loss: 0.08599365789626466, Test loss: 0.09187939310190725\n",
      "Epoch: 4450, Train loss: 0.08586287327477189, Test loss: 0.09199020295750861\n",
      "Epoch: 4455, Train loss: 0.08600556471797287, Test loss: 0.09177096334158205\n",
      "Epoch: 4460, Train loss: 0.08590423254693141, Test loss: 0.09168468620262894\n",
      "Epoch: 4465, Train loss: 0.08600769625579724, Test loss: 0.09160087447540433\n",
      "Epoch: 4470, Train loss: 0.08574141165027853, Test loss: 0.0918680109521922\n",
      "Epoch: 4475, Train loss: 0.08593245210950493, Test loss: 0.09164584544943828\n",
      "Epoch: 4480, Train loss: 0.08591281500507573, Test loss: 0.09171917946899638\n",
      "Epoch: 4485, Train loss: 0.08583701322557497, Test loss: 0.09173659439764771\n",
      "Epoch: 4490, Train loss: 0.08591731081976266, Test loss: 0.09201369682947795\n",
      "Epoch: 4495, Train loss: 0.0857725732150625, Test loss: 0.09183076374671038\n",
      "Epoch: 4500, Train loss: 0.08585878730308814, Test loss: 0.09156628931854285\n",
      "Epoch: 4505, Train loss: 0.08593835645034666, Test loss: 0.09180855327377133\n",
      "Epoch: 4510, Train loss: 0.08584988721814311, Test loss: 0.0919652527161673\n",
      "Epoch: 4515, Train loss: 0.0860812709834732, Test loss: 0.09173930480199702\n",
      "Epoch: 4520, Train loss: 0.08573059420116612, Test loss: 0.09176994918608199\n",
      "Epoch: 4525, Train loss: 0.08567618087056231, Test loss: 0.09185650374959498\n",
      "Epoch: 4530, Train loss: 0.0857219001064535, Test loss: 0.09168129049095453\n",
      "Epoch: 4535, Train loss: 0.0858313617769812, Test loss: 0.09204715870174707\n",
      "Epoch: 4540, Train loss: 0.08570465305056728, Test loss: 0.09176963582342747\n",
      "Epoch: 4545, Train loss: 0.08574893356102412, Test loss: 0.09196745604276657\n",
      "Epoch: 4550, Train loss: 0.08587578122244506, Test loss: 0.09176896424854503\n",
      "Epoch: 4555, Train loss: 0.08585814545389081, Test loss: 0.0918836511817633\n",
      "Epoch: 4560, Train loss: 0.08611079439765117, Test loss: 0.0917380051869972\n",
      "Epoch: 4565, Train loss: 0.08579254315280524, Test loss: 0.09178311480026619\n",
      "Epoch: 4570, Train loss: 0.08578997129788164, Test loss: 0.09176219167078242\n",
      "Epoch: 4575, Train loss: 0.08581052787724089, Test loss: 0.09188467424874212\n",
      "Epoch: 4580, Train loss: 0.08604909497939173, Test loss: 0.09181748301375146\n",
      "Epoch: 4585, Train loss: 0.08590408897057908, Test loss: 0.09175022501571506\n",
      "Epoch: 4590, Train loss: 0.08598729809288119, Test loss: 0.09167133929098353\n",
      "Epoch: 4595, Train loss: 0.08577802710112978, Test loss: 0.09199224719229866\n",
      "Epoch: 4600, Train loss: 0.08576492244591478, Test loss: 0.09173719526505937\n",
      "Epoch: 4605, Train loss: 0.08583534905900721, Test loss: 0.09182250455898397\n",
      "Epoch: 4610, Train loss: 0.08607589746596384, Test loss: 0.09176770831440009\n",
      "Epoch: 4615, Train loss: 0.08576184175297862, Test loss: 0.09187965051216238\n",
      "Epoch: 4620, Train loss: 0.08612482622265816, Test loss: 0.09159696058315389\n",
      "Epoch: 4625, Train loss: 0.0859858262367913, Test loss: 0.0916309149241915\n",
      "Epoch: 4630, Train loss: 0.08597161504821699, Test loss: 0.09160378034792695\n",
      "Epoch: 4635, Train loss: 0.08571798317745084, Test loss: 0.09163107664561738\n",
      "Epoch: 4640, Train loss: 0.08567662158461868, Test loss: 0.09176473611710119\n",
      "Epoch: 4645, Train loss: 0.0859395584977064, Test loss: 0.09171728220056086\n",
      "Epoch: 4650, Train loss: 0.0857720417810268, Test loss: 0.09188383467057172\n",
      "Epoch: 4655, Train loss: 0.08575299675347375, Test loss: 0.0915612018868035\n",
      "Epoch: 4660, Train loss: 0.08600325253410418, Test loss: 0.09177189524851594\n",
      "Epoch: 4665, Train loss: 0.08585665222318446, Test loss: 0.09195216058516036\n",
      "Epoch: 4670, Train loss: 0.08574503189960464, Test loss: 0.09179817213147294\n",
      "Epoch: 4675, Train loss: 0.0857443775309891, Test loss: 0.09156192444703158\n",
      "Epoch: 4680, Train loss: 0.08585111296079198, Test loss: 0.09181850418156269\n",
      "Epoch: 4685, Train loss: 0.0858136058830824, Test loss: 0.09180724796126871\n",
      "Epoch: 4690, Train loss: 0.0855639823330719, Test loss: 0.09170765590433981\n",
      "Epoch: 4695, Train loss: 0.08594493061059812, Test loss: 0.09158288702076557\n",
      "Epoch: 4700, Train loss: 0.08571953766170096, Test loss: 0.09171505360042348\n",
      "Epoch: 4705, Train loss: 0.08584414416405022, Test loss: 0.09177063873001173\n",
      "Epoch: 4710, Train loss: 0.08565560642813073, Test loss: 0.09181818439095628\n",
      "Epoch: 4715, Train loss: 0.08596489767803521, Test loss: 0.091577421800763\n",
      "Epoch: 4720, Train loss: 0.085969121668671, Test loss: 0.09180085989190083\n",
      "Epoch: 4725, Train loss: 0.08571314817813576, Test loss: 0.09177201927876939\n",
      "Epoch: 4730, Train loss: 0.08563804779140675, Test loss: 0.09151874351150849\n",
      "Epoch: 4735, Train loss: 0.08574020911435612, Test loss: 0.09161276940037222\n",
      "Epoch: 4740, Train loss: 0.08567905316098792, Test loss: 0.09179551753343321\n",
      "Epoch: 4745, Train loss: 0.08573283841375445, Test loss: 0.09169186914668363\n",
      "Epoch: 4750, Train loss: 0.08588877668390509, Test loss: 0.09175761146288292\n",
      "Epoch: 4755, Train loss: 0.08587775196208329, Test loss: 0.09165425978454889\n",
      "Epoch: 4760, Train loss: 0.08569291612652481, Test loss: 0.09175483035106285\n",
      "Epoch: 4765, Train loss: 0.08587789993549956, Test loss: 0.09187968893378388\n",
      "Epoch: 4770, Train loss: 0.0858191129003392, Test loss: 0.09155424684286118\n",
      "Epoch: 4775, Train loss: 0.08589616234673829, Test loss: 0.09177594164422914\n",
      "Epoch: 4780, Train loss: 0.08575707252641193, Test loss: 0.09178244059576708\n",
      "Epoch: 4785, Train loss: 0.08561640354942103, Test loss: 0.09177268252653234\n",
      "Epoch: 4790, Train loss: 0.08564715716438215, Test loss: 0.09168653862149108\n",
      "Epoch: 4795, Train loss: 0.08570592593951304, Test loss: 0.09171614357653786\n",
      "Epoch: 4800, Train loss: 0.08572778099628746, Test loss: 0.09172357107494392\n",
      "Epoch: 4805, Train loss: 0.08595131123896504, Test loss: 0.0917610115572518\n",
      "Epoch: 4810, Train loss: 0.08573890764449464, Test loss: 0.09175106269471786\n",
      "Epoch: 4815, Train loss: 0.08581095463672622, Test loss: 0.09167462631183512\n",
      "Epoch: 4820, Train loss: 0.0858205366940772, Test loss: 0.09168557296780978\n",
      "Epoch: 4825, Train loss: 0.0858418818746434, Test loss: 0.09168431323532965\n",
      "Epoch: 4830, Train loss: 0.08572626168854901, Test loss: 0.091732025438664\n",
      "Epoch: 4835, Train loss: 0.08574407981311688, Test loss: 0.09197201053885852\n",
      "Epoch: 4840, Train loss: 0.08566662247796528, Test loss: 0.09169667769296497\n",
      "Epoch: 4845, Train loss: 0.0855954182563258, Test loss: 0.09175789443885579\n",
      "Epoch: 4850, Train loss: 0.08578904931906794, Test loss: 0.09167879293946658\n",
      "Epoch: 4855, Train loss: 0.08575116293352159, Test loss: 0.09170299812274821\n",
      "Epoch: 4860, Train loss: 0.08587541632720681, Test loss: 0.09171292565616906\n",
      "Epoch: 4865, Train loss: 0.08584501417201074, Test loss: 0.09174396185313954\n",
      "Epoch: 4870, Train loss: 0.08578708261007169, Test loss: 0.09160976769293055\n",
      "Epoch: 4875, Train loss: 0.08580938482382258, Test loss: 0.09187529133815392\n",
      "Epoch: 4880, Train loss: 0.08563132022247939, Test loss: 0.0917438670408492\n",
      "Epoch: 4885, Train loss: 0.08572154076861553, Test loss: 0.09175259371598561\n",
      "Epoch: 4890, Train loss: 0.08579147539910723, Test loss: 0.09181089334043802\n",
      "Epoch: 4895, Train loss: 0.08574424433659335, Test loss: 0.09176117736919254\n",
      "Epoch: 4900, Train loss: 0.08572849356493012, Test loss: 0.09172961028183207\n",
      "Epoch: 4905, Train loss: 0.08569071099894945, Test loss: 0.09180263546751995\n",
      "Epoch: 4910, Train loss: 0.08574870021128264, Test loss: 0.09171994147347469\n",
      "Epoch: 4915, Train loss: 0.0857844527994023, Test loss: 0.09154275249616772\n",
      "Epoch: 4920, Train loss: 0.08585558440841612, Test loss: 0.09164728092796662\n",
      "Epoch: 4925, Train loss: 0.08570991664147767, Test loss: 0.09171487946136325\n",
      "Epoch: 4930, Train loss: 0.08575402334576747, Test loss: 0.09177496576426077\n",
      "Epoch: 4935, Train loss: 0.08566965840634752, Test loss: 0.09169139260170507\n",
      "Epoch: 4940, Train loss: 0.08558207011369408, Test loss: 0.09186667174685235\n",
      "Epoch: 4945, Train loss: 0.0857675631271034, Test loss: 0.09174450866731942\n",
      "Epoch: 4950, Train loss: 0.08557762242242938, Test loss: 0.09185404958678227\n",
      "Epoch: 4955, Train loss: 0.0857357826144969, Test loss: 0.09161539755615533\n",
      "Epoch: 4960, Train loss: 0.08570034508822394, Test loss: 0.09154028094866697\n",
      "Epoch: 4965, Train loss: 0.08572238640951328, Test loss: 0.09158864062206418\n",
      "Epoch: 4970, Train loss: 0.08572646364813945, Test loss: 0.09176207027014564\n",
      "Epoch: 4975, Train loss: 0.0855935567715129, Test loss: 0.09175187875242795\n",
      "Epoch: 4980, Train loss: 0.0855648579900382, Test loss: 0.09186628811499652\n",
      "Epoch: 4985, Train loss: 0.08568726742609603, Test loss: 0.09168782450404822\n",
      "Epoch: 4990, Train loss: 0.08591931044566826, Test loss: 0.09181058275349\n",
      "Epoch: 4995, Train loss: 0.08561726873282527, Test loss: 0.09176578445761811\n",
      "Epoch: 5000, Train loss: 0.08570865016491687, Test loss: 0.09183272091197033\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(0, 5000+1):    \n",
    "    net.train()\n",
    "    loss_tracker = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = net(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_tracker+=loss.item()\n",
    "    train_loss.append(loss_tracker/len(train_dataloader))\n",
    "    writer.add_scalar('train/loss', loss_tracker/len(train_dataloader), epoch)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        net.eval()\n",
    "        test_tracker = 0.0\n",
    "        for x, y in test_dataloader:\n",
    "            preds = net(x)\n",
    "            loss = criterion(preds, y)\n",
    "            test_tracker += loss.item()\n",
    "        test_loss.append(test_tracker/len(test_dataloader))\n",
    "        writer.add_scalar('val/loss', test_tracker/len(test_dataloader), epoch)\n",
    "\n",
    "        # Log all the good metrics to the board.\n",
    "        mets = eval_metrics(net, [ham_los, accuracy_score, micro_f1, micro_p, micro_r, macro_f1, macro_p, macro_r], \n",
    "                                [test_dataset, train_dataset], device, apply_sig=True)\n",
    "        for k, v in mets['dataset_1'].items():\n",
    "            writer.add_scalar(f'train/{k}', v, epoch)\n",
    "        # Train\n",
    "        for k, v in mets['dataset_0'].items():\n",
    "            writer.add_scalar(f'val/{k}', v, epoch)\n",
    "        print(\"Epoch: {}, Train loss: {}, Test loss: {}\".format(epoch, train_loss[-1], test_loss[-1]))\n",
    "    torch.save(net.state_dict(), f'./models/mediamill/fc/{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_net = load_model(MediamillModel, './models/mediamill/fc/5000.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stone\\miniconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_0': {'ham_los': 0.030890567762057296,\n",
       "  'accuracy_score': 0.08990243146972278,\n",
       "  'micro_f1': 0.5416320633439893,\n",
       "  'micro_p': 0.7679032258064517,\n",
       "  'micro_r': 0.4183582010860969,\n",
       "  'macro_f1': 0.06557044295932339,\n",
       "  'macro_p': 0.2319994582527386,\n",
       "  'macro_r': 0.05073817881601729},\n",
       " 'dataset_1': {'ham_los': 0.02998569143527459,\n",
       "  'accuracy_score': 0.08437389087858549,\n",
       "  'micro_f1': 0.5517606945359732,\n",
       "  'micro_p': 0.7787003464125409,\n",
       "  'micro_r': 0.4272465740254554,\n",
       "  'macro_f1': 0.08180357845370219,\n",
       "  'macro_p': 0.3220003934810215,\n",
       "  'macro_r': 0.06097828672199412}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mets = eval_metrics(eval_net, [ham_los, accuracy_score, micro_f1, micro_p, micro_r, macro_f1, macro_p, macro_r], [test_dataset, train_dataset], device, apply_sig=True)\n",
    "mets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
